# Enhanced Bi-Fusionï¼ˆCategory-Agnosticï¼ŒScanNet200-SVï¼‰
# å®Œå…¨ç‹¬ç«‹çš„BiFusioné…ç½®ï¼Œæ— TinySAä¾èµ–ï¼Œç»´åº¦åŒ¹é…ä¼˜åŒ–

_base_ = [
    'mmdet3d::_base_/default_runtime.py',
    'mmdet3d::_base_/datasets/scannet-seg.py'
]

custom_imports = dict(imports=[
    'oneformer3d', 
    'oneformer3d.partial_load_hook',
    'oneformer3d.detailed_loss_hook',
    'oneformer3d.enhanced_training_hook'  # ğŸ”¥ å¯¼å…¥å¢å¼ºè®­ç»ƒç›‘æ§Hook
])

# ======== ç±»åˆ«å’Œç»´åº¦è®¾ç½® ========
num_instance_classes = 1
num_semanti# ğŸ”¥ å¢å¼ºæ—¥å¿—é…ç½® - æš‚æ—¶ç¦ç”¨TensorBoardè§£å†³Python 3.8å…¼å®¹æ€§é—®é¢˜
log_config = dict(
    interval=5,   # æ¯5ä¸ªiterè¾“å‡ºä¸€æ¬¡åŸºç¡€æ—¥å¿— 
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        # dict(type='TensorboardLoggerHook', by_epoch=False)  # æš‚æ—¶ç¦ç”¨
    ])es = 200
num_instance_classes_eval = 1

# ======== æ•°æ®è·¯å¾„é…ç½® ========
DATA_ROOT = '/home/nebula/xxy/ESAM/data/scannet200-sv/'

# ======== é¢œè‰²å½’ä¸€åŒ–å‚æ•° ========
color_mean = (
    0.47793125906962 * 255,
    0.4303257521323044 * 255,
    0.3749598901421883 * 255)
color_std = (
    0.2834475483823543 * 255,
    0.27566157565723015 * 255,
    0.27018971370874995 * 255)

# ======== ç±»åˆ«åç§° ========
class_names = [
    'wall', 'floor', 'chair', 'table', 'door', 'couch', 'cabinet', 'shelf',
    'desk', 'office chair', 'bed', 'pillow', 'sink', 'picture', 'window',
    'toilet', 'bookshelf', 'monitor', 'curtain', 'book', 'armchair',
    'coffee table', 'box', 'refrigerator', 'lamp', 'kitchen cabinet', 'towel',
    'clothes', 'tv', 'nightstand', 'counter', 'dresser', 'stool', 'cushion',
    'plant', 'ceiling', 'bathtub', 'end table', 'dining table', 'keyboard',
    'bag', 'backpack', 'toilet paper', 'printer', 'tv stand', 'whiteboard',
    'blanket', 'shower curtain', 'trash can', 'closet', 'stairs', 'microwave',
    'stove', 'shoe', 'computer tower', 'bottle', 'bin', 'ottoman', 'bench',
    'board', 'washing machine', 'mirror', 'copier', 'basket', 'sofa chair',
    'file cabinet', 'fan', 'laptop', 'shower', 'paper', 'person',
    'paper towel dispenser', 'oven', 'blinds', 'rack', 'plate', 'blackboard',
    'piano', 'suitcase', 'rail', 'radiator', 'recycling bin', 'container',
    'wardrobe', 'soap dispenser', 'telephone', 'bucket', 'clock', 'stand',
    'light', 'laundry basket', 'pipe', 'clothes dryer', 'guitar',
    'toilet paper holder', 'seat', 'speaker', 'column', 'bicycle', 'ladder',
    'bathroom stall', 'shower wall', 'cup', 'jacket', 'storage bin',
    'coffee maker', 'dishwasher', 'paper towel roll', 'machine', 'mat',
    'windowsill', 'bar', 'toaster', 'bulletin board', 'ironing board',
    'fireplace', 'soap dish', 'kitchen counter', 'doorframe',
    'toilet paper dispenser', 'mini fridge', 'fire extinguisher', 'ball',
    'hat', 'shower curtain rod', 'water cooler', 'paper cutter', 'tray',
    'shower door', 'pillar', 'ledge', 'toaster oven', 'mouse',
    'toilet seat cover dispenser', 'furniture', 'cart', 'storage container',
    'scale', 'tissue box', 'light switch', 'crate', 'power outlet',
    'decoration', 'sign', 'projector', 'closet door', 'vacuum cleaner',
    'candle', 'plunger', 'stuffed animal', 'headphones', 'dish rack',
    'broom', 'guitar case', 'range hood', 'dustpan', 'hair dryer',
    'water bottle', 'handicap bar', 'purse', 'vent', 'shower floor',
    'water pitcher', 'mailbox', 'bowl', 'paper bag', 'alarm clock',
    'music stand', 'projector screen', 'divider', 'laundry detergent',
    'bathroom counter', 'object', 'bathroom vanity', 'closet wall',
    'laundry hamper', 'bathroom stall door', 'ceiling light', 'trash bin',
    'dumbbell', 'stair rail', 'tube', 'bathroom cabinet', 'cd case',
    'closet rod', 'coffee kettle', 'structure', 'shower head',
    'keyboard piano', 'case of water bottles', 'coat rack',
    'storage organizer', 'folded chair', 'fire alarm', 'power strip',
    'calendar', 'poster', 'potted plant', 'luggage', 'mattress'
]

# ======== æ•°æ®ç®¡é“ ========
train_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='DEPTH',
        shift_height=False,
        use_color=True,
        load_dim=6,
        use_dim=[0, 1, 2, 3, 4, 5]),
    dict(type='LoadClipFeature', data_root=DATA_ROOT),
    dict(type='LoadSingleImageFromFile'),
    dict(
        type='LoadAnnotations3D_',
        with_bbox_3d=False,
        with_label_3d=False,
        with_mask_3d=True,
        with_seg_3d=True,
        with_sp_mask_3d=True),
    dict(type='SwapChairAndFloor'),
    dict(type='PointSegClassMapping'),
    dict(
        type='RandomFlip3D',
        sync_2d=False,
        flip_ratio_bev_horizontal=0.5,
        flip_ratio_bev_vertical=0.5),
    dict(
        type='GlobalRotScaleTrans',
        rot_range=[-3.14, 3.14],
        scale_ratio_range=[0.8, 1.2],
        translation_std=[0.1, 0.1, 0.1],
        shift_height=False),
    dict(
        type='NormalizePointsColor_',
        color_mean=color_mean,
        color_std=color_std,
        clamp_range=[-3.0, 3.0]),
    dict(
        type='AddSuperPointAnnotations',
        num_classes=num_semantic_classes,
        stuff_classes=[0, 1],
        merge_non_stuff_cls=False),
    dict(
        type='ElasticTransfrom',
        gran=[6, 20],
        mag=[40, 160],
        voxel_size=0.02,
        p=0.2),  # ä»0.5é™åˆ°0.2ï¼Œå‡å°‘å¢å¼ºé¢‘ç‡
    dict(
        type='Pack3DDetInputs_',
        keys=[
            'points', 'imgs', 'cam_info', 'clip_pix', 'clip_global', 
            'gt_labels_3d', 'pts_semantic_mask', 'pts_instance_mask',
            'sp_pts_mask', 'gt_sp_masks', 'elastic_coords'
        ])
]

test_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='DEPTH',
        shift_height=False,
        use_color=True,
        load_dim=6,
        use_dim=[0, 1, 2, 3, 4, 5]),
    dict(type='LoadClipFeature', data_root=DATA_ROOT),
    dict(type='LoadSingleImageFromFile'),
    dict(
        type='LoadAnnotations3D_',
        with_bbox_3d=False,
        with_label_3d=False,
        with_mask_3d=True,
        with_seg_3d=True,
        with_sp_mask_3d=True),
    dict(type='SwapChairAndFloor'),
    dict(type='PointSegClassMapping'),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='NormalizePointsColor_',
                color_mean=color_mean,
                color_std=color_std,
                clamp_range=[-3.0, 3.0]),
            dict(
                type='AddSuperPointAnnotations',
                num_classes=num_semantic_classes,
                stuff_classes=[0, 1],
                merge_non_stuff_cls=False),
        ]),
    dict(
        type='Pack3DDetInputs_', 
        keys=['points', 'imgs', 'cam_info', 'clip_pix', 'clip_global', 'sp_pts_mask'])
]

# ======== æ¨¡å‹é…ç½® ========
model = dict(
    type='ScanNet200MixFormer3D',
    data_preprocessor=dict(type='Det3DDataPreprocessor_'),
    voxel_size=0.02,
    num_classes=num_instance_classes_eval,
    query_thr=0.5,
    
    # ä¼ ç»Ÿbackboneé…ç½®ï¼ˆä¸ºäº†æ»¡è¶³æ¨¡å‹åˆå§‹åŒ–è¦æ±‚ï¼Œå®é™…ä¸ä½¿ç”¨ï¼‰
    backbone=dict(
        type='Res16UNet34C',
        in_channels=3,
        out_channels=96,
        config=dict(
            dilations=[1, 1, 1, 1],
            conv1_kernel_size=5,
            bn_momentum=0.02)),
    
    # ä½¿ç”¨BiFusionEncoderæ›¿ä»£ä¼ ç»Ÿbackbone+neckç»„åˆ
    bi_encoder=dict(
        type='BiFusionEncoder',
        clip_pretrained='/home/nebula/xxy/ESAM/data/open_clip_pytorch_model.bin',
        voxel_size=0.02,
        
        # Enhanced CLIPé…ç½®
        clip_num_layers=6,              # ä½¿ç”¨å‰6å±‚Transformer
        freeze_clip_conv1=False,        # å…è®¸conv1å¾®è°ƒ
        freeze_clip_early_layers=True,  # å†»ç»“å‰3å±‚
        
        # Enhanced Gateé…ç½® - ä½¿ç”¨LiteFusionGate 
        use_enhanced_gate=False,        # ç¦ç”¨å¤æ‚Gateï¼Œä½¿ç”¨é»˜è®¤LiteFusionGate
        use_spatial_attention=False,    # å¯¹åº”ç¦ç”¨ç©ºé—´æ³¨æ„åŠ›
        spatial_k=16,                   # Kè¿‘é‚»æ•°é‡ï¼ˆæš‚ä¸ä½¿ç”¨ï¼‰
        
        # å…³é”®ï¼šç§»é™¤TinySAä¾èµ–
        use_tiny_sa_2d=False,           # ç¦ç”¨2D TinySA
        use_tiny_sa_3d=False,           # ç¦ç”¨3D TinySA
        
        # ğŸ”¥ Stage 2: ç»Ÿè®¡æ”¶é›†é…ç½®
        _collect_fusion_stats=True,     # å¯ç”¨èåˆç»Ÿè®¡æ”¶é›†
        collect_projection_stats=True,  # å¯ç”¨æŠ•å½±ç»Ÿè®¡æ”¶é›†
        
        # å…¶ä»–é…ç½®
        use_amp=True,
        freeze_blocks=0,
    ),
    
    # å‡ ä½•æ„ŸçŸ¥æ± åŒ– - ä½¿ç”¨256ç»´åŒ¹é…BiFusionEncoderè¾“å‡º
    pool=dict(type='GeoAwarePooling', channel_proj=256),
    
    # æŸ¥è¯¢è§£ç å™¨ - ä½¿ç”¨256ç»´è¾“å…¥
    decoder=dict(
        type='ScanNetMixQueryDecoder',
        num_layers=3,
        share_attn_mlp=False, 
        share_mask_mlp=False,
        cross_attn_mode=["", "SP", "SP", "SP"], 
        mask_pred_mode=["SP", "SP", "P", "P"],
        num_instance_queries=0,
        num_semantic_queries=0,
        num_instance_classes=num_instance_classes,
        num_semantic_classes=num_semantic_classes,
        num_semantic_linears=1,
        in_channels=256,  # åŒ¹é…BiFusionEncoderè¾“å‡ºç»´åº¦
        d_model=256,
        num_heads=8,
        hidden_dim=1024,
        dropout=0.0,
        activation_fn='gelu',
        iter_pred=True,
        attn_mask=True,
        fix_attention=True,
        objectness_flag=False),
    
    # EnhancedæŸå¤±å‡½æ•°é…ç½®
    criterion=dict(
        type='ScanNetMixedCriterion',
        num_semantic_classes=num_semantic_classes,
        sem_criterion=dict(
            type='ScanNetSemanticCriterion',
            ignore_index=num_semantic_classes,
            loss_weight=0.4),  # åŠ¨æ€è°ƒæ•´ï¼Œè¿™é‡Œæ˜¯åŸºå‡†å€¼
        inst_criterion=dict(
            type='MixedInstanceCriterion',
            matcher=dict(
                type='SparseMatcher',
                costs=[
                    dict(type='QueryClassificationCost', weight=0.5),
                    dict(type='MaskBCECost', weight=1.0),
                    dict(type='MaskDiceCost', weight=1.0)],
                topk=1),
            bbox_loss=dict(type='AxisAlignedIoULoss'),
            loss_weight=[1.0, 0.8, 0.6, 0.3, 0.0],  # [åˆ†ç±», BCE, Dice, Score, BBoxç§»é™¤]
            num_classes=num_instance_classes,
            non_object_weight=0.05,  # é™ä½èƒŒæ™¯æƒé‡ï¼Œçªå‡ºå‰æ™¯
            fix_dice_loss_weight=True,
            iter_matcher=True,
            fix_mean_loss=True)),
    
    # ï¿½ ä¼˜åŒ–åï¼šå¯ç”¨å¢å¼ºç‰ˆCLIPä¸€è‡´æ€§æŸå¤±
    clip_criterion=dict(
        type='ClipConsCriterion',
        loss_weight=0.1,              # ğŸ”¥ å¯ç”¨CLIPæŸå¤±ï¼Œæƒé‡0.1
        temperature=0.07,             # âœ… ä¼˜åŒ–æ¸©åº¦å‚æ•°ï¼Œæ›´å¥½çš„å¯¹æ¯”å­¦ä¹ 
        gradient_flow_ratio=0.05,     # âœ… 5%æ¢¯åº¦æµæ§åˆ¶ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
    ),
    
    # ğŸ”¥ å¯ç”¨è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæå‡ç‰¹å¾è´¨é‡
    auxiliary_losses=dict(
        spatial_consistency=dict(
            type='SpatialConsistencyLoss',
            loss_weight=0.02,          # ğŸ”¥ å¯ç”¨ç©ºé—´ä¸€è‡´æ€§æŸå¤±
            k_neighbors=8              # KNNé‚»å±…æ•°é‡
        ),
        # no_view_supervision=dict(    # æš‚æ—¶ä¿æŒç¦ç”¨
        #     type='NoViewSupervisionLoss', 
        #     loss_weight=0.01,
        #     confidence_threshold=0.8
        # )
    ),
    
    train_cfg=dict(),
    test_cfg=dict(
        topk_insts=100,
        inst_score_thr=0.0,
        pan_score_thr=0.5,
        npoint_thr=100,
        obj_normalization=True,
        sp_score_thr=0.4,
        nms=True,
        matrix_nms_kernel='linear',
        stuff_classes=[0, 1]))

# ======== æ•°æ®é›†é…ç½® ========
dataset_type = 'ScanNet200SegDataset_'
data_prefix = dict(
    pts='points',
    pts_instance_mask='instance_mask',
    pts_semantic_mask='semantic_mask',
    sp_pts_mask='super_points')

train_dataloader = dict(
    batch_size=6,           # æå‡åˆ°6ï¼Œåˆ©ç”¨ä¼˜åŒ–åçš„æ˜¾å­˜æ•ˆç‡
    num_workers=8,          # å‡å°‘workerï¼Œé™ä½å†…å­˜å‹åŠ›
    persistent_workers=True,
    # pin_memory=True,      # ä¿æŒæ³¨é‡Šï¼Œé¿å…å†…å­˜å‹åŠ›
    prefetch_factor=2,      # é™ä½é¢„å–ï¼Œè¿›ä¸€æ­¥èŠ‚çœå†…å­˜
    dataset=dict(
        type=dataset_type,
        data_root=DATA_ROOT,
        ann_file='scannet200_sv_oneformer3d_infos_train_clip.pkl',
        data_prefix=data_prefix,
        metainfo=dict(classes=class_names),
        pipeline=train_pipeline,
        ignore_index=num_semantic_classes,
        scene_idxs=None,
        test_mode=False))

val_dataloader = dict(
    dataset=dict(
        type=dataset_type,
        data_root=DATA_ROOT,
        ann_file='scannet200_sv_oneformer3d_infos_val_clip.pkl',
        data_prefix=data_prefix,
        metainfo=dict(classes=class_names),
        pipeline=test_pipeline,
        ignore_index=num_semantic_classes,
        test_mode=True))

test_dataloader = val_dataloader

# ======== è¯„ä¼°é…ç½® ========
label2cat = {i: name for i, name in enumerate(class_names + ['unlabeled'])}
metric_meta = dict(
    label2cat=label2cat,
    ignore_index=[num_semantic_classes],
    classes=class_names + ['unlabeled'])

sem_mapping = [
    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23,
    24, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 44, 45, 46,
    47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68,
    69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 84, 86, 87, 88, 89, 90,
    93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 110, 112,
    115, 116, 118, 120, 121, 122, 125, 128, 130, 131, 132, 134, 136, 138, 139,
    140, 141, 145, 148, 154, 155, 156, 157, 159, 161, 163, 165, 166, 168, 169,
    170, 177, 180, 185, 188, 191, 193, 195, 202, 208, 213, 214, 221, 229, 230,
    232, 233, 242, 250, 261, 264, 276, 283, 286, 300, 304, 312, 323, 325, 331,
    342, 356, 370, 392, 395, 399, 408, 417, 488, 540, 562, 570, 572, 581, 609,
    748, 776, 1156, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172,
    1173, 1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,
    1186, 1187, 1188, 1189, 1190, 1191
]
inst_mapping = sem_mapping[2:]

val_evaluator = dict(
    type='UnifiedSegMetric',
    stuff_class_inds=[0, 1], 
    thing_class_inds=list(range(2, num_semantic_classes)),
    min_num_points=1, 
    id_offset=2**16,
    sem_mapping=sem_mapping,
    inst_mapping=inst_mapping,
    metric_meta=metric_meta)
test_evaluator = val_evaluator

# ======== è®­ç»ƒé…ç½® ========
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='AdamW', lr=0.00005, weight_decay=0.01),  # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡
    clip_grad=dict(max_norm=1.0, norm_type=2),  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
    # å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘æ˜¾å­˜ä½¿ç”¨
    accumulative_counts=4)  # å¢åŠ ç´¯ç§¯ï¼š4Ã—4=16ç­‰æ•ˆbatch_size

param_scheduler = dict(type='PolyLR', begin=0, end=128, power=0.9)

custom_hooks = [
    dict(type='EmptyCacheHook', after_iter=True),
    
    # ï¿½ å¢å¼ºè®­ç»ƒç›‘æ§Hook - è¯¦ç»†æŸå¤±ã€fusion gateã€æŠ•å½±ç‡ã€æ¢¯åº¦ç›‘æ§
    dict(
        type='EnhancedTrainingHook',
        log_interval=10,              # æ¯10ä¸ªiterè¾“å‡ºè¯¦ç»†ç»Ÿè®¡
        grad_monitor_interval=50,     # æ¯50ä¸ªiterç›‘æ§æ¢¯åº¦å¥åº·åº¦
        detailed_stats=True           # å¯ç”¨è¯¦ç»†ç»Ÿè®¡
    ),
    
    # ğŸ” åŸæœ‰è¯¦ç»†æŸå¤±ç›‘æ§Hook (ä¿ç•™å…¼å®¹æ€§)
    dict(
        type='DetailedLossMonitorHook',
        log_interval=20,              # é™ä½é¢‘ç‡é¿å…é‡å¤
        collect_grad_norm=True,
        collect_fusion_stats=True,
        collect_clip_stats=True
    ),
    
    # ğŸš¨ NaNæ£€æµ‹Hook
    dict(
        type='NaNDetectionHook',
        check_interval=5  # æ¯5ä¸ªiteræ£€æµ‹ä¸€æ¬¡NaN
    ),
    
    # å…³é”®ä¿®å¤ï¼šåŠ è½½3Dé¢„è®­ç»ƒæƒé‡åˆ°BiFusionçš„backbone3d
    dict(
        type='PartialLoadHook',
        pretrained='/home/nebula/xxy/ESAM/work_dirs/sv3d_scannet200_ca/best_all_ap_50%_epoch_128.pth',
        submodule='bi_encoder.backbone3d',
        prefix_replace=('backbone\\.', 'bi_encoder.backbone3d.'),  # æ­£ç¡®æ˜ å°„ï¼šbackbone.xxx -> bi_encoder.backbone3d.xxx
        strict=False
    )
]
default_hooks = dict(
    checkpoint=dict(
        interval=1,
        max_keep_ckpts=1,
        save_best=['all_ap_50%'],
        rule='greater'),
    logger=dict(
        type='LoggerHook', 
        interval=10,  # æ¯10ä¸ªiterè¾“å‡ºä¸€æ¬¡ï¼Œæ›´é¢‘ç¹çš„ç›‘æ§
        log_metric_by_epoch=False,  # æŒ‰iterè¾“å‡ºï¼Œä¾¿äºè§‚å¯Ÿè®­ç»ƒç¨³å®šæ€§
        out_suffix='.log'
    ))

# ======== é¢„è®­ç»ƒæƒé‡åŠ è½½ ========
# ç§»é™¤å…¨å±€load_fromï¼Œæ”¹ç”¨PartialLoadHookç²¾ç¡®åŠ è½½3Dæƒé‡åˆ°bi_encoder.backbone3d
# load_from = '/home/nebula/xxy/ESAM/work_dirs/sv3d_scannet200_ca/best_all_ap_50%_epoch_128.pth'
# ======== è®­ç»ƒè°ƒåº¦ - æ¯5epochè¯„ä¼° ========
train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=128, val_interval=5)  # ğŸ”¥ æ¯5epochè¯„ä¼°
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')

# ======== å¢å¼ºæ—¥å¿—é…ç½® - è¯¦ç»†æŸå¤±ã€fusion gateã€æŠ•å½±ç‡è¾“å‡º ========
log_processor = dict(type='LogProcessor', window_size=1, by_epoch=True)

# ï¿½ å¢å¼ºæ—¥å¿—é…ç½® - è¾“å‡ºæ‰€æœ‰æŸå¤±å’ŒBiFusionç»Ÿè®¡ä¿¡æ¯
log_config = dict(
    interval=5,   # æ¯5ä¸ªiterè¾“å‡ºä¸€æ¬¡åŸºç¡€æ—¥å¿— 
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook', by_epoch=False)
    ])

# ğŸ”¥ å¯è§†åŒ–é…ç½® - ç¦ç”¨TensorBoardè§£å†³Python 3.8å…¼å®¹æ€§é—®é¢˜
vis_backends = [
    dict(type='LocalVisBackend'),
    # dict(type='TensorboardVisBackend',  # æš‚æ—¶ç¦ç”¨TensorBoard
    #      save_dir='./work_dirs/tensorboard_logs')
]
visualizer = dict(
    type='Det3DLocalVisualizer',
    vis_backends=vis_backends,
    name='visualizer') 