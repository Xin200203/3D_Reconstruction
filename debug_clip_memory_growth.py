#!/usr/bin/env python3
"""
è°ƒè¯•CLIPå‚æ•°ä¸AdamWä¼˜åŒ–å™¨å†…å­˜å¢é•¿çš„å…³ç³»
åˆ†æ8ä¸ªepochåOOMçš„æ ¹æœ¬åŸå› 
"""

import torch
import torch.nn as nn
from typing import Dict, List
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/home/nebula/xxy/ESAM')

def analyze_adamw_memory_growth():
    """åˆ†æAdamWä¼˜åŒ–å™¨å†…å­˜å¢é•¿æ¨¡å¼"""
    
    print("=" * 60)
    print("ğŸ” åˆ†æAdamWä¼˜åŒ–å™¨å†…å­˜åˆ†é…æœºåˆ¶")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿä¸åŒå¤§å°çš„å‚æ•°ç»„
    param_sizes = [
        ('å°å‹ç½‘ç»œ (1Må‚æ•°)', 1_000_000),
        ('ä¸­å‹ç½‘ç»œ (10Må‚æ•°)', 10_000_000), 
        ('CLIP ViT-B/16 (86Må‚æ•°)', 86_000_000),
        ('å®Œæ•´BiFusion (200Må‚æ•°)', 200_000_000)
    ]
    
    for name, num_params in param_sizes:
        # åˆ›å»ºè™šæ‹Ÿå‚æ•°
        param = nn.Parameter(torch.randn(num_params, dtype=torch.float32))
        
        # åˆ›å»ºAdamWä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW([param], lr=0.001)
        
        # è®¡ç®—å†…å­˜å ç”¨
        param_memory = param.numel() * param.element_size() / (1024**3)  # GB
        
        # AdamWä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¸¤ä¸ªçŠ¶æ€ï¼šmomentum (exp_avg) å’Œ variance (exp_avg_sq)
        state_memory = param_memory * 2  # ä¸¤ä¸ªfloat32ç¼“å†²åŒº
        total_memory = param_memory + state_memory
        
        print(f"\nğŸ“Š {name}:")
        print(f"   å‚æ•°å†…å­˜: {param_memory:.2f} GB")
        print(f"   çŠ¶æ€å†…å­˜: {state_memory:.2f} GB (momentum + variance)")
        print(f"   æ€»è®¡å†…å­˜: {total_memory:.2f} GB")
        
        del param, optimizer
    
    print("\n" + "=" * 60)

def analyze_clip_parameter_groups():
    """åˆ†æCLIPå‚æ•°åˆ†ç»„å’Œé€æ­¥è§£å†»æ¨¡å¼"""
    
    print("ğŸ” åˆ†æCLIPå‚æ•°åˆ†ç»„ç­–ç•¥")
    print("=" * 60)
    
    # æ¨¡æ‹ŸCLIP ViT-B/16ç»“æ„
    layer_groups = {
        'patch_embedding': 590_592,     # conv1 + pos_embed + class_embed
        'ln_pre': 1_536,               # layer norm
        'transformer_layer_0': 7_077_888,  # æ¯å±‚çº¦7Må‚æ•°
        'transformer_layer_1': 7_077_888,
        'transformer_layer_2': 7_077_888,
        'transformer_layer_3': 7_077_888,
        'transformer_layer_4': 7_077_888,
        'transformer_layer_5': 7_077_888,
        'transformer_layer_6': 7_077_888,
        'transformer_layer_7': 7_077_888,
        'transformer_layer_8': 7_077_888,
        'transformer_layer_9': 7_077_888,
        'transformer_layer_10': 7_077_888,
        'transformer_layer_11': 7_077_888,
        'ln_post': 1_536,              # final layer norm
    }
    
    print(f"\nğŸ“‹ CLIP ViT-B/16 å‚æ•°åˆ†ç»„:")
    total_params = 0
    for group, size in layer_groups.items():
        total_params += size
        print(f"   {group:25s}: {size:>10,} å‚æ•°")
    
    print(f"\n   {'æ€»è®¡':25s}: {total_params:>10,} å‚æ•°")
    
    # åˆ†æç°æœ‰å†»ç»“ç­–ç•¥
    print(f"\nğŸ”§ å½“å‰å†»ç»“ç­–ç•¥åˆ†æ:")
    print(f"   freeze_clip_early_layers=True (å†»ç»“å‰3å±‚)")
    print(f"   clip_num_layers=6 (ä½¿ç”¨å‰6å±‚)")
    
    frozen_params = (layer_groups['transformer_layer_0'] + 
                    layer_groups['transformer_layer_1'] + 
                    layer_groups['transformer_layer_2'])
    
    trainable_params = (layer_groups['patch_embedding'] +  # conv1ä¸å†»ç»“
                       layer_groups['ln_pre'] +
                       layer_groups['transformer_layer_3'] + 
                       layer_groups['transformer_layer_4'] + 
                       layer_groups['transformer_layer_5'])
    
    print(f"\n   å†»ç»“å‚æ•°:   {frozen_params:>10,} ({frozen_params/total_params*100:.1f}%)")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:>10,} ({trainable_params/total_params*100:.1f}%)")
    
    # è®¡ç®—AdamWçŠ¶æ€å†…å­˜
    trainable_memory = trainable_params * 4 / (1024**3)  # float32, GB
    optimizer_state_memory = trainable_memory * 2  # momentum + variance
    
    print(f"\nğŸ’¾ å†…å­˜å ç”¨åˆ†æ:")
    print(f"   å¯è®­ç»ƒå‚æ•°å†…å­˜: {trainable_memory:.2f} GB")
    print(f"   ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜: {optimizer_state_memory:.2f} GB")
    print(f"   CLIPæ€»é¢å¤–å†…å­˜: {trainable_memory + optimizer_state_memory:.2f} GB")
    
    return trainable_params, optimizer_state_memory

def simulate_epoch_by_epoch_memory():
    """æ¨¡æ‹Ÿé€epochçš„å†…å­˜å¢é•¿"""
    
    print("ğŸ•’ æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹å†…å­˜å˜åŒ–")
    print("=" * 60)
    
    # åŸºç¡€æ¨¡å‹å†…å­˜å ç”¨ (å‡è®¾)
    base_model_memory = 12.0  # GB
    base_optimizer_memory = 8.0  # GB for 3D backbone + other params
    
    # CLIPåˆ†æ”¯å†…å­˜ (ä»ä¸Šé¢çš„åˆ†æå¾—å‡º)
    _, clip_optimizer_memory = analyze_clip_parameter_groups()
    
    print(f"\nğŸ“ˆ é€Epochå†…å­˜å˜åŒ–æ¨¡æ‹Ÿ:")
    print(f"   åŸºç¡€æ¨¡å‹:     {base_model_memory:.1f} GB")
    print(f"   åŸºç¡€ä¼˜åŒ–å™¨:   {base_optimizer_memory:.1f} GB")
    print(f"   CLIPä¼˜åŒ–å™¨:   {clip_optimizer_memory:.1f} GB (ä¸€æ¬¡æ€§åˆ†é…)")
    
    total_memory = base_model_memory + base_optimizer_memory + clip_optimizer_memory
    
    for epoch in range(1, 10):
        print(f"\n   Epoch {epoch:2d}: {total_memory:.1f} GB")
        if epoch == 8:
            print(f"             âš ï¸  æ¥è¿‘24GBé™åˆ¶! å¯èƒ½è§¦å‘OOM")
        
        # æ¨¡æ‹Ÿå†…å­˜ç¢ç‰‡åŒ–çš„é€æ­¥å¢é•¿
        if epoch > 5:
            total_memory += 0.1  # å†…å­˜ç¢ç‰‡åŒ–

def analyze_solution_strategies():
    """åˆ†æè§£å†³æ–¹æ¡ˆç­–ç•¥"""
    
    print("ğŸ¯ è§£å†³æ–¹æ¡ˆç­–ç•¥åˆ†æ")
    print("=" * 60)
    
    strategies = [
        ("é™ä½batch_size", "ä»20->8->6", "ç«‹å³å‡å°‘4-6GB"),
        ("æ¢¯åº¦ç´¯ç§¯", "accumulative_counts=2-3", "è¡¥å¿batch_sizeé™ä½"),
        ("æ›´æ¿€è¿›çš„CLIPå†»ç»“", "åªè®­ç»ƒæœ€å1-2å±‚", "å‡å°‘50%ä¼˜åŒ–å™¨å†…å­˜"),
        ("åˆ†é˜¶æ®µè®­ç»ƒ", "å…ˆè®­3Dï¼Œå†è®­CLIP", "é¿å…åŒæ—¶ä¼˜åŒ–"),
        ("æ··åˆç²¾åº¦", "å¯ç”¨AMP", "å‡å°‘30-50%å†…å­˜"),
        ("æ¢¯åº¦æ£€æŸ¥ç‚¹", "é‡è®¡ç®—vså­˜å‚¨", "å‡å°‘æ¿€æ´»å†…å­˜"),
    ]
    
    print(f"\nğŸ’¡ å¯é€‰ç­–ç•¥:")
    for i, (strategy, implementation, benefit) in enumerate(strategies, 1):
        print(f"   {i}. {strategy:20s}: {implementation:20s} -> {benefit}")
        
    print(f"\nğŸ† æ¨èç­–ç•¥ç»„åˆ:")
    print(f"   1. ç«‹å³: batch_size=6 + accumulative_counts=3")
    print(f"   2. ä¸­æœŸ: æ›´æ¿€è¿›CLIPå†»ç»“ (åªè®­ç»ƒå±‚5)")  
    print(f"   3. é•¿æœŸ: åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥")

if __name__ == "__main__":
    try:
        # åˆ†æAdamWå†…å­˜æœºåˆ¶
        analyze_adamw_memory_growth()
        
        print("\n" + "="*80 + "\n")
        
        # åˆ†æCLIPå‚æ•°ç­–ç•¥
        analyze_clip_parameter_groups()
        
        print("\n" + "="*80 + "\n")
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        simulate_epoch_by_epoch_memory()
        
        print("\n" + "="*80 + "\n")
        
        # è§£å†³æ–¹æ¡ˆåˆ†æ
        analyze_solution_strategies()
        
        print(f"\nğŸ“ å…³é”®ç»“è®º:")
        print(f"   â€¢ 8ä¸ªepoch OOMçš„æ ¹æœ¬åŸå› : AdamWä¸ºæ‰€æœ‰å¯è®­ç»ƒCLIPå‚æ•°é¢„åˆ†é…äº†çŠ¶æ€å†…å­˜")
        print(f"   â€¢ è¿™ä¸æ˜¯å†…å­˜æ³„æ¼ï¼Œè€Œæ˜¯æ­£å¸¸çš„ä¼˜åŒ–å™¨è¡Œä¸º")
        print(f"   â€¢ åˆ†æ®µåŠ è½½æ¨¡å‹ä¸æ˜¯ç›´æ¥åŸå› ï¼Œä½†å¢åŠ äº†æ€»å‚æ•°é‡")
        print(f"   â€¢ è§£å†³æ–¹æ¡ˆ: é™ä½batch_size + æ›´æ¿€è¿›çš„å‚æ•°å†»ç»“ç­–ç•¥")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 