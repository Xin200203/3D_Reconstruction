#!/usr/bin/env python3
"""
ä¿®å¤QueryDecoderåŠ¨æ€åˆå§‹åŒ–å¯¼è‡´çš„æƒé‡åŠ è½½é—®é¢˜
æ‰‹åŠ¨å°†input_projçš„æƒé‡ä»checkpointåŠ è½½åˆ°åŠ¨æ€åˆ›å»ºçš„å±‚ä¸­
"""

import torch
import sys
import os
sys.path.insert(0, '/home/nebula/xxy/ESAM')

def fix_decoder_weights():
    """ä¿®å¤decoderæƒé‡åŠ è½½é—®é¢˜"""
    
    print("ğŸ”§ ä¿®å¤QueryDecoderæƒé‡åŠ è½½é—®é¢˜")
    print("=" * 50)
    
    # åŠ è½½checkpoint
    checkpoint_path = "/home/nebula/xxy/ESAM/work_dirs/enhanced_bifusion_debug/epoch_49.pth"
    print(f"ğŸ“‚ åŠ è½½checkpoint: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint['state_dict']
    
    # æ£€æŸ¥decoderç›¸å…³çš„é”®
    decoder_keys = [k for k in state_dict.keys() if 'decoder' in k]
    input_proj_keys = [k for k in decoder_keys if 'input_proj' in k]
    
    print(f"\nğŸ” å‘ç°çš„decoder keys: {len(decoder_keys)}")
    print(f"ğŸ” å‘ç°çš„input_proj keys: {len(input_proj_keys)}")
    
    for key in input_proj_keys:
        print(f"   {key}: {state_dict[key].shape}")
    
    # åˆ†æinput_projç»“æ„
    if input_proj_keys:
        # å‡è®¾æ˜¯Sequential(Linear, LayerNorm, ReLU)ç»“æ„
        linear_weight = None
        linear_bias = None
        norm_weight = None
        norm_bias = None
        
        for key in input_proj_keys:
            if 'input_proj.0.weight' in key:
                linear_weight = state_dict[key]
            elif 'input_proj.0.bias' in key:
                linear_bias = state_dict[key]
            elif 'input_proj.1.weight' in key:
                norm_weight = state_dict[key]
            elif 'input_proj.1.bias' in key:
                norm_bias = state_dict[key]
        
        if linear_weight is not None:
            in_features = linear_weight.shape[1]
            out_features = linear_weight.shape[0]
            print(f"\nğŸ“Š input_projç»“æ„åˆ†æ:")
            print(f"   Linear: {in_features} -> {out_features}")
            if norm_weight is not None:
                print(f"   LayerNorm: {norm_weight.shape[0]} features")
            
            # åˆ›å»ºä¿®å¤é…ç½®
            fix_config = {
                'expected_in_channels': in_features,
                'd_model': out_features,
                'linear_weight': linear_weight,
                'linear_bias': linear_bias,
                'norm_weight': norm_weight,
                'norm_bias': norm_bias,
            }
            
            print(f"\nğŸ’¾ ä¿å­˜ä¿®å¤é…ç½®åˆ° decoder_fix_config.pth")
            torch.save(fix_config, '/home/nebula/xxy/ESAM/decoder_fix_config.pth')
            
            return fix_config
    
    return None

def create_fixed_test_script():
    """åˆ›å»ºä¿®å¤åçš„æµ‹è¯•è„šæœ¬"""
    
    script_content = '''#!/usr/bin/env python3
"""
ä¿®å¤QueryDecoderæƒé‡åçš„æµ‹è¯•è„šæœ¬
"""

import torch
import torch.nn as nn
import sys
import os
sys.path.insert(0, '/home/nebula/xxy/ESAM')

from mmengine.config import Config
from mmengine.runner import Runner

def patch_query_decoder():
    """ç»™QueryDecoderæ‰“è¡¥ä¸ï¼Œæ­£ç¡®åŠ è½½input_projæƒé‡"""
    
    # åŠ è½½ä¿®å¤é…ç½®
    if os.path.exists('/home/nebula/xxy/ESAM/decoder_fix_config.pth'):
        fix_config = torch.load('/home/nebula/xxy/ESAM/decoder_fix_config.pth', map_location='cpu')
        print(f"ğŸ”§ åŠ è½½ä¿®å¤é…ç½®: {fix_config['expected_in_channels']} -> {fix_config['d_model']}")
        
        # è·å–åŸå§‹QueryDecoderç±»
        from oneformer3d.query_decoder import QueryDecoder
        original_forward_iter_pred = QueryDecoder.forward_iter_pred
        
        def patched_forward_iter_pred(self, sp_feats, p_feats, queries, super_points, prev_queries=None):
            """ä¿®å¤åçš„forwardæ–¹æ³•"""
            
            # å¦‚æœinput_projæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–
            if self.input_proj is None and sp_feats and "SP" in self.cross_attn_mode:
                print(f"ğŸ”§ ä½¿ç”¨é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–input_proj")
                
                # åˆ›å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç»“æ„
                self.input_proj = nn.Sequential(
                    nn.Linear(fix_config['expected_in_channels'], fix_config['d_model']),
                    nn.LayerNorm(fix_config['d_model']),
                    nn.ReLU()
                ).to(sp_feats[0].device)
                
                # åŠ è½½é¢„è®­ç»ƒæƒé‡
                with torch.no_grad():
                    self.input_proj[0].weight.copy_(fix_config['linear_weight'])
                    self.input_proj[0].bias.copy_(fix_config['linear_bias'])
                    if fix_config['norm_weight'] is not None:
                        self.input_proj[1].weight.copy_(fix_config['norm_weight'])
                        self.input_proj[1].bias.copy_(fix_config['norm_bias'])
                
                print(f"âœ… input_projæƒé‡åŠ è½½å®Œæˆ")
            
            # è°ƒç”¨åŸå§‹æ–¹æ³•
            return original_forward_iter_pred(self, sp_feats, p_feats, queries, super_points, prev_queries)
        
        # åº”ç”¨è¡¥ä¸
        QueryDecoder.forward_iter_pred = patched_forward_iter_pred
        print("ğŸ”§ QueryDecoderè¡¥ä¸å·²åº”ç”¨")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°ä¿®å¤é…ç½®ï¼Œè·³è¿‡æƒé‡ä¿®å¤")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹ä¿®å¤åçš„æµ‹è¯•")
    
    # åº”ç”¨è¡¥ä¸
    patch_query_decoder()
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['PYTHONPATH'] = '/home/nebula/xxy/ESAM:' + os.environ.get('PYTHONPATH', '')
    
    # åŠ è½½é…ç½®
    config_path = 'configs/ESAM_CA/sv_bifusion_scannet200.py'
    checkpoint_path = '/home/nebula/xxy/ESAM/work_dirs/enhanced_bifusion_debug/epoch_49.pth'
    work_dir = 'work_dirs/test_epoch49_ca_fixed'
    
    print(f"ğŸ“‚ é…ç½®æ–‡ä»¶: {config_path}")
    print(f"ğŸ¯ æ¨¡å‹æ–‡ä»¶: {checkpoint_path}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {work_dir}")
    
    # åŠ è½½é…ç½®
    cfg = Config.fromfile(config_path)
    
    # è®¾ç½®CAæ¨¡å¼
    cfg.test_evaluator.eval_mode = 'cat_agnostic'
    cfg.work_dir = work_dir
    cfg.load_from = checkpoint_path
    
    # åˆ›å»ºrunnerå¹¶æµ‹è¯•
    runner = Runner.from_cfg(cfg)
    runner.test()
    
    print("âœ… ä¿®å¤åçš„æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()
'''
    
    with open('/home/nebula/xxy/ESAM/test_with_decoder_fix.py', 'w') as f:
        f.write(script_content)
    
    print("ğŸ“ åˆ›å»ºä¿®å¤æµ‹è¯•è„šæœ¬: test_with_decoder_fix.py")

if __name__ == "__main__":
    try:
        # åˆ†ææƒé‡é—®é¢˜
        fix_config = fix_decoder_weights()
        
        if fix_config:
            # åˆ›å»ºä¿®å¤è„šæœ¬
            create_fixed_test_script()
            
            print(f"\nğŸ¯ ä¸‹ä¸€æ­¥æ“ä½œ:")
            print(f"cd /home/nebula/xxy/ESAM")
            print(f"conda activate ESAM")
            print(f"python test_with_decoder_fix.py")
            
        else:
            print(f"âŒ æœªæ‰¾åˆ°input_projæƒé‡ï¼Œæ— æ³•ä¿®å¤")
            
    except Exception as e:
        print(f"âŒ ä¿®å¤è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 