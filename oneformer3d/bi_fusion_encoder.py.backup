import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
import contextlib
from typing import List, Dict, Optional, Tuple, cast

import MinkowskiEngine as ME
from mmdet3d.registry import MODELS
from .mink_unet import Res16UNet34C
from types import SimpleNamespace


class EnhancedProjectionHead3D(nn.Module):
    """ç®€åŒ–çš„3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´
    
    æŒ‰ç…§ä¼˜åŒ–æŒ‡å—è¦æ±‚ï¼šLinear(96â†’256) + LayerNorm
    """
    
    def __init__(self,
                 input_dim: int = 96,
                 output_dim: int = 256):
        super().__init__()
        
        # ç®€åŒ–æŠ•å½±ï¼šå•å±‚Linear + LayerNorm
        self.projection = nn.Sequential(
            nn.Linear(input_dim, output_dim),        # èåˆç‰¹å¾
            nn.LayerNorm(output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """3Dç‰¹å¾æŠ•å½± (N, 96) -> (N, 256)"""
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.any(torch.isnan(x)) or torch.any(torch.isinf(x)):
            print("Warning: NaN/Inf in 3D projection input, clamping")
            x = torch.clamp(x, -10, 10)
        return self.projection(x)


class MaskedSE1D(nn.Module):
    """æ©ç åŒ–SEæ¨¡å— - åªç»Ÿè®¡æœ‰æ•ˆç‚¹çš„é€šé“å‡å€¼"""
    def __init__(self, C, r=16):
        super().__init__()
        self.excite = nn.Sequential(
            nn.Conv1d(C, C//r, 1), nn.ReLU(),
            nn.Conv1d(C//r, C, 1), nn.Sigmoid()
        )
    
    def forward(self, x, valid_mask): 
        # x: (B, C, N), valid_mask: (B, N)
        m = valid_mask.unsqueeze(1).float()             # (B,1,N)
        s = (x * m).sum(-1, keepdim=True)               # (B,C,1)  æœ‰æ•ˆç‚¹çš„é€šé“æ±‚å’Œ
        cnt = m.sum(-1, keepdim=True).clamp_min(1.0)    # (B,1,1)  æœ‰æ•ˆç‚¹è®¡æ•°
        z = s / cnt                                     # (B,C,1)  æ©ç åŒ–å‡å€¼
        w = self.excite(z)                              # (B,C,1)
        return x * w                                    # é€šé“é‡åŠ æƒ


class Head(nn.Module):
    """ç»Ÿä¸€çš„Headç»“æ„ - 2D/3Dåˆ†æ”¯å¯¹ç§°ä½¿ç”¨"""
    def __init__(self, dim=256, hidden=256, p=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden),
            nn.ReLU(),
            nn.Dropout(p),
            nn.Linear(hidden, dim),
            nn.LayerNorm(dim)
        )
    def forward(self, x):
        return self.net(x)


class LiteFusionGate(nn.Module):
    """Lite Fusion Gate - è½»é‡çº§èåˆé—¨æ§æœºåˆ¶
    
    ç®€åŒ–ç‰ˆæœ¬ï¼šç‚¹çº§èåˆ + æ©ç åŒ–SEé€šé“æ³¨æ„åŠ›ï¼Œç§»é™¤åˆ†é˜¶æ®µè®­ç»ƒé€»è¾‘
    å‚æ•°é‡çº¦0.12Mï¼Œè¿œä½äºåŸEnhancedGate
    """
    
    def __init__(self, 
                 feat_dim: int = 256,
                 use_masked_se: bool = True):
        super().__init__()
        
        self.feat_dim = feat_dim
        self.use_masked_se = use_masked_se
        
        # ç‚¹çº§èåˆæƒé‡MLP: Linear(512â†’64â†’1) + Sigmoid
        self.point_mlp = nn.Sequential(
            nn.Linear(feat_dim * 2, 64),  # 256*2 -> 64
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # ğŸ”§ åˆå§‹åŒ–æœ€åä¸€å±‚biasä¸º0ï¼Œä½¿åˆå§‹alphaâ‰ˆ0.5
        nn.init.constant_(self.point_mlp[-2].bias, 0.0)
        
        # æ©ç åŒ–SEé€šé“æ³¨æ„åŠ›æ¨¡å—
        if use_masked_se:
            self.se_masked = MaskedSE1D(feat_dim, r=16)
        else:
            # åŸç‰ˆSEæ¨¡å—ï¼ˆå¤‡ç”¨ï¼‰
            self.se_module = nn.Sequential(
                nn.AdaptiveAvgPool1d(1),
                nn.Conv1d(feat_dim, feat_dim // 16, 1),
                nn.ReLU(),
                nn.Conv1d(feat_dim // 16, feat_dim, 1),
                nn.Sigmoid()
            )
        
    def forward(self, 
                f2d: torch.Tensor, 
                f3d: torch.Tensor, 
                valid_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            f2d: 2Dç‰¹å¾ (B, N, 256)
            f3d: 3Dç‰¹å¾ (B, N, 256) 
            valid_mask: æœ‰æ•ˆæŠ•å½±æ©ç  (B, N)
        Returns:
            fused_feat: èåˆç‰¹å¾ (B, N, 256)
            confidence: èåˆç½®ä¿¡åº¦ (B, N, 1)
        """
        B, N, C = f2d.shape
        
        # 1. è®¡ç®—ç‚¹çº§èåˆæƒé‡Î± - ç›´æ¥ä½¿ç”¨MLPè®¡ç®—ï¼Œä¸å†æœ‰åˆ†é˜¶æ®µé€»è¾‘
        feat_concat = torch.cat([f2d, f3d], dim=-1)  # (B, N, 512)
        alpha_raw = self.point_mlp(feat_concat)  # (B, N, 1)
        
        # 2. åº”ç”¨æœ‰æ•ˆæ©ç è°ƒæ•´ï¼šinvalidç‚¹ç›´æ¥ä½¿ç”¨3Dç‰¹å¾ï¼ˆÎ±=0ï¼‰
        valid_mask_expanded = valid_mask.unsqueeze(-1).float()  # (B, N, 1)
        alpha = alpha_raw * valid_mask_expanded  # invalidç‚¹Î±=0ï¼Œåªç”¨3D
        
        # 3. ç‚¹çº§èåˆï¼šf_mix = Î±Â·fâ‚‚D + (1-Î±)Â·fâ‚ƒD
        f_mix = alpha * f2d + (1 - alpha) * f3d  # (B, N, 256)
        
        # 4. æ©ç åŒ–SEé€šé“é‡åŠ æƒ
        f_mix_t = f_mix.permute(0, 2, 1)  # (B, 256, N)
        if self.use_masked_se:
            fused_t = self.se_masked(f_mix_t, valid_mask)  # (B, 256, N)
        else:
            # å›é€€åˆ°åŸç‰ˆSE
            se_weights = self.se_module(f_mix_t)  # (B, 256, 1)
            fused_t = se_weights * f_mix_t  # (B, 256, N)
        fused_feat = fused_t.permute(0, 2, 1)  # (B, N, 256)
        
        # è¿”å›èåˆç‰¹å¾å’Œç½®ä¿¡åº¦
        confidence = alpha  # èåˆæƒé‡å¯ä½œä¸ºç½®ä¿¡åº¦
        
        return fused_feat, confidence


# Remove FiLM and PE modules - they are no longer used in simplified architecture


@MODELS.register_module()
class BiFusionEncoder(nn.Module):
    """Enhanced Bi-Fusion Encoder combining 2D CLIP visual features and 3D Sparse features."""

    def __init__(self,
                 voxel_size: float = 0.02,
                 use_amp: bool = True,
                 # ğŸ¯ ç‰¹å¾åŸŸé…ç½®ï¼ˆç®€åŒ–ä¸ºä»…æ”¯æŒ60Ã—80é¢„è®¡ç®—ï¼‰
                 feat_space: str = "precomp_60x80",      # å›ºå®šä¸ºé¢„è®¡ç®—ç‰¹å¾
                 use_precomp_2d: bool = True,            # é»˜è®¤å¯ç”¨é¢„è®¡ç®—ç‰¹å¾
                 # è°ƒè¯•æ¨¡å¼æ§åˆ¶
                 debug: bool = False,
                 # å…¼å®¹æ€§å‚æ•°ï¼ˆå¿½ç•¥ï¼Œä¿è¯é…ç½®å…¼å®¹æ€§ï¼‰
                 clip_pretrained: str = 'openai',  # å¿½ç•¥
                 freeze_blocks: int = 0,  # å¿½ç•¥
                 use_tiny_sa_2d: bool = False,  # å¿½ç•¥
                 use_enhanced_gate: bool = True,  # å¿½ç•¥
                 use_spatial_attention: bool = True,  # å¿½ç•¥
                 spatial_k: int = 16,  # å¿½ç•¥
                 use_tiny_sa_3d: bool = False,  # å¿½ç•¥
                 clip_num_layers: int = 6,  # å¿½ç•¥
                 freeze_clip_conv1: bool = False,  # å¿½ç•¥
                 freeze_clip_early_layers: bool = True,  # å¿½ç•¥
                 target_resolution: tuple = (224, 224),  # å¿½ç•¥
                 **kwargs):  # æ¥æ”¶å…¶ä»–æœªçŸ¥å‚æ•°
        super().__init__()
        
        # ğŸ¯ ç‰¹å¾åŸŸé…ç½®
        self.feat_space = feat_space
        self.use_precomp_2d = use_precomp_2d
        self.debug = debug
        self.target_resolution = target_resolution if isinstance(target_resolution, tuple) else (target_resolution, target_resolution)
        
        # ğŸ¯ æ ¹æ®ç‰¹å¾åŸŸè®¾ç½®ï¼ˆç®€åŒ–ï¼Œåªæ”¯æŒ60Ã—80é¢„è®¡ç®—ï¼‰
        if feat_space != "precomp_60x80":
            print(f"è­¦å‘Š: å½“å‰ä»…æ”¯æŒprecomp_60x80ç‰¹å¾åŸŸï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°precomp_60x80")
            feat_space = "precomp_60x80"
        
        # åˆ é™¤Enhanced CLIPç¼–ç å™¨ï¼ˆä¸å†éœ€è¦ï¼‰
        # self.enhanced_clip = None
        
        # 3D encoder - ä¿æŒåŸå§‹96ç»´ä»¥å…¼å®¹é¢„è®­ç»ƒæƒé‡ï¼Œç„¶åä½¿ç”¨æŠ•å½±å¤´åˆ°256ç»´
        cfg_backbone = SimpleNamespace(dilations=[1, 1, 1, 1], bn_momentum=0.02, conv1_kernel_size=5)
        self.backbone3d = Res16UNet34C(in_channels=3, out_channels=96, config=cfg_backbone, D=3)
        
        # 3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        self.proj_3d = EnhancedProjectionHead3D(
            input_dim=96,
            output_dim=256
        )
        
        # ç»Ÿä¸€çš„Headç»“æ„ï¼ˆ2D/3Då¯¹ç§°ï¼‰
        self.head3d = Head(256, 256, p=0.1)
        self.head2d = Head(256, 256, p=0.1)
        
        # èåˆæœºåˆ¶ï¼šä½¿ç”¨æ©ç åŒ–SEçš„LiteFusionGate      
        self.fusion_gate = LiteFusionGate(
            feat_dim=256,
            use_masked_se=True
        )
        
        # ğŸ¯ é¢„è®¡ç®—ç‰¹å¾é€‚é…å™¨ï¼ˆæƒ°æ€§åˆå§‹åŒ–ï¼‰
        self.precomp_adapter = None
        
        # ğŸ¯ Alphaå›é€€å€¼ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰
        
        # ğŸ¯ æŸå¤±å†å²è®°å½•ï¼ˆç”¨äºæŠ–åŠ¨åˆ†æï¼‰
        from collections import deque
        self._loss_hist = deque(maxlen=100)

        # åŸºæœ¬è¿è¡Œ/è°ƒè¯•å¼€å…³å’Œç»Ÿè®¡ç»“æ„
        self.voxel_size = voxel_size
        self.use_amp = use_amp
        self.use_lite_gate = True
        
        # ğŸ¯ æ ‡å‡†åˆ†è¾¨ç‡ä¸å†…å‚é…ç½®
        self.W0, self.H0 = 640, 480
        self.standard_scannet_intrinsics = (577.870605, 577.870605, 319.5, 239.5)
        self.warn_valid_ratio = 0.85   # è®­ç»ƒæœŸé™ä½é˜ˆå€¼ï¼Œé¿å…é¢‘ç¹å‘Šè­¦
        self.align_corners = True
        self.max_depth = 20.0
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¦ç”¨å¤–å‚è‡ªåŠ¨æ¨æ–­ï¼Œç»Ÿä¸€ä½¿ç”¨ç¡®å®šæ€§å¤„ç†
        self.auto_pose = False  # å¼ºåˆ¶ç¦ç”¨ï¼ŒæŒ‰ä¼˜åŒ–æŒ‡å—è¦æ±‚
        self._pose_pick_stats = {'direct': 0, 'inv': 0}
        self._collect_fusion_stats = debug
        self._collect_gradient_stats = debug
        self._fusion_stats = {}
        self._stats_history = []

    def _intrinsics_for_feat(self, Hf: int, Wf: int):
        """ç»Ÿä¸€å†…å‚æ¢ç®—å‡½æ•° - æ ¹æ®ç‰¹å¾å›¾å°ºå¯¸åŠ¨æ€è®¡ç®—å†…å‚
        
        Args:
            Hf: ç‰¹å¾å›¾é«˜åº¦
            Wf: ç‰¹å¾å›¾å®½åº¦
            
        Returns:
            tuple: (fx_feat, fy_feat, cx_feat, cy_feat)
        """
        fx0, fy0, cx0, cy0 = self.standard_scannet_intrinsics
        scale_x, scale_y = Wf / self.W0, Hf / self.H0
        return (fx0*scale_x, fy0*scale_y, cx0*scale_x, cy0*scale_y)
        
        # å›ºå®šä½¿ç”¨ScanNetæ ‡å‡†å†…å‚ï¼š[577.870605, 577.870605, 319.5, 239.5]  
        # ç‰¹å¾åŸŸå†…å‚ï¼ˆç»è¿‡æ— è£å‰ªCLIPå¤„ç†ï¼Œæ€»ç¼©æ”¾0.125xï¼‰ï¼š[72.233826, 72.233826, 39.9375, 29.9375]
        if self.debug:
            print(f"ğŸ”§ BiFusionåˆå§‹åŒ–: ä½¿ç”¨å›ºå®šæ ‡å‡†å†…å‚é…ç½®")
            print(f"ğŸ“ ç¼©æ”¾ç³»æ•°: x={self.scale_x:.3f}, y={self.scale_y:.3f}")
            print(f"ğŸš€ æŠ•å½±ç­–ç•¥: æ·±åº¦åæ ‡æŠ•å½± â†’ ç‰¹å¾å›¾ç¼©æ”¾ (æŒ‰å®˜æ–¹æµç¨‹)")

    def get_pose_pick_stats(self):
        return dict(self._pose_pick_stats)

    def reset_pose_pick_stats(self):
        self._pose_pick_stats = {'direct': 0, 'inv': 0}
    
    def _ensure_precomp_adapter(self, c_in: int):
        """æƒ°æ€§åˆå§‹åŒ–é¢„è®¡ç®—ç‰¹å¾é€‚é…å™¨ï¼š512 â†’ 256"""
        if (self.precomp_adapter is None) or (self.precomp_adapter[0].in_features != c_in):
            # æŒ‰ç…§ä¼˜åŒ–æŒ‡å—è¦æ±‚ï¼šLinear(512â†’256) + LayerNorm
            self.precomp_adapter = nn.Sequential(
                nn.Linear(c_in, 256),
                nn.LayerNorm(256)
            ).to(next(self.parameters()).device)
            if self.debug:
                print(f"ğŸ”§ åˆå§‹åŒ–é¢„è®¡ç®—é€‚é…å™¨: {c_in} â†’ 256 (ä¼˜åŒ–ç‰ˆæœ¬)")
    
    def get_grad_stats(self):
        """è·å–æ¢¯åº¦å¥åº·åº¦ç»Ÿè®¡"""
        stats = {}
        for name, module in [("head2d", self.head2d), ("head3d", self.head3d), ("gate", self.fusion_gate)]:
            total = 0.0
            cnt = 0
            for p in module.parameters():
                if p.grad is not None:
                    total += p.grad.data.norm().item()
                    cnt += 1
            stats[f"grad_{name}"] = total / max(cnt, 1)
        return stats
    
    def update_loss_stat(self, loss_val: float):
        """æ›´æ–°æŸå¤±å†å²è®°å½•"""
        self._loss_hist.append(float(loss_val))
    
    def get_loss_var(self):
        """è·å–æŸå¤±æ»‘çª—æ–¹å·®"""
        if len(self._loss_hist) < 20:
            return None
        arr = torch.tensor(list(self._loss_hist))
        return float(arr.var(unbiased=False))
    
    def _log_key_metrics(self, valid: torch.Tensor, conf: torch.Tensor):
        """ç®€åŒ–ç›‘æ§è¾“å‡ºï¼šä»…è¾“å‡ºå…³é”®æŒ‡æ ‡"""
        if not self.debug:
            return  # éè°ƒè¯•æ¨¡å¼ä¸è¾“å‡º
            
        with torch.no_grad():
            # 1. Validæ¯”ä¾‹
            valid_ratio = valid.float().mean().item()
            
            # 2. Fusion gateå‚æ•°ï¼ˆalphaç»Ÿè®¡ï¼‰- åªè®¡ç®—æœ‰æ•ˆç‚¹çš„alpha
            alpha = conf.squeeze(-1) if conf.dim() == 2 else conf  # (N,)
            
            if valid.any():
                # åªç»Ÿè®¡æœ‰æ•ˆæŠ•å½±ç‚¹çš„alpha
                alpha_valid = alpha[valid]
                alpha_mean = float(alpha_valid.mean()) if alpha_valid.numel() else 0.0
                alpha_std = float(alpha_valid.std(unbiased=False)) if alpha_valid.numel() > 1 else 0.0
            else:
                # æ²¡æœ‰æœ‰æ•ˆç‚¹æ—¶çš„å¤„ç†
                alpha_mean = 0.0
                alpha_std = 0.0
            
            # ç®€åŒ–è¾“å‡ºæ ¼å¼
            print(f"ğŸ¯ Validæ¯”ä¾‹: {valid_ratio:.3f} | Fusion-Î±: å‡å€¼={alpha_mean:.3f}, æ ‡å‡†å·®={alpha_std:.3f}")
            
            # å¦‚æœvalidæ¯”ä¾‹ä¸º0ï¼Œè¾“å‡ºè°ƒè¯•ä¿¡æ¯
            if valid_ratio == 0.0:
                print(f"âš ï¸ DEBUG: validå…¨ä¸º0ï¼Œæ€»ç‚¹æ•°={valid.numel()}")
            
            # å¯é…ç½®çš„æœ‰æ•ˆæ¯”ä¾‹è­¦å‘Š
            if self.warn_valid_ratio and valid_ratio < self.warn_valid_ratio:
                print(f"âš ï¸ æœ‰æ•ˆæ¯”ä¾‹è¿‡ä½: {valid_ratio:.3f} < {self.warn_valid_ratio}")
    
    def _collect_fusion_statistics(self, conf: torch.Tensor, valid: torch.Tensor, 
                                 f2d: torch.Tensor, f3d: torch.Tensor):
        """æ”¶é›†èåˆé—¨æ§ç»Ÿè®¡ä¿¡æ¯ - ğŸ”§ åªç»Ÿè®¡validç‚¹"""
        try:
            with torch.no_grad():
                # åŸºç¡€ç»Ÿè®¡
                if conf.dim() == 2:  # (N, 1)
                    conf_values = conf.squeeze(-1)  # (N,)
                else:
                    conf_values = conf
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåªç»Ÿè®¡validç‚¹ï¼Œé¿å…invalidç‚¹æ±¡æŸ“ç»Ÿè®¡
                if valid.any():
                    # åªä½¿ç”¨æœ‰æ•ˆç‚¹è¿›è¡Œç»Ÿè®¡
                    valid_conf = conf_values[valid]
                    valid_f2d = f2d[valid]
                    valid_f3d = f3d[valid]
                    
                    # è®¡ç®—èåˆæ¯”ä¾‹ï¼ˆåŸºäºæœ‰æ•ˆç‚¹ï¼‰
                    fusion_2d_ratio = valid_conf.mean().item()
                    fusion_3d_ratio = 1.0 - fusion_2d_ratio
                    avg_confidence = valid_conf.mean().item()
                    
                    # ç‰¹å¾è´¨é‡ç»Ÿè®¡ï¼ˆåŸºäºæœ‰æ•ˆç‚¹ï¼‰
                    f2d_norm = torch.norm(valid_f2d, dim=-1).mean().item()
                    f3d_norm = torch.norm(valid_f3d, dim=-1).mean().item()
                    
                    # ç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆåŸºäºæœ‰æ•ˆç‚¹ï¼‰
                    cos_sim = F.cosine_similarity(valid_f2d, valid_f3d, dim=-1).mean().item()
                    
                    total_valid_points = valid.sum().item()
                else:
                    # æ²¡æœ‰æœ‰æ•ˆç‚¹çš„fallback
                    fusion_2d_ratio = 0.0
                    fusion_3d_ratio = 1.0  
                    avg_confidence = 0.0
                    f2d_norm = 0.0
                    f3d_norm = 0.0
                    cos_sim = 0.0
                    total_valid_points = 0
                
                # æœ‰æ•ˆç‚¹æ¯”ä¾‹ï¼ˆç›¸å¯¹äºæ€»ç‚¹æ•°ï¼‰
                valid_points_ratio = valid.float().mean().item()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._fusion_stats = {
                    'fusion_2d_ratio': fusion_2d_ratio,
                    'fusion_3d_ratio': fusion_3d_ratio, 
                    'avg_confidence': avg_confidence,
                    'valid_points_ratio': valid_points_ratio,
                    'f2d_norm_avg': f2d_norm,
                    'f3d_norm_avg': f3d_norm,
                    'feature_similarity': cos_sim,
                    'total_points': conf_values.numel(),
                    'total_valid_points': total_valid_points  # æ–°å¢æœ‰æ•ˆç‚¹æ•°é‡
                }
                
                # ä¿ç•™å†å²è®°å½•ï¼ˆæœ€å¤š100æ¡ï¼‰
                self._stats_history.append(self._fusion_stats.copy())
                if len(self._stats_history) > 100:
                    self._stats_history.pop(0)
                    
        except Exception as e:
            if self.debug:
                print(f"Warning: Failed to collect fusion stats: {e}")
    
    def get_fusion_statistics(self):
        """è·å–èåˆç»Ÿè®¡ä¿¡æ¯"""
        return self._fusion_stats.copy() if self._fusion_stats else {}
    
    def get_statistics_summary(self, last_n: int = 10):
        """è·å–æœ€è¿‘Næ¬¡çš„ç»Ÿè®¡æ‘˜è¦"""
        if not self._stats_history:
            return {}
            
        recent_stats = self._stats_history[-last_n:]
        summary = {}
        
        for key in recent_stats[0].keys():
            if key != 'total_points':
                values = [stats[key] for stats in recent_stats if key in stats]
                if values:
                    summary[f'{key}_mean'] = sum(values) / len(values)
                    summary[f'{key}_std'] = (sum((x - summary[f'{key}_mean'])**2 for x in values) / len(values))**0.5
        
        return summary

    # åˆ é™¤äº†å¤æ‚çš„ _improved_projection_with_geometry å‡½æ•°ï¼Œ
    # ç»Ÿä¸€ä½¿ç”¨ unified_projection_and_sample

    def _pixels_to_grid(self, uv_feat: torch.Tensor,
                        feat_hw: Tuple[int,int],
                        align_corners: bool = True) -> torch.Tensor:
        """
        ğŸš¨ å…³é”®ä¿®å¤ï¼šç»Ÿä¸€grid_sampleè§„èŒƒåŒ–æ ‡å‡†
        æŠŠåƒç´ åæ ‡ (u,v) è½¬ä¸º grid_sample éœ€è¦çš„ [-1,1] å½’ä¸€åŒ–åæ ‡ã€‚
        - uv_feat: (M,2) åƒç´ åæ ‡ï¼ˆç‰¹å¾å›¾å°ºåº¦ï¼‰
        - feat_hw: (H_feat, W_feat)
        - è¿”å›: (1, M, 1, 2) çš„ grid
        """
        H, W = feat_hw
        u = uv_feat[:, 0]
        v = uv_feat[:, 1]
        
        if align_corners:
            # ğŸ”§ align_corners=True: è¾¹ç•Œä¸º [0, W-1] [0, H-1]
            # è¿™æ · (0,0) æ˜ å°„åˆ° (-1,-1), (W-1,H-1) æ˜ å°„åˆ° (1,1)
            x_norm = 2.0 * u / max(float(W - 1), 1.0) - 1.0
            y_norm = 2.0 * v / max(float(H - 1), 1.0) - 1.0
        else:
            # align_corners=False: è¾¹ç•Œä¸º [0, W) [0, H)
            x_norm = 2.0 * (u + 0.5) / float(W) - 1.0
            y_norm = 2.0 * (v + 0.5) / float(H) - 1.0
            
        grid = torch.stack([x_norm, y_norm], dim=-1).view(1, -1, 1, 2)
        return grid

    def _sample_img_feat(self, feat_map: torch.Tensor,
                         uv_feat: torch.Tensor,
                         valid_mask: torch.Tensor,
                         align_corners: bool = True) -> torch.Tensor:
        """
        ä»ç‰¹å¾å›¾ (1,C,H,W) é‡‡æ · N ä¸ªç‚¹çš„ç‰¹å¾ã€‚
        - feat_map: (1, C, H, W)
        - uv_feat:  (N, 2) åƒç´ åæ ‡ï¼ˆç‰¹å¾å›¾å°ºåº¦ï¼‰
        - valid_mask: (N,) bool
        - è¿”å›: (N, C)
        """
        assert feat_map.dim() == 4 and feat_map.size(0) == 1
        H, W = feat_map.shape[-2], feat_map.shape[-1]

        # åªå¯¹ valid çš„ç‚¹æ„é€  gridï¼Œå¯ä»¥å‡å°‘è¾¹ç•Œå¼‚å¸¸
        idx = torch.nonzero(valid_mask, as_tuple=False).squeeze(-1)
        if idx.numel() == 0:
            return feat_map.new_zeros((uv_feat.size(0), feat_map.size(1)))

        uv_valid = uv_feat[idx]  # (M,2)
        grid = self._pixels_to_grid(uv_valid, (H, W), align_corners=align_corners)  # 1xMx1x2

        # ç¡®ä¿feat_mapå’Œgridæœ‰ç›¸åŒçš„æ•°æ®ç±»å‹
        if feat_map.dtype != grid.dtype:
            grid = grid.to(feat_map.dtype)

        # é‡‡æ ·: F.grid_sample(1, C, H, W), (1, M, 1, 2) -> (1, C, 1, M)
        sampled = F.grid_sample(
            feat_map, grid, mode='bilinear',
            align_corners=align_corners
        ).squeeze(3).squeeze(0).T  # (1, C, M) -> (C, M) -> (M, C)

        out = feat_map.new_zeros((uv_feat.size(0), feat_map.size(1)))
        out[idx] = sampled
        return out

    def unified_projection_and_sample(self,
                                      xyz_cam: torch.Tensor,
                                      intr: Optional[torch.Tensor],
                                      feat_map: torch.Tensor,
                                      target_resolution: Optional[Tuple[int, int]] = None):
        """
        ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šåŠ¨æ€å†…å‚æ¢ç®—ï¼Œè§£å†³validæ¯”ä¾‹ä¸º0çš„é—®é¢˜
        
        æ ¸å¿ƒåŸåˆ™ï¼š
        1. æ ¹æ®å½“å‰ç‰¹å¾å›¾å°ºå¯¸åŠ¨æ€è®¡ç®—å†…å‚ï¼Œæ”¯æŒä»»æ„HÃ—W
        2. ä¸¥æ ¼çš„è¾¹ç•Œå’Œæ·±åº¦æ£€æŸ¥
        3. ä¸align_cornersä¸€è‡´çš„gridé‡‡æ ·
        
        Args:
            xyz_cam: (N, 3) ç›¸æœºåæ ‡ç³»ç‚¹äº‘
            intr: (4,) åŸå›¾å†…å‚ [fx, fy, cx, cy] - å·²åºŸå¼ƒï¼Œæ”¹ç”¨åŠ¨æ€è®¡ç®—
            feat_map: (1, C, H, W) ç‰¹å¾å›¾
            target_resolution: å·²åºŸå¼ƒå‚æ•°ï¼Œä¿æŒå…¼å®¹æ€§
        
        Returns:
            sampled_feat: (N, C) é‡‡æ ·ç‰¹å¾
            valid_mask: (N,) æœ‰æ•ˆæŠ•å½±æ©ç 
        """
        N = xyz_cam.shape[0]
        C, Hf, Wf = feat_map.shape[1:]
        device = xyz_cam.device
        dtype = xyz_cam.dtype
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåŠ¨æ€è®¡ç®—ç‰¹å¾å›¾å°ºåº¦å†…å‚
        fx_feat, fy_feat, cx_feat, cy_feat = self._intrinsics_for_feat(Hf, Wf)
        
        # ğŸ¯ è°ƒè¯•è¾“å‡ºï¼šæ‰“å°ç‰¹å¾å›¾å°ºåº¦å’Œå¯¹åº”å†…å‚
        if self.debug:
            if not hasattr(self, f'_logged_{Hf}x{Wf}'):
                print(f"ğŸ¯ ç‰¹å¾å›¾{Hf}Ã—{Wf}: åŠ¨æ€å†…å‚ fx={fx_feat:.3f}, fy={fy_feat:.3f}, cx={cx_feat:.3f}, cy={cy_feat:.3f}")
                setattr(self, f'_logged_{Hf}x{Wf}', True)
        
        # 3DæŠ•å½±ï¼šç›¸æœºåæ ‡ç³» â†’ ç‰¹å¾å›¾åƒç´ åæ ‡
        x, y, z = xyz_cam[:, 0], xyz_cam[:, 1], xyz_cam[:, 2]
        eps = 1e-6
        
        u_feat = fx_feat * (x / (z + eps)) + cx_feat
        v_feat = fy_feat * (y / (z + eps)) + cy_feat
        
        # ï¿½ ä¸¥æ ¼çš„æœ‰æ•ˆæ€§æ£€æŸ¥
        valid_z = (z > 0.1) & (z < self.max_depth)  # æ·±åº¦èŒƒå›´æ£€æŸ¥
        
        # è¾¹ç•Œæ£€æŸ¥ï¼šè€ƒè™‘align_cornersçš„å½±å“
        margin = 0.5 if self.align_corners else 0.0
        valid_u = (u_feat >= margin) & (u_feat < Wf - margin)
        valid_v = (v_feat >= margin) & (v_feat < Hf - margin)
        
        # ç»¼åˆæœ‰æ•ˆæ€§åˆ¤å®š
        valid = valid_z & valid_u & valid_v
        
        # ğŸš¨ å…³é”®è°ƒè¯•ï¼šæ£€æŸ¥æŠ•å½±åæ ‡åˆ†å¸ƒ
        if self.debug:
            total_points = len(z)
            depth_valid = valid_z.sum().item()
            boundary_valid = valid.sum().item()
            
            # åæ ‡ç»Ÿè®¡
            u_min, u_max = u_feat.min().item(), u_feat.max().item()
            v_min, v_max = v_feat.min().item(), v_feat.max().item()
            z_min, z_max = z.min().item(), z.max().item()
            
            print(f"ğŸ” æŠ•å½±åæ ‡è¯Šæ–­({Hf}Ã—{Wf}):")
            print(f"   æ€»ç‚¹æ•°: {total_points}")
            print(f"   æ·±åº¦èŒƒå›´: [{z_min:.3f}, {z_max:.3f}]m, æœ‰æ•ˆæ·±åº¦: {depth_valid}/{total_points} ({100*depth_valid/total_points:.1f}%)")
            print(f"   Uåæ ‡èŒƒå›´: [{u_min:.1f}, {u_max:.1f}], ç›®æ ‡[0, {Wf})")
            print(f"   Våæ ‡èŒƒå›´: [{v_min:.1f}, {v_max:.1f}], ç›®æ ‡[0, {Hf})")
            print(f"   æœ€ç»ˆæœ‰æ•ˆ: {boundary_valid}/{total_points} ({100*boundary_valid/total_points:.1f}%)")
            
            # å¦‚æœæœ‰æ•ˆç‡æä½ï¼Œæ‰“å°æ ·æœ¬åæ ‡ç”¨äºè¯Šæ–­
            if boundary_valid < total_points * 0.1:  # æœ‰æ•ˆç‡ä½äº10%
                print(f"âš ï¸ æœ‰æ•ˆç‡è¿‡ä½ï¼Œæ‰“å°å‰10ä¸ªç‚¹åæ ‡ç”¨äºè¯Šæ–­:")
                for i in range(min(10, total_points)):
                    print(f"   ç‚¹{i}: 3D=({xyz_cam[i,0]:.2f},{xyz_cam[i,1]:.2f},{xyz_cam[i,2]:.2f}) â†’ 2D=({u_feat[i]:.1f},{v_feat[i]:.1f})")
        
        # ç‰¹å¾é‡‡æ · - ä½¿ç”¨align_cornersç¡®ä¿ä¸€è‡´æ€§
        uv_feat = torch.stack([u_feat, v_feat], dim=-1)  # (N, 2)
        sampled_feat = self._sample_img_feat(feat_map, uv_feat, valid, align_corners=self.align_corners)
        
        return sampled_feat, valid

    def _process_single(self, points: torch.Tensor, img: torch.Tensor, cam_meta: Dict):
        """å¤„ç†å•å¸§æ•°æ®ï¼Œä½¿ç”¨ç®€åŒ–çš„æ•°æ®æµ"""
        # æå–åŸºç¡€ä¿¡æ¯
        xyz = points[:, :3].contiguous()  
        dev = xyz.device
        dtype = xyz.dtype
        
        # ç®€åŒ–poseè§£æ
        pose_matrix = None
        pose_type = None
        
        if isinstance(cam_meta, dict):
            # åªå¤„ç†pose_centeredï¼ˆæ•°æ®ä¸­å®é™…ä½¿ç”¨çš„æ ¼å¼ï¼‰
            if 'pose_centered' in cam_meta:
                pose_matrix = cam_meta['pose_centered']
                pose_type = 'pose_centered'
            elif 'cam_info' in cam_meta and 'pose_centered' in cam_meta['cam_info']:
                pose_matrix = cam_meta['cam_info']['pose_centered']
                pose_type = 'pose_centered'
        
        xyz_cam_proj = None
        
        # ç®€åŒ–å¤–å‚å¤„ç†ï¼šåªå¤„ç†pose_centered
        if pose_matrix is not None:
            try:
                T_matrix = torch.as_tensor(pose_matrix, dtype=dtype, device=dev)
                W2C = torch.inverse(T_matrix)  # pose_centeredæ˜¯C2Wæ ¼å¼ï¼Œæ±‚é€†å¾—W2C
                
                # åŸºæœ¬æœ‰æ•ˆæ€§æ£€æŸ¥
                if torch.det(W2C[:3, :3]).abs() > 0.001:
                    # pose_centeredå·²ç»å¤„ç†è¿‡xyz_offsetï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‚¹äº‘åæ ‡
                    xyz1 = torch.cat([xyz, torch.ones(xyz.shape[0], 1, device=dev, dtype=dtype)], dim=1)
                    xyz_cam_proj = (xyz1 @ W2C.t())[:, :3]
                    self._pose_pick_stats['inv'] += 1
                    
            except (RuntimeError, torch.linalg.LinAlgError):
                pass  # é™é»˜å¤„ç†å¤±è´¥
        
        # å›é€€å¤„ç†ï¼šå¦‚æœå¤–å‚å¤„ç†å¤±è´¥ï¼Œxyz_cam_projä¿æŒNone
        # è¿™å°†å¯¼è‡´åç»­è·³è¿‡2DæŠ•å½±ï¼Œåªä½¿ç”¨3Dç‰¹å¾
        
        # 3Dåˆ†æ”¯å§‹ç»ˆä½¿ç”¨ä¸–ç•Œåæ ‡
        xyz_world = xyz

        # 3Dåˆ†æ”¯ï¼šMinkUNet â†’ 96d â†’ Proj3D(96â†’256, LN inside) â†’ Head3D(256â†’256, LN inside) â†’ (ä¸åšL2)
        coords_int = torch.round(xyz_world / self.voxel_size).to(torch.int32)
        coords = torch.cat([torch.zeros(coords_int.size(0), 1, dtype=torch.int32, device=coords_int.device),
                             coords_int], dim=1)
        feats = points[:, 3:6].contiguous()
        field = ME.TensorField(coordinates=coords, features=feats)
        sparse_tensor = field.sparse()
        feat3d_sparse = self.backbone3d(sparse_tensor)
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨sliceæ“ä½œå°†ç¨€ç–ç‰¹å¾æ˜ å°„å›åŸå§‹ç‚¹äº‘
        feat3d = feat3d_sparse.slice(field).features
        
        # éªŒè¯ç‰¹å¾æ•°é‡åŒ¹é…ï¼ˆç°åœ¨åº”è¯¥åŒ¹é…äº†ï¼‰
        if feat3d.shape[0] != points.shape[0]:
            raise RuntimeError(f"3D features shape mismatch: got {feat3d.shape[0]}, expected {points.shape[0]}")
        
        # 3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´ (å†…å«LNï¼Œä¸é¢å¤–åšL2)
        feat3d = self.proj_3d(feat3d.float())  # (N, 96) -> (N, 256), ç¡®ä¿floatç±»å‹
        
        # ç»Ÿä¸€Headï¼šä¸åšL2å½’ä¸€åŒ–
        f3d = self.head3d(feat3d)  # (N, 256)

        # ğŸ¯ æŠ•å½±é‡‡æ · - ç®€åŒ–å†…å‚å¤„ç†ï¼Œå®Œå…¨ä¿¡ä»»forwardçš„å†»ç»“æœºåˆ¶
        if xyz_cam_proj is None:
            if self.debug:
                print("âŒ xyz_cam_projä¸ºç©ºï¼Œæ— æ³•æŠ•å½±ï¼Œä½¿ç”¨é›¶ç‰¹å¾")
            feat2d_raw = f3d.new_zeros((f3d.shape[0], 256))
            valid = f3d.new_zeros((f3d.shape[0],), dtype=torch.bool)
            f2d = self.head2d(feat2d_raw)
        
        # å†…å‚å¤„ç†å·²åœ¨unified_projection_and_sampleå†…éƒ¨åŠ¨æ€å®Œæˆï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–å¤„ç†

        # 2Dç‰¹å¾å¤„ç†ï¼šé¢„è®¡ç®—CLIPç‰¹å¾ï¼ˆéœ€è¦æœ‰æ•ˆçš„ç›¸æœºæŠ•å½±ï¼‰
        elif xyz_cam_proj is not None and (isinstance(cam_meta, dict) and "clip_pix" in cam_meta and cam_meta["clip_pix"] is not None):
            # 60Ã—80é¢„è®¡ç®—è·¯å¾„
            feat_map = cam_meta["clip_pix"]
            if not torch.is_tensor(feat_map):
                feat_map = torch.as_tensor(feat_map, device=dev, dtype=dtype)
            feat_map = feat_map.to(dev).float().unsqueeze(0)  # ç¡®ä¿è®¾å¤‡å’Œç±»å‹æ­£ç¡®
            
            # æŠ•å½±é‡‡æ ·
            feat2d_raw, valid = self.unified_projection_and_sample(
                xyz_cam=xyz_cam_proj,
                intr=None,  # å†…éƒ¨åŠ¨æ€è®¡ç®—å†…å‚
                feat_map=feat_map,
                target_resolution=(img.shape[-2], img.shape[-1])
            )
            
            # é€šé“é€‚é…ï¼š512 â†’ 256
            if feat2d_raw.shape[-1] != 256:
                self._ensure_precomp_adapter(feat2d_raw.shape[-1])
                assert self.precomp_adapter is not None, "é¢„è®¡ç®—é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥"
                feat2d_raw = self.precomp_adapter(feat2d_raw)
            
            f2d = self.head2d(feat2d_raw)
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæŠ•å½±æˆ–é¢„è®¡ç®—ç‰¹å¾ï¼Œä½¿ç”¨é›¶ç‰¹å¾
            feat2d_raw = f3d.new_zeros((f3d.shape[0], 256))
            valid = f3d.new_zeros((f3d.shape[0],), dtype=torch.bool)
            f2d = self.head2d(feat2d_raw)
        if (isinstance(cam_meta, dict) and "clip_pix" in cam_meta and cam_meta["clip_pix"] is not None):
            #å¦‚æœâ€œclip_pixâ€ä¸ºNone,åˆ™è¾“å‡ºè¯¥è·¯å¾„
            if cam_meta["clip_pix"] is None:
                print(f"âŒ clip_pixä¸ºç©ºï¼Œæ— æ³•ä½¿ç”¨è¯¥è·¯å¾„{cam_meta['clip_pix']}")
                return torch.zeros(points.shape[0], 256, device=dev, dtype=dtype)

            # 60Ã—80é¢„è®¡ç®—è·¯å¾„
            feat_map = cam_meta["clip_pix"]
            if not torch.is_tensor(feat_map):
                feat_map = torch.as_tensor(feat_map, device=dev, dtype=dtype)
            
            if feat_map.device != dev:
                feat_map = feat_map.to(dev)
            
            # ä½¿ç”¨ç»Ÿä¸€æŠ•å½±æ¥å£ï¼ˆ60Ã—80 CNNç‰¹å¾ä¸“ç”¨ï¼Œä¸ä½¿ç”¨CLIPè£å‰ªï¼‰
            feat2d_raw, valid = self.unified_projection_and_sample(
                xyz_cam=xyz_cam_proj,
                intr=None,  # intrå‚æ•°å·²åºŸå¼ƒï¼Œå‡½æ•°å†…éƒ¨åŠ¨æ€è®¡ç®—
                feat_map=feat_map.float().unsqueeze(0),  # è½¬æ¢ä¸ºfloat32å¹¶æ·»åŠ batchç»´åº¦
                target_resolution=(img.shape[-2], img.shape[-1])  # åŸå›¾åˆ†è¾¨ç‡ç”¨äºç¼©æ”¾è®¡ç®—
            )
            
            # é€šé“é€‚é…ï¼š512 â†’ 256
            if feat2d_raw.shape[-1] != 256:
                self._ensure_precomp_adapter(feat2d_raw.shape[-1])
                assert self.precomp_adapter is not None, "é¢„è®¡ç®—é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥"
                feat2d_raw = self.precomp_adapter(feat2d_raw)
            
            f2d = self.head2d(feat2d_raw)
            
            if self.debug:
                Hf, Wf = feat_map.shape[-2:]
                valid_ratio = valid.float().mean().item()
                print(f"ğŸ¯ 60Ã—80è·¯å¾„: æœ‰æ•ˆæ¯”ä¾‹={valid_ratio:.3f}, ç‰¹å¾åŸŸå°ºå¯¸=({Hf}, {Wf}), "
                      f"è¾“å…¥é€šé“={feat_map.shape[0]}, é€‚é…åé€šé“={feat2d_raw.shape[-1]}")
        else:
            # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—ç‰¹å¾ï¼Œä½¿ç”¨é›¶ç‰¹å¾
            print("âš ï¸ æœªæ‰¾åˆ°é¢„è®¡ç®—CLIPç‰¹å¾ (clip_pix)ï¼Œä½¿ç”¨é›¶ç‰¹å¾")
            feat2d_raw = f3d.new_zeros((f3d.shape[0], 256))
            valid = f3d.new_zeros((f3d.shape[0],), dtype=torch.bool)
            f2d = self.head2d(feat2d_raw)

        # èåˆç‰¹å¾
        f2d_batch = f2d.unsqueeze(0)
        f3d_batch = f3d.unsqueeze(0)
        valid_batch = valid.unsqueeze(0)
        
        fused_batch, conf_batch = self.fusion_gate(f2d_batch, f3d_batch, valid_batch)
        fused = fused_batch.squeeze(0)
        conf = conf_batch.squeeze(0)
        
        # L2å½’ä¸€åŒ–
        fused = F.normalize(fused, dim=-1)
        
        # ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
        if self._collect_fusion_stats:
            self._collect_fusion_statistics(conf, valid, f2d, f3d)
        self._log_key_metrics(valid, conf)

        return fused, conf, valid

    def forward(self, points_list, imgs, cam_info):
        """ç®€åŒ–çš„forwardå‡½æ•°ï¼šæ‰¹é‡å¤„ç†3D-2Dèåˆ"""
        
        # 1. è¾“å…¥æ ¼å¼æ ‡å‡†åŒ–
        if torch.is_tensor(points_list):
            points_list = list(points_list) if points_list.dim() == 3 else [points_list]
        if torch.is_tensor(imgs):
            imgs = list(imgs) if imgs.dim() == 4 else [imgs]
        
        batch_size = len(points_list)
        if len(imgs) != batch_size:
            raise RuntimeError(f"è¾“å…¥é•¿åº¦ä¸åŒ¹é…: points({len(points_list)}) != imgs({len(imgs)})")
        
        # 2. cam_infoæ ‡å‡†åŒ–
        if cam_info is None or isinstance(cam_info, dict):
            cam_info = [cam_info] * batch_size
        elif len(cam_info) == 1:
            cam_info = cam_info * batch_size
        elif len(cam_info) != batch_size:
            raise RuntimeError(f"cam_infoé•¿åº¦({len(cam_info)})ä¸batch_size({batch_size})ä¸åŒ¹é…")
        
        # 3. é€æ ·æœ¬å¤„ç†
        feat_fusion_list, conf_list, valid_mask_list = [], [], []
        
        for idx, (pts, img, meta) in enumerate(zip(points_list, imgs, cam_info)):
            # ç®€åŒ–metaä¿¡æ¯å¤„ç†
            if meta is None:
                meta = {}
            
            meta_std = {}
            
            # å¤„ç†metaä¿¡æ¯ï¼šç®€åŒ–poseå¤„ç†
            meta_std = {}
            for key, value in meta.items():
                if isinstance(value, list) and idx < len(value):
                    if key == 'pose_centered':  # åªå¤„ç†pose_centered
                        meta_std[key] = torch.as_tensor(value[idx], dtype=torch.float32)
                    else:
                        meta_std[key] = value[idx]
                else:
                    meta_std[key] = value
            
            # å¤„ç†å•ä¸ªæ ·æœ¬
            fused, conf, valid_mask = self._process_single(pts, img, meta_std)
            
            feat_fusion_list.append(fused)
            conf_list.append(conf)
            valid_mask_list.append(valid_mask)
        
        return {
            'feat_fusion': feat_fusion_list,
            'conf_2d': conf_list,
            'valid_projection_mask': valid_mask_list
        }
