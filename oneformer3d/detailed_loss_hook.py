"""
è‡ªå®šä¹‰Hookï¼šè¯¦ç»†æŸå¤±å’Œç»Ÿè®¡ä¿¡æ¯ç›‘æ§
ç”¨äºBiFusionè®­ç»ƒçš„å…¨é¢æ—¥å¿—è¾“å‡º
"""

import torch
from mmengine.hooks import Hook
from mmengine.registry import HOOKS
from typing import Optional, Dict, Any


@HOOKS.register_module()
class DetailedLossMonitorHook(Hook):
    """è¯¦ç»†æŸå¤±å’Œèåˆç»Ÿè®¡ç›‘æ§Hook
    
    åŠŸèƒ½ï¼š
    1. è¾“å‡ºæ‰€æœ‰æŸå¤±ç»„ä»¶çš„è¯¦ç»†æ•°å€¼
    2. ç›‘æ§èåˆé—¨æ§çš„ç»Ÿè®¡ä¿¡æ¯
    3. è®°å½•æ¢¯åº¦èŒƒæ•°å’Œæ•°å€¼ç¨³å®šæ€§æŒ‡æ ‡
    4. è¾“å‡ºCLIPç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
    """
    
    def __init__(self, 
                 log_interval: int = 10,
                 collect_grad_norm: bool = True,
                 collect_fusion_stats: bool = True,
                 collect_clip_stats: bool = True):
        self.log_interval = log_interval
        self.collect_grad_norm = collect_grad_norm
        self.collect_fusion_stats = collect_fusion_stats
        self.collect_clip_stats = collect_clip_stats
        
    def after_train_iter(self, 
                        runner, 
                        batch_idx: int, 
                        data_batch = None, 
                        outputs = None) -> None:
        """è®­ç»ƒè¿­ä»£åçš„è¯¦ç»†æ—¥å¿—è¾“å‡º"""
        
        if batch_idx % self.log_interval != 0:
            return
            
        # è·å–å½“å‰æŸå¤±
        if outputs and 'log_vars' in outputs:
            log_vars = outputs['log_vars']
            
            # æ„å»ºè¯¦ç»†æŸå¤±æŠ¥å‘Šï¼ˆåŒ…å«èåˆç»Ÿè®¡ï¼‰
            loss_report = self._build_loss_report(log_vars)
            
            # è·å–æ¢¯åº¦ç»Ÿè®¡
            grad_stats = self._get_gradient_stats(runner.model) if self.collect_grad_norm else {}
            
            # è·å–CLIPç»Ÿè®¡
            clip_stats = self._get_clip_stats(runner.model) if self.collect_clip_stats else {}
            
            # è¾“å‡ºå®Œæ•´æŠ¥å‘Šï¼ˆä¸å†éœ€è¦å•ç‹¬çš„fusion_statsï¼Œå·²åœ¨loss_reportä¸­ï¼‰
            self._print_detailed_report(batch_idx, loss_report, {}, grad_stats, clip_stats)
    
    def _build_loss_report(self, log_vars: Dict) -> Dict[str, float]:
        """æ„å»ºæŸå¤±å‡½æ•°è¯¦ç»†æŠ¥å‘Š"""
        loss_report = {}
        
        # ä¸»è¦æŸå¤±
        if 'loss' in log_vars:
            loss_report['total_loss'] = log_vars['loss']
            
        # è¯­ä¹‰æŸå¤±
        for key in log_vars:
            if 'sem_loss' in key or 'semantic' in key:
                loss_report[f'semantic_{key}'] = log_vars[key]
                
        # å®ä¾‹æŸå¤±
        for key in log_vars:
            if any(x in key for x in ['inst_loss', 'cls_loss', 'mask_bce_loss', 'mask_dice_loss', 'score_loss']):
                loss_report[f'instance_{key}'] = log_vars[key]
                
        # CLIPæŸå¤±
        for key in log_vars:
            if 'clip' in key.lower():
                loss_report[f'clip_{key}'] = log_vars[key]
                
        # èåˆç»Ÿè®¡ï¼ˆç›´æ¥ä»log_varsä¸­æå–ï¼‰
        fusion_keys = [
            'avg_confidence',
            'valid_ratio',
            'norm_ratio_2d_over_3d',
            'cos_2d3d_mean',
            'cos_2d3d_mean_ln',
            'feat3d_mean_abs',
            'feat3d_std',
            'feat3d_nonzero_ratio',
            'feat2d_mean_abs',
            'feat2d_std',
            'feat2d_nonzero_ratio',
            'fused_mean_abs',
            'fused_std',
            'grad_norm_feat3d',
            'grad_norm_feat2d',
            'grad_norm_fusion',
            'grad_norm_feat3d_raw',
            'grad_norm_feat2d_raw',
            'grad_norm_fusion_raw',
            'grad_params_feat2d',
            'grad_params_feat3d',
            'grad_params_fusion',
            'grad_params_decoder',
            'grad_ratio_2d_over_3d'
        ]
        for key in fusion_keys:
            if key in log_vars:
                loss_report[key] = log_vars[key]
                
        return loss_report
    
    def _get_gradient_stats(self, model) -> Dict[str, float]:
        """è·å–æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯"""
        grad_stats = {}
        
        try:
            total_norm = 0.0
            param_count = 0
            max_grad = 0.0
            
            # åˆ†æ¨¡å—ç»Ÿè®¡æ¢¯åº¦
            for name, module in model.named_modules():
                if any(x in name for x in ['bi_encoder', 'decoder', 'criterion']):
                    module_norm = 0.0
                    module_params = 0
                    
                    for param in module.parameters():
                        if param.grad is not None:
                            param_norm = param.grad.data.norm(2)
                            module_norm += param_norm.item() ** 2
                            module_params += 1
                            max_grad = max(max_grad, param.grad.data.abs().max().item())
                    
                    if module_params > 0:
                        grad_stats[f'grad_norm_{name.split(".")[0]}'] = (module_norm ** 0.5)
                        
            # æ€»ä½“æ¢¯åº¦ç»Ÿè®¡
            for param in model.parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
                    param_count += 1
                    
            if param_count > 0:
                grad_stats['grad_norm_total'] = (total_norm ** 0.5)
                grad_stats['grad_max'] = max_grad
                grad_stats['grad_param_count'] = param_count
                
        except Exception as e:
            grad_stats['grad_error'] = str(e)[:50]
            
        return grad_stats
    
    def _get_clip_stats(self, model) -> Dict[str, float]:
        """è·å–CLIPç‰¹å¾ç»Ÿè®¡ä¿¡æ¯"""
        clip_stats = {}
        
        try:
            if hasattr(model, 'bi_encoder') and hasattr(model.bi_encoder, 'enhanced_2d_encoder'):
                encoder_2d = model.bi_encoder.enhanced_2d_encoder
                
                # CLIPæ¨¡å‹å‚æ•°ç»Ÿè®¡
                if hasattr(encoder_2d, 'clip_visual'):
                    clip_model = encoder_2d.clip_visual
                    trainable_params = sum(p.numel() for p in clip_model.parameters() if p.requires_grad)
                    total_params = sum(p.numel() for p in clip_model.parameters())
                    
                    clip_stats['clip_trainable_ratio'] = trainable_params / max(total_params, 1)
                    clip_stats['clip_total_params'] = total_params
                    clip_stats['clip_trainable_params'] = trainable_params
                
        except Exception as e:
            clip_stats['clip_error'] = str(e)[:50]
            
        return clip_stats
    
    def _print_detailed_report(self, batch_idx: int, loss_report: Dict, fusion_stats: Dict, 
                             grad_stats: Dict, clip_stats: Dict):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š - fusion_statså·²ä¸å†ä½¿ç”¨ï¼Œå› ä¸ºèåˆç»Ÿè®¡ç°åœ¨åœ¨loss_reportä¸­"""
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¯¦ç»†è®­ç»ƒæŠ¥å‘Š - Iteration {batch_idx}")
        print(f"{'='*80}")
        
        # æŸå¤±å‡½æ•°æŠ¥å‘Š  
        if loss_report:
            print(f"ğŸ”¥ æŸå¤±å‡½æ•°:")
            # å…ˆæ˜¾ç¤ºä¸»è¦æŸå¤±
            main_losses = {}
            fusion_losses = {}
            other_losses = {}
            
            fusion_prefixes = [
                'avg_confidence',
                'valid_ratio',
                'norm_ratio_2d_over_3d',
                'cos_2d3d',
                'feat3d_',
                'feat2d_',
                'fused_',
                'grad_norm_feat'
            ]

            for key, value in loss_report.items():
                if not isinstance(value, (int, float)):
                    continue

                if any(key.startswith(prefix) for prefix in fusion_prefixes):
                    fusion_losses[key] = value
                elif any(x in key for x in ['loss', 'semantic', 'instance', 'clip']):
                    main_losses[key] = value
                else:
                    other_losses[key] = value
            
            # æ˜¾ç¤ºä¸»è¦æŸå¤±
            for key, value in main_losses.items():
                print(f"  {key:<25}: {value:.6f}")
                
            # æ˜¾ç¤ºèåˆä¸åˆ†æ”¯ç»Ÿè®¡
            if fusion_losses:
                print(f"\nğŸ”€ èåˆä¸åˆ†æ”¯ç»Ÿè®¡:")
                for key, value in fusion_losses.items():
                    print(f"  ğŸ“Š {key:<30}: {value:.6f}")
                        
            # æ˜¾ç¤ºå…¶ä»–ç»Ÿè®¡
            if other_losses:
                print(f"\nğŸ“ˆ å…¶ä»–ç»Ÿè®¡:")
                for key, value in other_losses.items():
                    print(f"  {key:<25}: {value:.6f}")
                    
        # æ¢¯åº¦ç»Ÿè®¡æŠ¥å‘Š
        if grad_stats:
            print(f"\nğŸ“ˆ æ¢¯åº¦ç»Ÿè®¡:")
            for key, value in grad_stats.items():
                if isinstance(value, (int, float)):
                    print(f"  {key:<25}: {value:.6f}")
                    
        # CLIPç»Ÿè®¡æŠ¥å‘Š
        if clip_stats:
            print(f"\nğŸ¨ CLIPç»Ÿè®¡:")
            for key, value in clip_stats.items():
                if isinstance(value, (int, float)):
                    print(f"  {key:<25}: {value:.6f}")
                    
        print(f"{'='*80}\n")


@HOOKS.register_module()  
class NaNDetectionHook(Hook):
    """NaNæ£€æµ‹Hook - åŠæ—¶å‘ç°æ•°å€¼ä¸ç¨³å®š"""
    
    def __init__(self, check_interval: int = 1):
        self.check_interval = check_interval
        
    def after_train_iter(self, runner, batch_idx: int, data_batch=None, outputs=None):
        """æ£€æµ‹NaN/Inf"""
        if batch_idx % self.check_interval != 0:
            return
            
        # æ£€æŸ¥æŸå¤±
        if outputs and 'log_vars' in outputs:
            for key, value in outputs['log_vars'].items():
                if isinstance(value, (int, float)):
                    if torch.isnan(torch.tensor(value)) or torch.isinf(torch.tensor(value)):
                        print(f"ğŸš¨ NaN/Infæ£€æµ‹åˆ°åœ¨ {key}: {value}")
                        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°
        nan_params = []
        inf_params = []
        for name, param in runner.model.named_parameters():
            if torch.isnan(param.data).any():
                nan_params.append(name)
            if torch.isinf(param.data).any():
                inf_params.append(name)
                
        if nan_params:
            print(f"ğŸš¨ NaNå‚æ•°æ£€æµ‹: {nan_params[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ª
        if inf_params:
            print(f"ğŸš¨ Infå‚æ•°æ£€æµ‹: {inf_params[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ª
