"""
Enhanced Training Monitoring Hook for BiFusion
å¢å¼ºçš„è®­ç»ƒç›‘æ§Hookï¼Œè¾“å‡ºè¯¦ç»†æŸå¤±ã€fusion gateç»Ÿè®¡ã€æŠ•å½±æœ‰æ•ˆç‡ç­‰
"""

import torch
import torch.nn.functional as F
from mmengine.hooks import Hook
from mmengine.registry import HOOKS
from mmdet3d.registry import HOOKS as MMDET3D_HOOKS
import numpy as np
from typing import Dict, Any, Optional


@HOOKS.register_module()
@MMDET3D_HOOKS.register_module()
class EnhancedTrainingHook(Hook):
    """Enhanced training monitoring hook for BiFusion system.
    
    ç›‘æ§å†…å®¹ï¼š
    1. è¯¦ç»†æŸå¤±æ•°å€¼åˆ†è§£
    2. Fusion gateç»Ÿè®¡ (2D/3Dæƒé‡åˆ†é…)
    3. æŠ•å½±æœ‰æ•ˆç‡ (valid projection rate)
    4. æ¢¯åº¦å¥åº·åº¦ç›‘æ§
    5. ç‰¹å¾è´¨é‡æŒ‡æ ‡
    """
    
    def __init__(self, 
                 log_interval: int = 10,
                 grad_monitor_interval: int = 50,
                 detailed_stats: bool = True):
        super().__init__()
        self.log_interval = log_interval
        self.grad_monitor_interval = grad_monitor_interval
        self.detailed_stats = detailed_stats
        self.iter_count = 0
        
    def before_train_iter(self, runner, batch_idx: int, data_batch: Any = None) -> None:
        """è®­ç»ƒè¿­ä»£å‰çš„å‡†å¤‡å·¥ä½œ"""
        self.iter_count += 1
        
    def after_train_iter(self, runner, batch_idx: int, data_batch: Any = None, 
                        outputs: Optional[dict] = None) -> None:
        """è®­ç»ƒè¿­ä»£åçš„ç›‘æ§å’Œæ—¥å¿—è¾“å‡º"""
        
        if self.iter_count % self.log_interval != 0:
            return
            
        # è·å–æ¨¡å‹å’ŒæŸå¤±ä¿¡æ¯
        model = runner.model
        if hasattr(runner, 'log_buffer') and runner.log_buffer.output:
            loss_info = runner.log_buffer.output
        else:
            return
            
        # 1. è¯¦ç»†æŸå¤±åˆ†è§£
        detailed_losses = self._extract_detailed_losses(loss_info)
        
        # 2. BiFusionç»Ÿè®¡ä¿¡æ¯
        fusion_stats = self._extract_fusion_stats(model, outputs)
        
        # 3. æŠ•å½±æœ‰æ•ˆç‡ç»Ÿè®¡
        projection_stats = self._extract_projection_stats(model, outputs)
        
        # 4. æ¢¯åº¦å¥åº·åº¦ç›‘æ§
        if self.iter_count % self.grad_monitor_interval == 0:
            grad_stats = self._extract_gradient_health(model)
        else:
            grad_stats = {}
            
        # 5. è¾“å‡ºå¢å¼ºæ—¥å¿—
        self._log_enhanced_stats(runner, detailed_losses, fusion_stats, 
                               projection_stats, grad_stats)
    
    def _extract_detailed_losses(self, loss_info: Dict) -> Dict[str, float]:
        """æå–è¯¦ç»†æŸå¤±æ•°å€¼"""
        detailed_losses = {}
        
        # åŸºç¡€æŸå¤±
        if 'loss' in loss_info:
            detailed_losses['total_loss'] = float(loss_info['loss'])
        
        # è¯­ä¹‰æŸå¤±
        for key in ['semantic_loss', 'sem_loss']:
            if key in loss_info:
                detailed_losses['semantic_loss'] = float(loss_info[key])
                break
                
        # å®ä¾‹æŸå¤±ç»„ä»¶
        instance_keys = {
            'inst_cls_loss': ['inst_cls_loss', 'classification_loss'],
            'inst_bce_loss': ['inst_bce_loss', 'bce_loss', 'mask_bce_loss'],
            'inst_dice_loss': ['inst_dice_loss', 'dice_loss', 'mask_dice_loss'],
            'inst_score_loss': ['inst_score_loss', 'score_loss'],
            'inst_bbox_loss': ['inst_bbox_loss', 'bbox_loss']
        }
        
        for target_key, possible_keys in instance_keys.items():
            for key in possible_keys:
                if key in loss_info:
                    detailed_losses[target_key] = float(loss_info[key])
                    break
        
        # CLIPå’Œè¾…åŠ©æŸå¤±
        aux_keys = {
            'clip_cons_loss': ['clip_cons_loss', 'clip_loss', 'clip_consistency_loss'],
            'spatial_cons_loss': ['spatial_cons_loss', 'spatial_consistency_loss'],
            'no_view_loss': ['no_view_loss', 'no_view_supervision_loss']
        }
        
        for target_key, possible_keys in aux_keys.items():
            for key in possible_keys:
                if key in loss_info:
                    detailed_losses[target_key] = float(loss_info[key])
                    break
        
        return detailed_losses
    
    def _extract_fusion_stats(self, model, outputs) -> Dict[str, float]:
        """æå–BiFusioné—¨æ§ç»Ÿè®¡ä¿¡æ¯"""
        fusion_stats = {}
        
        try:
            # å°è¯•ä»æ¨¡å‹ä¸­è·å–BiFusionæ¨¡å—
            if hasattr(model, 'module'):  # DDPåŒ…è£…
                model = model.module
                
            bi_encoder = None
            if hasattr(model, 'bi_encoder'):
                bi_encoder = model.bi_encoder
            elif hasattr(model, 'backbone') and hasattr(model.backbone, 'bi_encoder'):
                bi_encoder = model.backbone.bi_encoder
                
            if bi_encoder is not None:
                # ğŸ”¥ æ–¹æ³•1: ç›´æ¥è·å–BiFusionEncoderçš„ç»Ÿè®¡ä¿¡æ¯
                if hasattr(bi_encoder, 'get_fusion_statistics'):
                    fusion_stats.update(bi_encoder.get_fusion_statistics())
                
                # ğŸ”¥ æ–¹æ³•2: ä»fusion gateæ¨¡å—è·å–æœ€æ–°ç»Ÿè®¡
                if hasattr(bi_encoder, 'fusion_gate'):
                    gate_module = bi_encoder.fusion_gate
                    if hasattr(gate_module, '_stats_buffer') and gate_module._stats_buffer:
                        latest_stats = gate_module._stats_buffer[-1]
                        fusion_stats.update(latest_stats)
                
                # ğŸ”¥ æ–¹æ³•3: æŸ¥æ‰¾å…·ä½“çš„gateæƒé‡ï¼ˆLiteFusionGateæˆ–EnhancedFusionGateï¼‰
                for name, module in bi_encoder.named_modules():
                    if 'fusion_gate' in name or isinstance(module, (type(bi_encoder.fusion_gate) if hasattr(bi_encoder, 'fusion_gate') else type(None))):
                        # æ£€æŸ¥æ˜¯å¦æœ‰alphaæƒé‡ï¼ˆLiteFusionGateçš„point_mlpè¾“å‡ºï¼‰
                        if hasattr(module, '_last_alpha'):
                            alpha = module._last_alpha
                            if isinstance(alpha, torch.Tensor):
                                alpha_np = alpha.detach().cpu().numpy()
                                fusion_stats['gate_2d_ratio'] = float(np.mean(alpha_np))
                                fusion_stats['gate_3d_ratio'] = float(np.mean(1 - alpha_np))
                                fusion_stats['gate_std'] = float(np.std(alpha_np))
                        break
                        
                # ğŸ”¥ æ–¹æ³•4: ä»æœ€è¿‘çš„forwardè¾“å‡ºä¸­è·å–
                if hasattr(bi_encoder, '_last_forward_stats'):
                    fusion_stats.update(bi_encoder._last_forward_stats)
                    
        except Exception as e:
            # å¦‚æœæ— æ³•è·å–fusionç»Ÿè®¡ï¼Œè®°å½•ä½†ä¸ä¸­æ–­
            fusion_stats['fusion_error'] = str(e)[:50]
            
        return fusion_stats
    
    def _extract_projection_stats(self, model, outputs) -> Dict[str, float]:
        """æå–æŠ•å½±æœ‰æ•ˆç‡ç»Ÿè®¡"""
        projection_stats = {}
        
        try:
            # å°è¯•ä»æ¨¡å‹ä¸­è·å–BiFusionæ¨¡å—
            if hasattr(model, 'module'):  # DDPåŒ…è£…
                model = model.module
                
            bi_encoder = None
            if hasattr(model, 'bi_encoder'):
                bi_encoder = model.bi_encoder
            elif hasattr(model, 'backbone') and hasattr(model.backbone, 'bi_encoder'):
                bi_encoder = model.backbone.bi_encoder
            
            # ğŸ”¥ æ–¹æ³•1: ä»BiFusionEncoderçš„èåˆç»Ÿè®¡ä¸­è·å–
            if bi_encoder is not None and hasattr(bi_encoder, '_fusion_stats'):
                fusion_stats = bi_encoder._fusion_stats
                if 'valid_points_ratio' in fusion_stats:
                    projection_stats['valid_projection_rate'] = fusion_stats['valid_points_ratio']
                if 'total_points' in fusion_stats:
                    projection_stats['total_points'] = fusion_stats['total_points']
                    projection_stats['valid_points'] = int(fusion_stats['total_points'] * fusion_stats.get('valid_points_ratio', 1.0))
                    projection_stats['invalid_points'] = fusion_stats['total_points'] - projection_stats['valid_points']
            
            # ğŸ”¥ æ–¹æ³•2: ä»outputsä¸­è·å–æŠ•å½±ç›¸å…³ä¿¡æ¯
            if outputs and isinstance(outputs, dict):
                # æŸ¥æ‰¾valid maskç›¸å…³ä¿¡æ¯ - ä¼˜å…ˆæŸ¥æ‰¾BiFusionè¾“å‡º
                for key in ['valid_projection_mask', 'valid_mask', 'projection_mask', 'valid_projection', 'conf_2d']:
                    if key in outputs:
                        mask_data = outputs[key]
                        
                        # å¤„ç†åˆ—è¡¨æ ¼å¼ï¼ˆBiFusionè¿”å›æ ¼å¼ï¼‰
                        if isinstance(mask_data, (list, tuple)):
                            if len(mask_data) > 0:
                                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç»Ÿè®¡ï¼ˆå‡è®¾batchå†…ç»Ÿè®¡ç›¸ä¼¼ï¼‰
                                mask = mask_data[0]
                                if isinstance(mask, torch.Tensor):
                                    if key == 'conf_2d':  # ç½®ä¿¡åº¦éœ€è¦è½¬æ¢ä¸ºæœ‰æ•ˆæ©ç 
                                        mask = mask.squeeze(-1) > 0.1 if mask.dim() > 1 else mask > 0.1
                                    
                                    valid_rate = float(mask.float().mean())
                                    projection_stats['valid_projection_rate'] = valid_rate
                                    projection_stats['total_points'] = int(mask.numel())
                                    projection_stats['valid_points'] = int(mask.sum())
                                    projection_stats['invalid_points'] = projection_stats['total_points'] - projection_stats['valid_points']
                                    
                                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡ç»Ÿè®¡
                                    if len(mask_data) > 1:
                                        total_points = 0
                                        total_valid = 0
                                        for m in mask_data:
                                            if isinstance(m, torch.Tensor):
                                                if key == 'conf_2d':
                                                    m = m.squeeze(-1) > 0.1 if m.dim() > 1 else m > 0.1
                                                total_points += m.numel()
                                                total_valid += int(m.sum())
                                        
                                        if total_points > 0:
                                            projection_stats['valid_projection_rate'] = total_valid / total_points
                                            projection_stats['total_points'] = total_points
                                            projection_stats['valid_points'] = total_valid
                                            projection_stats['invalid_points'] = total_points - total_valid
                                            projection_stats['batch_size'] = len(mask_data)
                                    break
                        
                        # å¤„ç†å•ä¸ªTensoræ ¼å¼
                        elif isinstance(mask_data, torch.Tensor):
                            if key == 'conf_2d':  # ç½®ä¿¡åº¦éœ€è¦è½¬æ¢ä¸ºæœ‰æ•ˆæ©ç 
                                mask_data = mask_data.squeeze(-1) > 0.1 if mask_data.dim() > 1 else mask_data > 0.1
                            
                            valid_rate = float(mask_data.float().mean())
                            projection_stats['valid_projection_rate'] = valid_rate
                            projection_stats['total_points'] = int(mask_data.numel())
                            projection_stats['valid_points'] = int(mask_data.sum())
                            projection_stats['invalid_points'] = projection_stats['total_points'] - projection_stats['valid_points']
                            break
            
            # ğŸ”¥ æ–¹æ³•3: å°è¯•ä»æ¨¡å‹ä¸­è·å–æœ€è¿‘çš„æŠ•å½±ç»Ÿè®¡
            if hasattr(model, '_last_projection_stats'):
                stats = model._last_projection_stats
                if isinstance(stats, dict):
                    projection_stats.update(stats)
            
            # ğŸ”¥ æ–¹æ³•4: ä»bi_encoderè·å–æœ€è¿‘forwardçš„æŠ•å½±ç»Ÿè®¡
            if bi_encoder is not None:
                if hasattr(bi_encoder, '_last_valid_mask'):
                    mask = bi_encoder._last_valid_mask
                    if isinstance(mask, torch.Tensor):
                        valid_rate = float(mask.float().mean())
                        projection_stats['valid_projection_rate'] = valid_rate
                        projection_stats['total_points'] = int(mask.numel())
                        projection_stats['valid_points'] = int(mask.sum())
                        projection_stats['invalid_points'] = projection_stats['total_points'] - projection_stats['valid_points']
                        
        except Exception as e:
            projection_stats['projection_error'] = str(e)[:50]
            
        return projection_stats
    
    def _extract_gradient_health(self, model) -> Dict[str, float]:
        """æå–æ¢¯åº¦å¥åº·ç»Ÿè®¡"""
        gradient_stats = {}
        
        try:
            # è·å–çœŸå®æ¨¡å‹ï¼ˆå¤„ç†DDPåŒ…è£…ï¼‰
            real_model = model.module if hasattr(model, 'module') else model
            
            # ğŸ”¥ æ ¸å¿ƒç»„ä»¶æ¢¯åº¦ç›‘æ§
            component_grads = {}
            
            # ç›‘æ§CLIPç›¸å…³æ¢¯åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            clip_grad_norm = 0.0
            clip_param_count = 0
            for name, param in real_model.named_parameters():
                if param.grad is not None:
                    if 'clip' in name.lower() or 'text_encoder' in name.lower() or 'vision_encoder' in name.lower():
                        clip_grad_norm += param.grad.data.norm(2).item() ** 2
                        clip_param_count += 1
            
            if clip_param_count > 0:
                component_grads['clip_grad_norm'] = (clip_grad_norm ** 0.5)
                component_grads['clip_param_count'] = clip_param_count
                gradient_stats['clip_grad_norm'] = component_grads['clip_grad_norm']
            
            # ç›‘æ§BiFusionç›¸å…³æ¢¯åº¦
            bifusion_grad_norm = 0.0
            bifusion_param_count = 0
            for name, param in real_model.named_parameters():
                if param.grad is not None:
                    if any(keyword in name.lower() for keyword in ['bi_fusion', 'bifusion', 'bi_encoder', 'fusion_gate', 'lite_fusion']):
                        bifusion_grad_norm += param.grad.data.norm(2).item() ** 2
                        bifusion_param_count += 1
            
            if bifusion_param_count > 0:
                component_grads['bifusion_grad_norm'] = (bifusion_grad_norm ** 0.5)
                component_grads['bifusion_param_count'] = bifusion_param_count
                gradient_stats['bifusion_grad_norm'] = component_grads['bifusion_grad_norm']
            
            # ç›‘æ§backboneç›¸å…³æ¢¯åº¦
            backbone_grad_norm = 0.0
            backbone_param_count = 0
            for name, param in real_model.named_parameters():
                if param.grad is not None:
                    if any(keyword in name.lower() for keyword in ['backbone', 'encoder', 'decoder']) and not any(exc in name.lower() for exc in ['clip', 'bi_fusion', 'bifusion']):
                        backbone_grad_norm += param.grad.data.norm(2).item() ** 2
                        backbone_param_count += 1
            
            if backbone_param_count > 0:
                component_grads['backbone_grad_norm'] = (backbone_grad_norm ** 0.5)
                component_grads['backbone_param_count'] = backbone_param_count
                gradient_stats['backbone_grad_norm'] = component_grads['backbone_grad_norm']
            
            # ğŸ”¥ å…¨å±€æ¢¯åº¦ç»Ÿè®¡
            total_grad_norm = 0.0
            total_param_count = 0
            max_grad_norm = 0.0
            min_grad_norm = float('inf')
            grad_norms = []
            
            for name, param in real_model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.data.norm(2).item()
                    total_grad_norm += grad_norm ** 2
                    total_param_count += 1
                    grad_norms.append(grad_norm)
                    max_grad_norm = max(max_grad_norm, grad_norm)
                    min_grad_norm = min(min_grad_norm, grad_norm)
            
            if total_param_count > 0:
                gradient_stats['total_grad_norm'] = (total_grad_norm ** 0.5)
                gradient_stats['max_grad_norm'] = max_grad_norm
                gradient_stats['min_grad_norm'] = min_grad_norm if min_grad_norm != float('inf') else 0.0
                gradient_stats['param_with_grad_count'] = total_param_count
                
                # è®¡ç®—æ¢¯åº¦åˆ†å¸ƒç»Ÿè®¡
                if grad_norms:
                    import numpy as np
                    grad_array = np.array(grad_norms)
                    gradient_stats['grad_mean'] = float(np.mean(grad_array))
                    gradient_stats['grad_std'] = float(np.std(grad_array))
                    gradient_stats['grad_median'] = float(np.median(grad_array))
            
            # ğŸ”¥ æ¢¯åº¦å¥åº·è­¦æŠ¥
            gradient_stats['gradient_health'] = 'healthy'
            
            # æ£€æµ‹NaNæˆ–Infæ¢¯åº¦
            has_nan_grad = False
            has_inf_grad = False
            for name, param in real_model.named_parameters():
                if param.grad is not None:
                    if torch.isnan(param.grad).any():
                        has_nan_grad = True
                    if torch.isinf(param.grad).any():
                        has_inf_grad = True
            
            if has_nan_grad:
                gradient_stats['gradient_health'] = 'nan_detected'
            elif has_inf_grad:
                gradient_stats['gradient_health'] = 'inf_detected'
            elif gradient_stats.get('total_grad_norm', 0) > 100.0:
                gradient_stats['gradient_health'] = 'exploding'
            elif gradient_stats.get('total_grad_norm', 0) < 1e-8:
                gradient_stats['gradient_health'] = 'vanishing'
            
            # ğŸ”¥ ç»„ä»¶æ¢¯åº¦æ¯”ä¾‹åˆ†æ
            if gradient_stats.get('total_grad_norm', 0) > 0:
                if 'clip_grad_norm' in gradient_stats:
                    gradient_stats['clip_grad_ratio'] = gradient_stats['clip_grad_norm'] / gradient_stats['total_grad_norm']
                if 'bifusion_grad_norm' in gradient_stats:
                    gradient_stats['bifusion_grad_ratio'] = gradient_stats['bifusion_grad_norm'] / gradient_stats['total_grad_norm']
                if 'backbone_grad_norm' in gradient_stats:
                    gradient_stats['backbone_grad_ratio'] = gradient_stats['backbone_grad_norm'] / gradient_stats['total_grad_norm']
                    
        except Exception as e:
            gradient_stats['gradient_error'] = str(e)[:50]
            
        return gradient_stats
    
    def _log_enhanced_stats(self, runner, detailed_losses: Dict, fusion_stats: Dict,
                          projection_stats: Dict, grad_stats: Dict) -> None:
        """è¾“å‡ºå¢å¼ºçš„è®­ç»ƒç»Ÿè®¡æ—¥å¿—"""
        
        # æ„å»ºæ—¥å¿—æ¶ˆæ¯
        log_msg = f"\n{'='*80}\n"
        log_msg += f"ğŸ“Š Enhanced Training Stats - Iter {self.iter_count}\n"
        log_msg += f"{'='*80}\n"

        # Optimizer / LR ä¿¡æ¯
        lr_stats = {}
        optim_wrapper = getattr(runner, 'optim_wrapper', None)
        if optim_wrapper is not None and hasattr(optim_wrapper, 'optimizer'):
            optimizer = optim_wrapper.optimizer
            for idx, group in enumerate(optimizer.param_groups):
                lr_stats[f'group{idx}'] = float(group.get('lr', 0.0))
        if lr_stats:
            log_msg += "âš™ï¸  Optimizer / LR:\n"
            for name, value in lr_stats.items():
                log_msg += f"  lr/{name:<16}: {value:.6e}\n"
            log_msg += "\n"
        
        # 1. è¯¦ç»†æŸå¤±ä¿¡æ¯
        if detailed_losses:
            log_msg += "ğŸ”¥ Detailed Loss Breakdown:\n"
            for loss_name, loss_value in detailed_losses.items():
                log_msg += f"  {loss_name:<20}: {loss_value:.6f}\n"
            log_msg += "\n"
        
        # 2. BiFusionç»Ÿè®¡
        if fusion_stats:
            log_msg += "ğŸ”€ BiFusion Gate Statistics:\n"
            for stat_name, stat_value in fusion_stats.items():
                if isinstance(stat_value, (int, float)):
                    log_msg += f"  {stat_name:<20}: {stat_value:.4f}\n"
                else:
                    log_msg += f"  {stat_name:<20}: {stat_value}\n"
            log_msg += "\n"
        
        # 3. æŠ•å½±ç»Ÿè®¡
        if projection_stats:
            log_msg += "ğŸ“¡ Projection Statistics:\n"
            for stat_name, stat_value in projection_stats.items():
                if isinstance(stat_value, (int, float)):
                    if 'rate' in stat_name:
                        log_msg += f"  {stat_name:<20}: {stat_value:.3%}\n"
                    else:
                        log_msg += f"  {stat_name:<20}: {stat_value}\n"
                else:
                    log_msg += f"  {stat_name:<20}: {stat_value}\n"
            log_msg += "\n"
        
        # 4. æ¢¯åº¦å¥åº·åº¦ (å®šæœŸè¾“å‡º)
        if grad_stats:
            log_msg += "ğŸ“ˆ Gradient Health Monitor:\n"
            for stat_name, stat_value in grad_stats.items():
                if isinstance(stat_value, (int, float)):
                    if 'norm' in stat_name:
                        log_msg += f"  {stat_name:<20}: {stat_value:.6f}\n"
                    else:
                        log_msg += f"  {stat_name:<20}: {stat_value}\n"
                else:
                    log_msg += f"  {stat_name:<20}: {stat_value}\n"
            log_msg += "\n"
        
        log_msg += f"{'='*80}\n"
        
        # è¾“å‡ºåˆ°logger
        runner.logger.info(log_msg)
        
        # åŒæ—¶è®°å½•åˆ°tensorboard (å¦‚æœæœ‰)
        if hasattr(runner, 'visualizer') and runner.visualizer is not None:
            for lr_name, lr_val in lr_stats.items():
                runner.visualizer.add_scalar(f'train/lr/{lr_name}', lr_val, self.iter_count)
            for loss_name, loss_value in detailed_losses.items():
                if isinstance(loss_value, (int, float)):
                    runner.visualizer.add_scalar(f'train/detailed_loss/{loss_name}', 
                                                loss_value, self.iter_count)
            
            for stat_name, stat_value in fusion_stats.items():
                if isinstance(stat_value, (int, float)):
                    runner.visualizer.add_scalar(f'train/fusion/{stat_name}', 
                                                stat_value, self.iter_count)
            
            for stat_name, stat_value in projection_stats.items():
                if isinstance(stat_value, (int, float)):
                    runner.visualizer.add_scalar(f'train/projection/{stat_name}', 
                                                stat_value, self.iter_count)
                                                
            for stat_name, stat_value in grad_stats.items():
                if isinstance(stat_value, (int, float)):
                    runner.visualizer.add_scalar(f'train/gradient/{stat_name}', 
                                                stat_value, self.iter_count)
