import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
import contextlib
from typing import List, Dict, Optional, Tuple, Union, cast

import MinkowskiEngine as ME
from mmdet3d.registry import MODELS
from .mink_unet import Res16UNet34C
from types import SimpleNamespace


class EnhancedProjectionHead3D(nn.Module):
    """ç®€åŒ–çš„3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´
    
    æŒ‰ç…§ä¼˜åŒ–æŒ‡å—è¦æ±‚ï¼šLinear(96â†’256) + LayerNorm
    """
    
    def __init__(self,
                 input_dim: int = 96,
                 output_dim: int = 256):
        super().__init__()
        
        # ç®€åŒ–æŠ•å½±ï¼šå•å±‚Linear + LayerNorm
        self.projection = nn.Sequential(
            nn.Linear(input_dim, output_dim),        # èåˆç‰¹å¾
            nn.LayerNorm(output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """3Dç‰¹å¾æŠ•å½± (N, 96) -> (N, 256)"""
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.any(torch.isnan(x)) or torch.any(torch.isinf(x)):
            print("Warning: NaN/Inf in 3D projection input, clamping")
            x = torch.clamp(x, -10, 10)
        return self.projection(x)


class MaskedSE1D(nn.Module):
    """æ©ç åŒ–SEæ¨¡å— - åªç»Ÿè®¡æœ‰æ•ˆç‚¹çš„é€šé“å‡å€¼"""
    def __init__(self, C, r=16):
        super().__init__()
        self.excite = nn.Sequential(
            nn.Conv1d(C, C//r, 1), nn.ReLU(),
            nn.Conv1d(C//r, C, 1), nn.Sigmoid()
        )
    
    def forward(self, x, valid_mask): 
        # x: (B, C, N), valid_mask: (B, N)
        m = valid_mask.unsqueeze(1).float()             # (B,1,N)
        s = (x * m).sum(-1, keepdim=True)               # (B,C,1)  æœ‰æ•ˆç‚¹çš„é€šé“æ±‚å’Œ
        cnt = m.sum(-1, keepdim=True).clamp_min(1.0)    # (B,1,1)  æœ‰æ•ˆç‚¹è®¡æ•°
        z = s / cnt                                     # (B,C,1)  æ©ç åŒ–å‡å€¼
        w = self.excite(z)                              # (B,C,1)
        return x * w                                    # é€šé“é‡åŠ æƒ


class Head(nn.Module):
    """ç»Ÿä¸€çš„Headç»“æ„ - 2D/3Dåˆ†æ”¯å¯¹ç§°ä½¿ç”¨"""
    def __init__(self, dim=256, hidden=256, p=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden),
            nn.ReLU(),
            nn.Dropout(p),
            nn.Linear(hidden, dim),
            nn.LayerNorm(dim)
        )
    def forward(self, x):
        return self.net(x)


class LiteFusionGate(nn.Module):
    """Lite Fusion Gate - è½»é‡çº§èåˆé—¨æ§æœºåˆ¶
    
    ç®€åŒ–ç‰ˆæœ¬ï¼šç‚¹çº§èåˆ + æ©ç åŒ–SEé€šé“æ³¨æ„åŠ›ï¼Œç§»é™¤åˆ†é˜¶æ®µè®­ç»ƒé€»è¾‘
    å‚æ•°é‡çº¦0.12Mï¼Œè¿œä½äºåŸEnhancedGate
    """
    
    def __init__(self, 
                 feat_dim: int = 256,
                 use_masked_se: bool = True):
        super().__init__()
        
        self.feat_dim = feat_dim
        self.use_masked_se = use_masked_se
        
        # ç‚¹çº§èåˆæƒé‡MLP: æ·»åŠ LayerNormç¡®ä¿ç‰¹å¾ç¨³å®šæ€§
        self.point_mlp = nn.Sequential(
            nn.Linear(feat_dim * 2, 64),  # 256*2 -> 64
            nn.LayerNorm(64),  # æ·»åŠ å½’ä¸€åŒ–å±‚
            nn.ReLU(),
            nn.Dropout(0.1),   # æ·»åŠ å°‘é‡dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # ğŸ”§ è°ƒæ•´åˆå§‹åŒ–ï¼šbiasè®¾ä¸ºæ­£å€¼ï¼Œé¼“åŠ±æ›´å¤šä½¿ç”¨2Dç‰¹å¾
        nn.init.constant_(self.point_mlp[-2].bias, 1.0)  # åˆå§‹åå‘2Dç‰¹å¾
        
        # ğŸ”§ åŒæ—¶è°ƒæ•´æƒé‡åˆå§‹åŒ–ï¼Œä½¿ç”¨è¾ƒå°çš„æƒé‡é¿å…æ¢¯åº¦æ¶ˆå¤±  
        # åªåˆå§‹åŒ–Linearå±‚çš„æƒé‡
        for module in self.point_mlp:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
        
        # æ©ç åŒ–SEé€šé“æ³¨æ„åŠ›æ¨¡å—
        if use_masked_se:
            self.se_masked = MaskedSE1D(feat_dim, r=16)
        else:
            # åŸç‰ˆSEæ¨¡å—ï¼ˆå¤‡ç”¨ï¼‰
            self.se_module = nn.Sequential(
                nn.AdaptiveAvgPool1d(1),
                nn.Conv1d(feat_dim, feat_dim // 16, 1),
                nn.ReLU(),
                nn.Conv1d(feat_dim // 16, feat_dim, 1),
                nn.Sigmoid()
            )
        
    def forward(self, 
                f2d: torch.Tensor, 
                f3d: torch.Tensor, 
                valid_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            f2d: 2Dç‰¹å¾ (B, N, 256)
            f3d: 3Dç‰¹å¾ (B, N, 256) 
            valid_mask: æœ‰æ•ˆæŠ•å½±æ©ç  (B, N)
        Returns:
            fused_feat: èåˆç‰¹å¾ (B, N, 256)
            confidence: èåˆç½®ä¿¡åº¦ (B, N, 1)
        """
        B, N, C = f2d.shape
        
        # 1. ç‰¹å¾æ ‡å‡†åŒ–ï¼šç¡®ä¿2Då’Œ3Dç‰¹å¾åœ¨ç›¸åŒæ•°å€¼èŒƒå›´
        f2d_norm = F.normalize(f2d, dim=-1, p=2)  # L2å½’ä¸€åŒ–
        f3d_norm = F.normalize(f3d, dim=-1, p=2)  # L2å½’ä¸€åŒ–
        
        # 2. è®¡ç®—ç‚¹çº§èåˆæƒé‡Î±
        feat_concat = torch.cat([f2d_norm, f3d_norm], dim=-1)  # (B, N, 512)
        alpha_raw = self.point_mlp(feat_concat)  # (B, N, 1)
        
        # 2. åº”ç”¨æœ‰æ•ˆæ©ç è°ƒæ•´ï¼šæ”¹è¿›invalidç‚¹å¤„ç†ç­–ç•¥
        valid_mask_expanded = valid_mask.unsqueeze(-1).float()  # (B, N, 1)
        
        # ğŸ”§ æ”¹è¿›ï¼šå¯¹invalidç‚¹ä½¿ç”¨æ›´æ™ºèƒ½çš„fallbackç­–ç•¥
        # å¦‚æœå¤§éƒ¨åˆ†ç‚¹éƒ½invalidï¼Œè¯´æ˜æŠ•å½±è´¨é‡å·®ï¼Œåº”è¯¥æ›´å¤šä¾èµ–3D
        valid_ratio = valid_mask.float().mean(dim=1, keepdim=True)  # (B, 1)
        
        # åŠ¨æ€è°ƒæ•´fallbackæƒé‡ï¼šæŠ•å½±è´¨é‡å¥½æ—¶ç”¨æ›´å¤š2Dç‰¹å¾
        fallback_alpha = 0.3 * valid_ratio.unsqueeze(-1)  # (B, 1, 1) -> (B, N, 1)
        alpha = torch.where(valid_mask_expanded.bool(), alpha_raw, fallback_alpha)
        
        # 3. ç‚¹çº§èåˆï¼šf_mix = Î±Â·fâ‚‚D + (1-Î±)Â·fâ‚ƒD  
        # ä½¿ç”¨å½’ä¸€åŒ–åçš„ç‰¹å¾è¿›è¡Œèåˆ
        f_mix = alpha * f2d_norm + (1 - alpha) * f3d_norm  # (B, N, 256)
        
        # 4. æ©ç åŒ–SEé€šé“é‡åŠ æƒ
        f_mix_t = f_mix.permute(0, 2, 1)  # (B, 256, N)
        if self.use_masked_se:
            fused_t = self.se_masked(f_mix_t, valid_mask)  # (B, 256, N)
        else:
            # å›é€€åˆ°åŸç‰ˆSE
            se_weights = self.se_module(f_mix_t)  # (B, 256, 1)
            fused_t = se_weights * f_mix_t  # (B, 256, N)
        fused_feat = fused_t.permute(0, 2, 1)  # (B, N, 256)
        
        # è¿”å›èåˆç‰¹å¾å’Œç½®ä¿¡åº¦
        confidence = alpha  # èåˆæƒé‡å¯ä½œä¸ºç½®ä¿¡åº¦
        
        return fused_feat, confidence
    
    def compute_fusion_balance_loss(self, alpha: torch.Tensor, valid_mask: torch.Tensor, 
                                   target_ratio: float = 0.4) -> torch.Tensor:
        """è®¡ç®—èåˆå¹³è¡¡æŸå¤±ï¼Œé¼“åŠ±åˆç†çš„2D-3Dèåˆæ¯”ä¾‹
        
        Args:
            alpha: èåˆæƒé‡ (B, N, 1)
            valid_mask: æœ‰æ•ˆæ©ç  (B, N)
            target_ratio: ç›®æ ‡2Dç‰¹å¾æ¯”ä¾‹ï¼Œé»˜è®¤0.4ï¼ˆç•¥åå‘3Dï¼‰
            
        Returns:
            balance_loss: æ ‡é‡æŸå¤±å€¼
        """
        if not valid_mask.any():
            return torch.tensor(0.0, device=alpha.device, requires_grad=True)
            
        # åªè€ƒè™‘æœ‰æ•ˆç‚¹çš„èåˆæ¯”ä¾‹
        valid_alpha = alpha[valid_mask.unsqueeze(-1).expand_as(alpha)]
        
        if valid_alpha.numel() == 0:
            return torch.tensor(0.0, device=alpha.device, requires_grad=True)
        
        # è®¡ç®—å½“å‰2Dç‰¹å¾å¹³å‡æ¯”ä¾‹
        current_ratio = valid_alpha.mean()
        
        # L2æŸå¤±é¼“åŠ±æ¥è¿‘ç›®æ ‡æ¯”ä¾‹
        balance_loss = F.mse_loss(current_ratio, torch.tensor(target_ratio, device=alpha.device))
        
        return balance_loss


# Remove FiLM and PE modules - they are no longer used in simplified architecture


@MODELS.register_module()
class BiFusionEncoder(nn.Module):
    """Enhanced Bi-Fusion Encoder combining 2D CLIP visual features and 3D Sparse features."""

    def __init__(self,
                 voxel_size: float = 0.02,
                 use_amp: bool = True,
                 # ğŸ¯ ç‰¹å¾åŸŸé…ç½®ï¼ˆç®€åŒ–ä¸ºä»…æ”¯æŒ60Ã—80é¢„è®¡ç®—ï¼‰
                 feat_space: str = "precomp_60x80",      # å›ºå®šä¸ºé¢„è®¡ç®—ç‰¹å¾
                 use_precomp_2d: bool = True,            # é»˜è®¤å¯ç”¨é¢„è®¡ç®—ç‰¹å¾
                 # è°ƒè¯•æ¨¡å¼æ§åˆ¶
                 debug: bool = False,
                 **kwargs):  # æ¥æ”¶å…¶ä»–æœªçŸ¥å‚æ•°
        super().__init__()
        
        # ğŸ¯ ç‰¹å¾åŸŸé…ç½®
        self.feat_space = feat_space
        self.use_precomp_2d = use_precomp_2d
        self.debug = debug

        # ğŸ¯ æ ¹æ®ç‰¹å¾åŸŸè®¾ç½®ï¼ˆç®€åŒ–ï¼Œåªæ”¯æŒ60Ã—80é¢„è®¡ç®—ï¼‰
        if feat_space != "precomp_60x80":
            print(f"è­¦å‘Š: å½“å‰ä»…æ”¯æŒprecomp_60x80ç‰¹å¾åŸŸï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°precomp_60x80")
            feat_space = "precomp_60x80"
        
        # åˆ é™¤Enhanced CLIPç¼–ç å™¨ï¼ˆä¸å†éœ€è¦ï¼‰
        # self.enhanced_clip = None
        
        # 3D encoder - ä¿æŒåŸå§‹96ç»´ä»¥å…¼å®¹é¢„è®­ç»ƒæƒé‡ï¼Œç„¶åä½¿ç”¨æŠ•å½±å¤´åˆ°256ç»´
        cfg_backbone = SimpleNamespace(dilations=[1, 1, 1, 1], bn_momentum=0.02, conv1_kernel_size=5)
        self.backbone3d = Res16UNet34C(in_channels=3, out_channels=96, config=cfg_backbone, D=3)
        
        # 3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        self.proj_3d = EnhancedProjectionHead3D(
            input_dim=96,
            output_dim=256
        )
        
        # ç»Ÿä¸€çš„Headç»“æ„ï¼ˆ2D/3Då¯¹ç§°ï¼‰
        self.head3d = Head(256, 256, p=0.1)
        self.head2d = Head(256, 256, p=0.1)
        
        # èåˆæœºåˆ¶ï¼šä½¿ç”¨æ©ç åŒ–SEçš„LiteFusionGate      
        self.fusion_gate = LiteFusionGate(
            feat_dim=256,
            use_masked_se=True
        )
        
        # ğŸ¯ é¢„è®¡ç®—ç‰¹å¾é€‚é…å™¨ï¼ˆæƒ°æ€§åˆå§‹åŒ–ï¼‰
        self.precomp_adapter = None
        
        # ğŸ¯ Alphaå›é€€å€¼ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰
        
        # ğŸ¯ æŸå¤±å†å²è®°å½•ï¼ˆç”¨äºæŠ–åŠ¨åˆ†æï¼‰
        from collections import deque
        self._loss_hist = deque(maxlen=100)

        # åŸºæœ¬è¿è¡Œ/è°ƒè¯•å¼€å…³å’Œç»Ÿè®¡ç»“æ„
        self.voxel_size = voxel_size
        self.use_amp = use_amp
        self.use_lite_gate = True
        
        # ğŸ¯ æ ‡å‡†åˆ†è¾¨ç‡ä¸å†…å‚é…ç½®
        self.W0, self.H0 = 640, 480
        self.standard_scannet_intrinsics = (577.870605, 577.870605, 319.5, 239.5)
        self.warn_valid_ratio = 0.60   # ğŸ”§ è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼Œå‡å°‘å¹²æ‰°ä¿¡æ¯
        self.align_corners = True  # ğŸš¨ ä¿®å¤ï¼šä¸æµ‹è¯•è„šæœ¬çš„ç›´æ¥ç´¢å¼•é‡‡æ ·ä¿æŒä¸€è‡´
        self.max_depth = 20.0
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¦ç”¨å¤–å‚è‡ªåŠ¨æ¨æ–­ï¼Œç»Ÿä¸€ä½¿ç”¨ç¡®å®šæ€§å¤„ç†
        self.auto_pose = False  # å¼ºåˆ¶ç¦ç”¨ï¼ŒæŒ‰ä¼˜åŒ–æŒ‡å—è¦æ±‚
        self._pose_pick_stats = {'direct': 0, 'inv': 0}
        # ğŸ”§ ä¿®å¤ï¼šå§‹ç»ˆæ”¶é›†èåˆç»Ÿè®¡ï¼Œæ–¹ä¾¿è®­ç»ƒç›‘æ§
        self._collect_fusion_stats = True  # å§‹ç»ˆå¯ç”¨ï¼Œä¾¿äºç›‘æ§èåˆæ•ˆæœ
        self._collect_gradient_stats = debug  # æ¢¯åº¦ç»Ÿè®¡ä»ç„¶å—debugæ§åˆ¶
        self._fusion_stats = {}
        self._stats_history = []

    def _intrinsics_for_feat(self, Hf: int, Wf: int):
        """ç»Ÿä¸€å†…å‚æ¢ç®—å‡½æ•° - ä½¿ç”¨æ­£ç¡®çš„ScanNetå†…å‚è®¡ç®—
        Args:
            Hf: ç‰¹å¾å›¾é«˜åº¦ (H)
            Wf: ç‰¹å¾å›¾å®½åº¦ (W)
        Returns:
            tuple: (fx_feat, fy_feat, cx_feat, cy_feat)
        """
        fx0, fy0, cx0, cy0 = self.standard_scannet_intrinsics
        # è¾“å‡ºç‰¹å¾å°ºå¯¸ - ä»…debugæ¨¡å¼
        if self.debug:
            print(f"ğŸ¯ è®¡ç®—ç‰¹å¾å†…å‚: ç‰¹å¾å›¾å°ºå¯¸=({Hf},{Wf}) - HÃ—Wæ ¼å¼")

        # ğŸ”§ ä¿®æ­£ï¼šç¡®ä¿ç¼©æ”¾æ–¹å‘æ­£ç¡®
        # åŸå§‹ScanNet: 640Ã—480 (WÃ—H)
        # ç‰¹å¾å›¾: WfÃ—Hf
        scale_w = Wf / 640.0  # å®½åº¦ç¼©æ”¾
        scale_h = Hf / 480.0  # é«˜åº¦ç¼©æ”¾

        # å†…å‚ç¼©æ”¾ï¼šä¿æŒx/yæ–¹å‘å¯¹åº”å…³ç³»
        fx_feat = fx0 * scale_w  # xæ–¹å‘ç„¦è·éšå®½åº¦ç¼©æ”¾
        fy_feat = fy0 * scale_h  # yæ–¹å‘ç„¦è·éšé«˜åº¦ç¼©æ”¾
        cx_feat = cx0 * scale_w  # xæ–¹å‘ä¸»ç‚¹éšå®½åº¦ç¼©æ”¾
        cy_feat = cy0 * scale_h  # yæ–¹å‘ä¸»ç‚¹éšé«˜åº¦ç¼©æ”¾

        if self.debug:
            print(f"ğŸ”§ å†…å‚ç¼©æ”¾: å®½åº¦ç¼©æ”¾={scale_w:.3f}, é«˜åº¦ç¼©æ”¾={scale_h:.3f}")
            print(f"ğŸ”§ è®¡ç®—ç»“æœ: fx={fx_feat:.1f}, fy={fy_feat:.1f}, cx={cx_feat:.1f}, cy={cy_feat:.1f}")

        return (fx_feat, fy_feat, cx_feat, cy_feat)


    def get_pose_pick_stats(self):
        return dict(self._pose_pick_stats)

    def reset_pose_pick_stats(self):
        self._pose_pick_stats = {'direct': 0, 'inv': 0}
    
    def _ensure_precomp_adapter(self, c_in: int):
        """æƒ°æ€§åˆå§‹åŒ–é¢„è®¡ç®—ç‰¹å¾é€‚é…å™¨ï¼š512 â†’ 256"""
        if (self.precomp_adapter is None) or (self.precomp_adapter[0].in_features != c_in):
            # æŒ‰ç…§ä¼˜åŒ–æŒ‡å—è¦æ±‚ï¼šLinear(512â†’256) + LayerNorm
            self.precomp_adapter = nn.Sequential(
                nn.Linear(c_in, 256),
                nn.LayerNorm(256)
            ).to(next(self.parameters()).device)
            if self.debug:
                print(f"ğŸ”§ åˆå§‹åŒ–é¢„è®¡ç®—é€‚é…å™¨: {c_in} â†’ 256 (ä¼˜åŒ–ç‰ˆæœ¬)")
    
    def get_grad_stats(self):
        """è·å–æ¢¯åº¦å¥åº·åº¦ç»Ÿè®¡"""
        stats = {}
        for name, module in [("head2d", self.head2d), ("head3d", self.head3d), ("gate", self.fusion_gate)]:
            total = 0.0
            cnt = 0
            for p in module.parameters():
                if p.grad is not None:
                    total += p.grad.data.norm().item()
                    cnt += 1
            stats[f"grad_{name}"] = total / max(cnt, 1)
        return stats
    
    def update_loss_stat(self, loss_val: float):
        """æ›´æ–°æŸå¤±å†å²è®°å½•"""
        self._loss_hist.append(float(loss_val))
    
    def get_loss_var(self):
        """è·å–æŸå¤±æ»‘çª—æ–¹å·®"""
        if len(self._loss_hist) < 20:
            return None
        arr = torch.tensor(list(self._loss_hist))
        return float(arr.var(unbiased=False))
    
    def _log_key_metrics(self, valid: torch.Tensor, conf: torch.Tensor):
        """ç®€åŒ–ç›‘æ§è¾“å‡ºï¼šä»…è¾“å‡ºå…³é”®æŒ‡æ ‡"""
        # ğŸ”§ ä¿®å¤ï¼šå§‹ç»ˆè¾“å‡ºå…³é”®æŒ‡æ ‡ï¼Œä¸å—debugæ¨¡å¼é™åˆ¶
        # if not self.debug:
        #     return  # éè°ƒè¯•æ¨¡å¼ä¸è¾“å‡º
            
        with torch.no_grad():
            # 1. Validæ¯”ä¾‹
            valid_ratio = valid.float().mean().item()
            
            # 2. Fusion gateå‚æ•°ï¼ˆalphaç»Ÿè®¡ï¼‰- åªè®¡ç®—æœ‰æ•ˆç‚¹çš„alpha
            alpha = conf.squeeze(-1) if conf.dim() == 2 else conf  # (N,)
            
            if valid.any():
                # åªç»Ÿè®¡æœ‰æ•ˆæŠ•å½±ç‚¹çš„alpha
                alpha_valid = alpha[valid]
                alpha_mean = float(alpha_valid.mean()) if alpha_valid.numel() else 0.0
                alpha_std = float(alpha_valid.std(unbiased=False)) if alpha_valid.numel() > 1 else 0.0
            else:
                # æ²¡æœ‰æœ‰æ•ˆç‚¹æ—¶çš„å¤„ç†
                alpha_mean = 0.0
                alpha_std = 0.0
            
            # ğŸ”§ å¢å¼ºè¾“å‡ºæ ¼å¼ï¼šåŒ…å«èåˆæ¯”ä¾‹ç»Ÿè®¡
            fusion_2d_ratio = alpha_mean  # Î±è¡¨ç¤º2Dç‰¹å¾æƒé‡
            fusion_3d_ratio = 1.0 - alpha_mean  # 1-Î±è¡¨ç¤º3Dç‰¹å¾æƒé‡
            
            print(f"ğŸ¯ Validæ¯”ä¾‹: {valid_ratio:.3f} | Fusion-Î±: å‡å€¼={alpha_mean:.3f}Â±{alpha_std:.3f}")
            print(f"ğŸ¯ èåˆæ¯”ä¾‹: 2D={fusion_2d_ratio:.3f} | 3D={fusion_3d_ratio:.3f} | æ€»ç‚¹æ•°={valid.numel()}")
            
            # å¦‚æœvalidæ¯”ä¾‹ä¸º0ï¼Œè¾“å‡ºè°ƒè¯•ä¿¡æ¯
            if valid_ratio == 0.0:
                print(f"âš ï¸ DEBUG: validå…¨ä¸º0ï¼Œæ€»ç‚¹æ•°={valid.numel()}")
                
            # ğŸ”§ æ·»åŠ èåˆæ¨¡å¼åˆ†æ
            if valid.any():
                if alpha_mean < 0.2:
                    print(f"ğŸ“Š èåˆæ¨¡å¼: ä¸»è¦ä½¿ç”¨3Dç‰¹å¾ (Î±={alpha_mean:.3f})")
                elif alpha_mean > 0.8:
                    print(f"ğŸ“Š èåˆæ¨¡å¼: ä¸»è¦ä½¿ç”¨2Dç‰¹å¾ (Î±={alpha_mean:.3f})")
                else:
                    print(f"ğŸ“Š èåˆæ¨¡å¼: å¹³è¡¡èåˆ (Î±={alpha_mean:.3f})")
            
            # å¯é…ç½®çš„æœ‰æ•ˆæ¯”ä¾‹è­¦å‘Š
            if self.warn_valid_ratio and valid_ratio < self.warn_valid_ratio:
                print(f"âš ï¸ æœ‰æ•ˆæ¯”ä¾‹è¿‡ä½: {valid_ratio:.3f} < {self.warn_valid_ratio}")
    
    def _collect_fusion_statistics(self, conf: torch.Tensor, valid: torch.Tensor, 
                                 f2d: torch.Tensor, f3d: torch.Tensor):
        """æ”¶é›†èåˆé—¨æ§ç»Ÿè®¡ä¿¡æ¯ - ğŸ”§ åªç»Ÿè®¡validç‚¹"""
        try:
            with torch.no_grad():
                # åŸºç¡€ç»Ÿè®¡
                if conf.dim() == 2:  # (N, 1)
                    conf_values = conf.squeeze(-1)  # (N,)
                else:
                    conf_values = conf
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåªç»Ÿè®¡validç‚¹ï¼Œé¿å…invalidç‚¹æ±¡æŸ“ç»Ÿè®¡
                if valid.any():
                    # åªä½¿ç”¨æœ‰æ•ˆç‚¹è¿›è¡Œç»Ÿè®¡
                    valid_conf = conf_values[valid]
                    valid_f2d = f2d[valid]
                    valid_f3d = f3d[valid]
                    
                    # è®¡ç®—èåˆæ¯”ä¾‹ï¼ˆåŸºäºæœ‰æ•ˆç‚¹ï¼‰
                    fusion_2d_ratio = valid_conf.mean().item()
                    fusion_3d_ratio = 1.0 - fusion_2d_ratio
                    avg_confidence = valid_conf.mean().item()
                    
                    # ç‰¹å¾è´¨é‡ç»Ÿè®¡ï¼ˆåŸºäºæœ‰æ•ˆç‚¹ï¼‰
                    f2d_norm = torch.norm(valid_f2d, dim=-1).mean().item()
                    f3d_norm = torch.norm(valid_f3d, dim=-1).mean().item()
                    
                    # ç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆåŸºäºæœ‰æ•ˆç‚¹ï¼‰
                    cos_sim = F.cosine_similarity(valid_f2d, valid_f3d, dim=-1).mean().item()
                    
                    total_valid_points = valid.sum().item()
                else:
                    # æ²¡æœ‰æœ‰æ•ˆç‚¹çš„fallback
                    fusion_2d_ratio = 0.0
                    fusion_3d_ratio = 1.0  
                    avg_confidence = 0.0
                    f2d_norm = 0.0
                    f3d_norm = 0.0
                    cos_sim = 0.0
                    total_valid_points = 0
                
                # æœ‰æ•ˆç‚¹æ¯”ä¾‹ï¼ˆç›¸å¯¹äºæ€»ç‚¹æ•°ï¼‰
                valid_points_ratio = valid.float().mean().item()
                
                # ğŸ”§ è®¡ç®—alphaåˆ†å¸ƒç»Ÿè®¡ï¼ˆåŸºäºæœ‰æ•ˆç‚¹ï¼‰
                if valid.any():
                    valid_alpha = conf_values[valid]
                    alpha_mean = float(valid_alpha.mean())
                    alpha_std = float(valid_alpha.std(unbiased=False)) if valid_alpha.numel() > 1 else 0.0
                    alpha_min = float(valid_alpha.min())
                    alpha_max = float(valid_alpha.max())
                else:
                    alpha_mean = avg_confidence  # ä½¿ç”¨æ€»ä½“å‡å€¼ä½œä¸ºfallback
                    alpha_std = 0.0
                    alpha_min = 0.0
                    alpha_max = 1.0
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ - ğŸ”§ åŒ…å«å®Œæ•´çš„alphaç»Ÿè®¡
                self._fusion_stats = {
                    'fusion_2d_ratio': fusion_2d_ratio,
                    'fusion_3d_ratio': fusion_3d_ratio, 
                    'avg_confidence': avg_confidence,
                    'valid_points_ratio': valid_points_ratio,
                    'f2d_norm_avg': f2d_norm,
                    'f3d_norm_avg': f3d_norm,
                    'feature_similarity': cos_sim,
                    'total_points': conf_values.numel(),
                    'total_valid_points': total_valid_points,
                    # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„alphaç»Ÿè®¡
                    'alpha_mean': alpha_mean,
                    'alpha_std': alpha_std,
                    'alpha_min': alpha_min,
                    'alpha_max': alpha_max,
                    'cos_2d3d_mean': cos_sim,  # åˆ«åï¼Œç¡®ä¿å…¼å®¹æ€§
                    'norm_ratio_2d_over_3d': f2d_norm / max(f3d_norm, 1e-8),  # é¿å…é™¤é›¶
                    'valid_ratio': valid_points_ratio,  # åˆ«åï¼Œç¡®ä¿å…¼å®¹æ€§
                    'in_feat': 1.0  # ç‰¹å¾è¾“å…¥çŠ¶æ€
                }
                
                # ä¿ç•™å†å²è®°å½•ï¼ˆæœ€å¤š100æ¡ï¼‰
                self._stats_history.append(self._fusion_stats.copy())
                if len(self._stats_history) > 100:
                    self._stats_history.pop(0)
                    
        except Exception as e:
            if self.debug:
                print(f"Warning: Failed to collect fusion stats: {e}")
    
    def get_fusion_statistics(self):
        """è·å–èåˆç»Ÿè®¡ä¿¡æ¯"""
        return self._fusion_stats.copy() if self._fusion_stats else {}
    
    def get_fusion_ratios(self):
        """ä¸“é—¨è·å–èåˆæ¯”ä¾‹ç»Ÿè®¡ - ä¾›Hookä½¿ç”¨"""
        if not self._fusion_stats:
            return {'fusion_2d_ratio': 0.0, 'fusion_3d_ratio': 1.0, 'valid_points_ratio': 0.0}
        
        return {
            'fusion_2d_ratio': self._fusion_stats.get('fusion_2d_ratio', 0.0),
            'fusion_3d_ratio': self._fusion_stats.get('fusion_3d_ratio', 1.0), 
            'valid_points_ratio': self._fusion_stats.get('valid_points_ratio', 0.0),
            'avg_confidence': self._fusion_stats.get('avg_confidence', 0.0),
            'feature_similarity': self._fusion_stats.get('feature_similarity', 0.0)
        }
    
    def get_fusion_balance_loss(self):
        """è·å–èåˆå¹³è¡¡æŸå¤± - ä¾›ä¸»æŸå¤±å‡½æ•°ä½¿ç”¨"""
        return getattr(self, '_fusion_balance_loss', None)
    
    def get_statistics_summary(self, last_n: int = 10):
        """è·å–æœ€è¿‘Næ¬¡çš„ç»Ÿè®¡æ‘˜è¦"""
        if not self._stats_history:
            return {}
            
        recent_stats = self._stats_history[-last_n:]
        summary = {}
        
        for key in recent_stats[0].keys():
            if key != 'total_points':
                values = [stats[key] for stats in recent_stats if key in stats]
                if values:
                    summary[f'{key}_mean'] = sum(values) / len(values)
                    summary[f'{key}_std'] = (sum((x - summary[f'{key}_mean'])**2 for x in values) / len(values))**0.5
        
        return summary

    # åˆ é™¤äº†å¤æ‚çš„ _improved_projection_with_geometry å‡½æ•°ï¼Œ
    # ç»Ÿä¸€ä½¿ç”¨ unified_projection_and_sample

    def _pixels_to_grid(self, uv_feat: torch.Tensor,
                        feat_hw: Tuple[int,int],
                        align_corners: bool = True) -> torch.Tensor:
        """
        å…³é”®ä¿®å¤ï¼šç»Ÿä¸€grid_sampleè§„èŒƒåŒ–æ ‡å‡†
        æŠŠåƒç´ åæ ‡ (u,v) è½¬ä¸º grid_sample éœ€è¦çš„ [-1,1] å½’ä¸€åŒ–åæ ‡ã€‚
        - uv_feat: (M,2) åƒç´ åæ ‡ï¼ˆç‰¹å¾å›¾å°ºåº¦ï¼‰
        - feat_hw: (H_feat, W_feat)
        - è¿”å›: (1, M, 1, 2) çš„ grid
        """
        H, W = feat_hw
        u = uv_feat[:, 0]
        v = uv_feat[:, 1]
        
        if align_corners:
            # ğŸ”§ align_corners=True: è¾¹ç•Œä¸º [0, W-1] [0, H-1]
            # è¿™æ · (0,0) æ˜ å°„åˆ° (-1,-1), (W-1,H-1) æ˜ å°„åˆ° (1,1)
            x_norm = 2.0 * u / max(float(W - 1), 1.0) - 1.0
            y_norm = 2.0 * v / max(float(H - 1), 1.0) - 1.0
        else:
            # align_corners=False: è¾¹ç•Œä¸º [0, W) [0, H)
            x_norm = 2.0 * (u + 0.5) / float(W) - 1.0
            y_norm = 2.0 * (v + 0.5) / float(H) - 1.0
            
        grid = torch.stack([x_norm, y_norm], dim=-1).view(1, -1, 1, 2)
        return grid

    def _sample_img_feat(self, feat_map: torch.Tensor,
                         uv_feat: torch.Tensor,
                         valid_mask: torch.Tensor,
                         align_corners: bool = True) -> torch.Tensor:
        """
        ä»ç‰¹å¾å›¾ (1,C,H,W) é‡‡æ · N ä¸ªç‚¹çš„ç‰¹å¾ã€‚
        - feat_map: (1, C, H, W)
        - uv_feat:  (N, 2) åƒç´ åæ ‡ï¼ˆç‰¹å¾å›¾å°ºåº¦ï¼‰
        - valid_mask: (N,) bool
        - è¿”å›: (N, C)
        """
        assert feat_map.dim() == 4 and feat_map.size(0) == 1
        H, W = feat_map.shape[-2], feat_map.shape[-1]

        # åªå¯¹ valid çš„ç‚¹æ„é€  gridï¼Œå¯ä»¥å‡å°‘è¾¹ç•Œå¼‚å¸¸
        idx = torch.nonzero(valid_mask, as_tuple=False).squeeze(-1)
        if idx.numel() == 0:
            return feat_map.new_zeros((uv_feat.size(0), feat_map.size(1)))

        uv_valid = uv_feat[idx]  # (M,2)
        grid = self._pixels_to_grid(uv_valid, (H, W), align_corners=align_corners)  # 1xMx1x2

        # ç¡®ä¿feat_mapå’Œgridæœ‰ç›¸åŒçš„æ•°æ®ç±»å‹
        if feat_map.dtype != grid.dtype:
            grid = grid.to(feat_map.dtype)

        # é‡‡æ ·: F.grid_sample(1, C, H, W), (1, M, 1, 2) -> (1, C, 1, M)
        sampled = F.grid_sample(
            feat_map, grid, mode='bilinear',
            align_corners=align_corners
        ).squeeze(3).squeeze(0).T  # (1, C, M) -> (C, M) -> (M, C)

        out = feat_map.new_zeros((uv_feat.size(0), feat_map.size(1)))
        out[idx] = sampled
        return out

    def unified_projection_and_sample(self,
                                      xyz_cam: torch.Tensor,
                                      feat_map: torch.Tensor):
        """
        ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šåŠ¨æ€å†…å‚æ¢ç®—ï¼Œè§£å†³validæ¯”ä¾‹ä¸º0çš„é—®é¢˜
        
        æ ¸å¿ƒåŸåˆ™ï¼š
        1. æ ¹æ®å½“å‰ç‰¹å¾å›¾å°ºå¯¸åŠ¨æ€è®¡ç®—å†…å‚ï¼Œæ”¯æŒä»»æ„HÃ—W
        2. ä¸¥æ ¼çš„è¾¹ç•Œå’Œæ·±åº¦æ£€æŸ¥
        
        Args:
            xyz_cam: (N, 3) ç›¸æœºåæ ‡ç³»ç‚¹äº‘
            intr: (4,) åŸå›¾å†…å‚ [fx, fy, cx, cy] - å·²åºŸå¼ƒï¼Œæ”¹ç”¨åŠ¨æ€è®¡ç®—
            feat_map: (1, C, H, W) ç‰¹å¾å›¾
        Returns:
            sampled_feat: (N, C) é‡‡æ ·ç‰¹å¾
            valid_mask: (N,) æœ‰æ•ˆæŠ•å½±æ©ç 
        """
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥å†…å‚ï¼Œå¦åˆ™åŠ¨æ€è®¡ç®—
        Hf, Wf = feat_map.shape[2], feat_map.shape[3]
        fx_feat, fy_feat, cx_feat, cy_feat = self._intrinsics_for_feat(Hf, Wf)

        # 3DæŠ•å½±ï¼šç›¸æœºåæ ‡ç³» â†’ ç‰¹å¾å›¾åƒç´ åæ ‡
        x, y, z = xyz_cam[:, 0], xyz_cam[:, 1], xyz_cam[:, 2]
        
        # ğŸ” ç›¸æœºåæ ‡è¯Šæ–­ - é™ä½è¾“å‡ºé¢‘ç‡
        if self.debug and torch.rand(1).item() < 0.05:  # 5%æ¦‚ç‡è¾“å‡º
            print(f"ğŸ” ç›¸æœºåæ ‡è¯Šæ–­:")
            print(f"   XèŒƒå›´: [{x.min():.3f}, {x.max():.3f}]")
            print(f"   YèŒƒå›´: [{y.min():.3f}, {y.max():.3f}]") 
            print(f"   ZèŒƒå›´: [{z.min():.3f}, {z.max():.3f}]")
            print(f"   å†…å‚: fx={fx_feat:.1f}, fy={fy_feat:.1f}, cx={cx_feat:.1f}, cy={cy_feat:.1f}")
        
        # ğŸ›¡ï¸ ä¸¥æ ¼çš„æ·±åº¦è¿‡æ»¤ - æé«˜é˜ˆå€¼é¿å…æŠ•å½±çˆ†ç‚¸
        min_depth = 0.5  # æé«˜æœ€å°æ·±åº¦é˜ˆå€¼åˆ°0.5m
        valid_z = (z > min_depth) & (z < self.max_depth)
        
        # åªå¯¹æœ‰æ•ˆæ·±åº¦çš„ç‚¹è¿›è¡ŒæŠ•å½±è®¡ç®—ï¼Œæ— æ•ˆç‚¹è®¾ä¸ºè¾¹ç•Œå¤–å€¼
        u_feat = torch.full_like(x, -1.0)  # æ— æ•ˆç‚¹è®¾ä¸º-1
        v_feat = torch.full_like(y, -1.0)  # æ— æ•ˆç‚¹è®¾ä¸º-1
        
        # åªå¯¹æœ‰æ•ˆæ·±åº¦çš„ç‚¹è¿›è¡ŒæŠ•å½±
        if valid_z.any():
            valid_x, valid_y, valid_z_vals = x[valid_z], y[valid_z], z[valid_z]
            u_valid = fx_feat * (valid_x / valid_z_vals) + cx_feat
            v_valid = fy_feat * (valid_y / valid_z_vals) + cy_feat
            
            # æ£€æŸ¥æŠ•å½±ç»“æœæ˜¯å¦åˆç†ï¼ˆç²—ç•¥èŒƒå›´æ£€æŸ¥ï¼‰
            reasonable_u = (u_valid > -1000) & (u_valid < 1000)  # å…è®¸è¾ƒå¤§èŒƒå›´ä½†æ’é™¤æå€¼
            reasonable_v = (v_valid > -1000) & (v_valid < 1000)
            reasonable_proj = reasonable_u & reasonable_v
            
            if self.debug and torch.rand(1).item() < 0.1:
                unreasonable_count = (~reasonable_proj).sum().item()
                if unreasonable_count > 0:
                    print(f"âš ï¸ æŠ•å½±å¼‚å¸¸: {unreasonable_count}/{len(u_valid)} ç‚¹æŠ•å½±åæ ‡å¼‚å¸¸")
                    print(f"   Uå¼‚å¸¸èŒƒå›´: [{u_valid[~reasonable_u].min():.1f}, {u_valid[~reasonable_u].max():.1f}]" if (~reasonable_u).any() else "   Uæ­£å¸¸")
                    print(f"   Vå¼‚å¸¸èŒƒå›´: [{v_valid[~reasonable_v].min():.1f}, {v_valid[~reasonable_v].max():.1f}]" if (~reasonable_v).any() else "   Væ­£å¸¸")
            
            # åªä¿ç•™åˆç†çš„æŠ•å½±ç»“æœ
            final_valid_mask = valid_z.clone()
            final_valid_mask[valid_z] = reasonable_proj
            
            u_feat[final_valid_mask] = u_valid[reasonable_proj]
            v_feat[final_valid_mask] = v_valid[reasonable_proj]
            
            # æ›´æ–°æœ‰æ•ˆæ·±åº¦æ©ç 
            valid_z = final_valid_mask
        
        # è¾¹ç•Œæ£€æŸ¥ï¼šä¸æµ‹è¯•è„šæœ¬ä¿æŒä¸€è‡´
        valid_u = (u_feat >= 0) & (u_feat < Wf)
        valid_v = (v_feat >= 0) & (v_feat < Hf)
        
        # ç»¼åˆæœ‰æ•ˆæ€§åˆ¤å®š
        valid = valid_z & valid_u & valid_v
        
        # ğŸš¨ å…³é”®è°ƒè¯•ï¼šæ£€æŸ¥æŠ•å½±åæ ‡åˆ†å¸ƒï¼ˆä¸´æ—¶å¯ç”¨ï¼‰
        total_points = len(z)
        depth_valid = valid_z.sum().item()
        boundary_valid = valid.sum().item()
        
        # åæ ‡ç»Ÿè®¡
        u_min, u_max = u_feat.min().item(), u_feat.max().item()
        v_min, v_max = v_feat.min().item(), v_feat.max().item()
        z_min, z_max = z.min().item(), z.max().item()
        
        if total_points > 0 and boundary_valid < total_points * 0.8:  # æœ‰æ•ˆç‡ä½äº80%æ—¶è¾“å‡º
            print(f"ğŸ” æŠ•å½±åæ ‡è¯Šæ–­({Hf}Ã—{Wf}):")
            print(f"   æ€»ç‚¹æ•°: {total_points}")
            print(f"   æ·±åº¦èŒƒå›´: [{z_min:.3f}, {z_max:.3f}]m, æœ‰æ•ˆæ·±åº¦: {depth_valid}/{total_points} ({100*depth_valid/total_points:.1f}%)")
            print(f"   Uåæ ‡èŒƒå›´: [{u_min:.1f}, {u_max:.1f}], ç›®æ ‡[0, {Wf})")
            print(f"   Våæ ‡èŒƒå›´: [{v_min:.1f}, {v_max:.1f}], ç›®æ ‡[0, {Hf})")
            print(f"   æœ€ç»ˆæœ‰æ•ˆ: {boundary_valid}/{total_points} ({100*boundary_valid/total_points:.1f}%)")
                    
        # ç‰¹å¾é‡‡æ · - ä½¿ç”¨align_cornersç¡®ä¿ä¸€è‡´æ€§
        uv_feat = torch.stack([u_feat, v_feat], dim=-1)  # (N, 2)
        sampled_feat = self._sample_img_feat(feat_map, uv_feat, valid, align_corners=self.align_corners)
        
        return sampled_feat, valid

    def _process_single(self, points: torch.Tensor, img: torch.Tensor, cam_meta: Dict, sample_idx: int = 0):
        """å¤„ç†å•å¸§æ•°æ®ï¼Œä½¿ç”¨ç®€åŒ–çš„æ•°æ®æµ"""
        # æå–åŸºç¡€ä¿¡æ¯
        xyz = points[:, :3].contiguous()  
        dev = xyz.device
        dtype = xyz.dtype
        
        # ğŸ”§ ä¼˜åŒ–poseè§£æ - ç›´æ¥æå–å½“å‰æ ·æœ¬çš„pose
        pose_matrix = None
        if isinstance(cam_meta, dict) and 'pose' in cam_meta:
            pose_data = cam_meta['pose']
            if isinstance(pose_data, list) and len(pose_data) > sample_idx:
                # PKLæ–‡ä»¶ä¸­çš„poseæ˜¯listï¼Œé€‰æ‹©å½“å‰æ ·æœ¬å¯¹åº”çš„pose
                pose_matrix = pose_data[sample_idx]
            elif isinstance(pose_data, (list, tuple, np.ndarray)) and len(pose_data) == 1:
                # å•ä¸ªposeçš„æƒ…å†µ
                pose_matrix = pose_data[0] if isinstance(pose_data, (list, tuple)) else pose_data
            else:
                # ç›´æ¥ä½¿ç”¨pose_data
                pose_matrix = pose_data
        
        if self.debug:
            print(f"ğŸ” æ ·æœ¬{sample_idx} poseçŸ©é˜µç±»å‹: {type(pose_matrix)}")
            if pose_matrix is not None:
                if hasattr(pose_matrix, 'shape') and not isinstance(pose_matrix, (list, tuple)):
                    print(f"ğŸ” poseçŸ©é˜µå½¢çŠ¶: {pose_matrix.shape}")
                elif isinstance(pose_matrix, (list, tuple)):
                    print(f"ğŸ” poseçŸ©é˜µé•¿åº¦: {len(pose_matrix)}")

        # ğŸ¯ åæ ‡è½¬æ¢ï¼šä¸–ç•Œåæ ‡ â†’ ç›¸æœºåæ ‡
        if pose_matrix is None:
            # æ²¡æœ‰poseçŸ©é˜µï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åæ ‡
            xyz_cam_proj = xyz.clone()
            if self.debug:
                print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°poseçŸ©é˜µï¼Œä½¿ç”¨åŸå§‹åæ ‡")
        else:
            try:
                # ç¡®ä¿poseçŸ©é˜µä¸ºtorchå¼ é‡
                if not isinstance(pose_matrix, torch.Tensor):
                    T_matrix = torch.as_tensor(pose_matrix, dtype=dtype, device=dev)
                else:
                    T_matrix = pose_matrix.to(dtype=dtype, device=dev)

                # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥çŸ©é˜µå±æ€§
                if self.debug:
                    print(f"ğŸ” T_matrixå½¢çŠ¶: {T_matrix.shape}")
                    print(f"ğŸ” T_matrixè®¾å¤‡: {T_matrix.device}, ç±»å‹: {T_matrix.dtype}")
                    det = torch.det(T_matrix).item()
                    print(f"ğŸ” T_matrixè¡Œåˆ—å¼: {det}")
                    if torch.isnan(T_matrix).any() or torch.isinf(T_matrix).any():
                        print(f"âš ï¸ T_matrixåŒ…å«NaN/Infå€¼")

                # poseæ˜¯C2Wæ ¼å¼ï¼Œæ±‚é€†å¾—åˆ°W2Cå˜æ¢çŸ©é˜µ
                W2C = torch.inverse(T_matrix)

                # é½æ¬¡åæ ‡å˜æ¢ï¼šä¸–ç•Œåæ ‡ â†’ ç›¸æœºåæ ‡
                xyz1 = torch.cat([xyz, torch.ones(xyz.shape[0], 1, device=dev, dtype=dtype)], dim=1)
                xyz_cam_proj = (xyz1 @ W2C.t())[:, :3]

                # è°ƒè¯•è¾“å‡º
                if self.debug:
                    z_cam = xyz_cam_proj[:, 2]
                    neg_z = (z_cam < 0).sum().item()
                    print(f"åæ ‡è½¬æ¢å®Œæˆ: {xyz_cam_proj.shape}, è´Ÿæ·±åº¦ç‚¹={neg_z}")

            except (RuntimeError, torch.linalg.LinAlgError) as e:
                # çŸ©é˜µæ±‚é€†å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åæ ‡
                xyz_cam_proj = xyz.clone()
                if self.debug:
                    print(f"åæ ‡è½¬æ¢å¼‚å¸¸ï¼Œä½¿ç”¨åŸå§‹åæ ‡: {e}")
                    print(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        
        # 3Dåˆ†æ”¯å§‹ç»ˆä½¿ç”¨ä¸–ç•Œåæ ‡
        xyz_world = xyz

        # 3Dåˆ†æ”¯ï¼šMinkUNet â†’ 96d â†’ Proj3D(96â†’256, LN inside) â†’ Head3D(256â†’256, LN inside) â†’ (ä¸åšL2)
        coords_int = torch.round(xyz_world / self.voxel_size).to(torch.int32)
        coords = torch.cat([torch.zeros(coords_int.size(0), 1, dtype=torch.int32, device=coords_int.device),
                             coords_int], dim=1)
        feats = points[:, 3:6].contiguous()
        field = ME.TensorField(coordinates=coords, features=feats)
        sparse_tensor = field.sparse()
        feat3d_sparse = self.backbone3d(sparse_tensor)
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨sliceæ“ä½œå°†ç¨€ç–ç‰¹å¾æ˜ å°„å›åŸå§‹ç‚¹äº‘
        feat3d = feat3d_sparse.slice(field).features
        
        # éªŒè¯ç‰¹å¾æ•°é‡åŒ¹é…ï¼ˆç°åœ¨åº”è¯¥åŒ¹é…äº†ï¼‰
        if feat3d.shape[0] != points.shape[0]:
            raise RuntimeError(f"3D features shape mismatch: got {feat3d.shape[0]}, expected {points.shape[0]}")
        
        # 3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´ (å†…å«LNï¼Œä¸é¢å¤–åšL2)
        feat3d = self.proj_3d(feat3d.float())  # (N, 96) -> (N, 256), ç¡®ä¿floatç±»å‹
        
        # ç»Ÿä¸€Headï¼šä¸åšL2å½’ä¸€åŒ–
        f3d = self.head3d(feat3d)  # (N, 256)

        # ğŸ¯ 2Dç‰¹å¾å¤„ç†ï¼šæŠ•å½±é‡‡æ ·æˆ–é›¶ç‰¹å¾fallback
        if xyz_cam_proj is None:
            # ç›¸æœºæŠ•å½±å¤±è´¥ï¼Œä½¿ç”¨é›¶ç‰¹å¾
            print(f"âš ï¸ ç›¸æœºæŠ•å½±å¤±è´¥ï¼Œä½¿ç”¨é›¶ç‰¹å¾")
            feat2d_raw = f3d.new_zeros((f3d.shape[0], 256))
            valid = f3d.new_zeros((f3d.shape[0],), dtype=torch.bool)
            f2d = self.head2d(feat2d_raw)

        elif isinstance(cam_meta, dict) and cam_meta.get("clip_pix") is not None:
            if self.debug:
                print(f"ğŸ¯ ä½¿ç”¨é¢„è®¡ç®—CLIPç‰¹å¾è¿›è¡ŒæŠ•å½±é‡‡æ ·")
            # æœ‰æœ‰æ•ˆæŠ•å½±å’ŒCLIPç‰¹å¾ï¼Œè¿›è¡ŒæŠ•å½±é‡‡æ ·
            clip_data = cam_meta["clip_pix"]
            
            # å¦‚æœclip_dataæ˜¯listï¼Œæ ¹æ®sample_idxé€‰æ‹©å¯¹åº”çš„ç‰¹å¾
            if isinstance(clip_data, list) and len(clip_data) > sample_idx:
                selected_clip = clip_data[sample_idx]
            elif isinstance(clip_data, (list, tuple)) and len(clip_data) == 1:
                selected_clip = clip_data[0]
            else:
                selected_clip = clip_data
            
            # ç¡®ä¿selected_clipæ˜¯tensor
            if isinstance(selected_clip, torch.Tensor):
                feat_map = selected_clip.to(device=dev, dtype=dtype)
            else:
                feat_map = torch.as_tensor(selected_clip, device=dev, dtype=dtype)
                
            feat_map = feat_map.float().unsqueeze(0)

            # æŠ•å½±é‡‡æ ·
            feat2d_raw, valid = self.unified_projection_and_sample(
                xyz_cam=xyz_cam_proj,
                feat_map=feat_map)

            # é€šé“é€‚é…ï¼š512 â†’ 256
            if feat2d_raw.shape[-1] != 256:
                self._ensure_precomp_adapter(feat2d_raw.shape[-1])
                if self.precomp_adapter is not None:
                    feat2d_raw = self.precomp_adapter(feat2d_raw)

            f2d = self.head2d(feat2d_raw)

        else:
            # ç¼ºå°‘CLIPç‰¹å¾ï¼Œä½¿ç”¨é›¶ç‰¹å¾
            print(f"âš ï¸ ç¼ºå°‘CLIPç‰¹å¾ï¼Œä½¿ç”¨é›¶ç‰¹å¾")
            feat2d_raw = f3d.new_zeros((f3d.shape[0], 256))
            valid = f3d.new_zeros((f3d.shape[0],), dtype=torch.bool)
            f2d = self.head2d(feat2d_raw)

        # èåˆç‰¹å¾
        f2d_batch = f2d.unsqueeze(0)
        f3d_batch = f3d.unsqueeze(0)
        valid_batch = valid.unsqueeze(0)
        
        fused_batch, conf_batch = self.fusion_gate(f2d_batch, f3d_batch, valid_batch)
        fused = fused_batch.squeeze(0)
        conf = conf_batch.squeeze(0)
        
        # L2å½’ä¸€åŒ–
        fused = F.normalize(fused, dim=-1)
        
        # ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
        if self._collect_fusion_stats:
            self._collect_fusion_statistics(conf, valid, f2d, f3d)
        self._log_key_metrics(valid, conf)

        # ğŸ”¥ è®¡ç®—å¹¶ä¿å­˜èåˆå¹³è¡¡æŸå¤±ï¼ˆç”¨äºä¸»æŸå¤±å‡½æ•°ï¼‰
        if self.training:
            fusion_balance_loss = self.fusion_gate.compute_fusion_balance_loss(
                conf, valid, target_ratio=0.4
            )
            # ä¿å­˜åˆ°å…¨å±€å˜é‡ä¸­ï¼Œä¾›æŸå¤±å‡½æ•°è·å–
            globals()['_current_fusion_balance_loss'] = fusion_balance_loss
        else:
            globals()['_current_fusion_balance_loss'] = None

        return fused, conf, valid

    def forward(self, points_list, imgs, cam_info):
        """ç®€åŒ–çš„forwardå‡½æ•°ï¼šæ‰¹é‡å¤„ç†3D-2Dèåˆ"""
        
        # 1. è¾“å…¥æ ¼å¼æ ‡å‡†åŒ–
        if torch.is_tensor(points_list):
            points_list = list(points_list) if points_list.dim() == 3 else [points_list]
        if torch.is_tensor(imgs):
            imgs = list(imgs) if imgs.dim() == 4 else [imgs]
        
        batch_size = len(points_list)
        if len(imgs) != batch_size:
            raise RuntimeError(f"è¾“å…¥é•¿åº¦ä¸åŒ¹é…: points({len(points_list)}) != imgs({len(imgs)})")
        
        # 2. cam_infoæ ‡å‡†åŒ–
        if cam_info is None or isinstance(cam_info, dict):
            cam_info = [cam_info] * batch_size
        elif len(cam_info) == 1:
            cam_info = cam_info * batch_size
        elif len(cam_info) != batch_size:
            raise RuntimeError(f"cam_infoé•¿åº¦({len(cam_info)})ä¸batch_size({batch_size})ä¸åŒ¹é…")
        
        # 3. é€æ ·æœ¬å¤„ç†
        feat_fusion_list, conf_list, valid_mask_list = [], [], []
        
        for idx, (pts, img, meta) in enumerate(zip(points_list, imgs, cam_info)):
            # ç®€åŒ–metaä¿¡æ¯å¤„ç†ï¼šPKLæ–‡ä»¶æ˜¯å¸§çº§ç»„ç»‡ï¼Œç›´æ¥å¤åˆ¶
            meta_std = meta if meta is not None else {}
            
            # å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œä¼ é€’æ ·æœ¬ç´¢å¼•
            fused, conf, valid_mask = self._process_single(pts, img, meta_std, idx)
            
            feat_fusion_list.append(fused)
            conf_list.append(conf)
            valid_mask_list.append(valid_mask)
        
        return {
            'feat_fusion': feat_fusion_list,
            'conf_2d': conf_list,
            'valid_projection_mask': valid_mask_list
        }
