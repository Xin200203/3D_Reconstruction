import torch
import torch.nn as nn
import torch.nn.functional as F
import open_clip
import math
import os
from typing import List, Dict, Optional, Tuple

import MinkowskiEngine as ME
from mmdet3d.registry import MODELS
from .mink_unet import Res16UNet34C
from .tiny_sa import TinySAModule, TinySA2D
from .clip_utils import (
    freeze_clip_except_last_blocks as _freeze_clip_except_last_blocks,
    build_uv_index as _build_uv_index,
    sample_img_feat as _sample_img_feat
)
from types import SimpleNamespace


class EnhancedProjectionHead3D(nn.Module):
    """Enhanced 3D Projection Head: 96ç»´ -> 256ç»´çš„æŠ•å½±å¤´
    
    æŒ‰ç…§ä¼˜åŒ–è„šæœ¬è¦æ±‚ï¼š1Ã—1 SparseConv(96â†’256) + BN + ReLU â†’ L2-Norm
    """
    
    def __init__(self,
                 input_dim: int = 96,
                 output_dim: int = 256,
                 use_dropout: bool = True,
                 dropout_rate: float = 0.1):
        super().__init__()
        
        # å¯¹åº”ç¨€ç–å·ç§¯çš„1Ã—1å·ç§¯æŠ•å½±
        self.projection = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate) if use_dropout else nn.Identity(),
            nn.Linear(output_dim, output_dim),
            nn.BatchNorm1d(output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.any(torch.isnan(x)) or torch.any(torch.isinf(x)):
            print("Warning: NaN/Inf in projection input, clamping")
            x = torch.clamp(x, -10, 10)
        """3Dç‰¹å¾æŠ•å½± (N, 96) -> (N, 256)"""
        return self.projection(x)


class EnhancedProjectionHead2D(nn.Module):
    """Enhanced 2D Projection Head: æ¸è¿›å¼ç»´åº¦å‹ç¼©
    
    æŒ‰ç…§ä¼˜åŒ–è„šæœ¬è¦æ±‚ï¼šLayerNorm(768) â†’ GELU â†’ Linear(768â†’512) â†’ GELU â†’ Linear(512â†’256)
    """
    
    def __init__(self,
                 input_dim: int = 768,
                 hidden_dim: int = 512,
                 output_dim: int = 256,
                 use_dropout: bool = True,
                 dropout_rate: float = 0.1):
        super().__init__()
        
        self.projection = nn.Sequential(
            nn.LayerNorm(input_dim),
            nn.GELU(),
            nn.Linear(input_dim, hidden_dim),
            nn.Dropout(dropout_rate) if use_dropout else nn.Identity(),
            nn.GELU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # ç©ºé—´ç‰¹å¾æŠ•å½± (for spatial features)
        self.spatial_projection = nn.Sequential(
            nn.Conv2d(input_dim, hidden_dim, kernel_size=1),
            nn.BatchNorm2d(hidden_dim),
            nn.GELU(),
            nn.Dropout2d(dropout_rate) if use_dropout else nn.Identity(),
            nn.Conv2d(hidden_dim, output_dim, kernel_size=1),
            nn.BatchNorm2d(output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward_global(self, x: torch.Tensor) -> torch.Tensor:
        """å…¨å±€ç‰¹å¾æŠ•å½± (B, 768) -> (B, 256)"""
        return self.projection(x)
    
    def forward_spatial(self, x: torch.Tensor) -> torch.Tensor:
        """ç©ºé—´ç‰¹å¾æŠ•å½± (B, 768, H, W) -> (B, 256, H, W)"""
        return self.spatial_projection(x)


def build_geo_pe(xyz_world: torch.Tensor, bbox_size: torch.Tensor,
                 pose_delta: torch.Tensor, height: torch.Tensor) -> torch.Tensor:
    """Assemble 64-d geometric positional encoding.
    xyz_world: (N,3) world coordinates
    bbox_size: (N,3) w,h,l
    pose_delta: (9,) repeat to N (R6 + t3)
    height: (N,1)
    return: (N,64)
    """
    N = xyz_world.shape[0]
    # base 3
    feats = [xyz_world]
    # sin/cos 48d (8 freq per axis)
    freq = torch.pow(2, torch.arange(8, device=xyz_world.device, dtype=xyz_world.dtype)) * math.pi
    sin_list = []
    for f in freq:
        sin_list.append(torch.sin(xyz_world * f))
        sin_list.append(torch.cos(xyz_world * f))
    feats.append(torch.cat(sin_list, dim=-1))  # (N,3*2*8)
    feats.append(bbox_size)  # 3
    feats.append(pose_delta.unsqueeze(0).repeat(N, 1))  # 9
    feats.append(height)  # 1
    return torch.cat(feats, dim=-1)  # (N,64)


class EnhancedCLIPEncoder(nn.Module):
    """æ”¹è¿›çš„CLIPç¼–ç å™¨ï¼Œä½¿ç”¨å‰å‡ å±‚Transformer blocks"""
    
    def __init__(self,
                 clip_pretrained: str = 'openai',
                 num_layers: int = 6,
                 freeze_conv1: bool = False,
                 freeze_early_layers: bool = True,
                 target_resolution: int = 224):
        super().__init__()
        
        # æ™ºèƒ½CLIPæ¨¡å‹åŠ è½½ - ä¼˜å…ˆæœ¬åœ°æ–‡ä»¶ï¼Œç½‘ç»œå¤±è´¥æ—¶å›é€€
        try:
            if os.path.exists(clip_pretrained):
                # æœ¬åœ°æ–‡ä»¶è·¯å¾„
                print(f"âœ… ä½¿ç”¨æœ¬åœ°CLIPæƒé‡: {clip_pretrained}")
                self.clip_model, _, self.clip_transform = open_clip.create_model_and_transforms(
                    'ViT-B-16', pretrained=clip_pretrained
                )
            else:
                # é¢„è®¾åç§°æˆ–ç½‘ç»œä¸‹è½½
                print(f"ğŸŒ å°è¯•ç½‘ç»œä¸‹è½½CLIPæƒé‡: {clip_pretrained}")
                self.clip_model, _, self.clip_transform = open_clip.create_model_and_transforms(
                    'ViT-B-16', pretrained=clip_pretrained
                )
        except Exception as e:
            print(f"âš ï¸  CLIPæƒé‡åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æ¨¡å¼: {e}")
            # å›é€€ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½†ä¸åŠ è½½æƒé‡
            self.clip_model, _, self.clip_transform = open_clip.create_model_and_transforms(
                'ViT-B-16', pretrained=None
            )
            # å¦‚æœæœ‰æœ¬åœ°æƒé‡æ–‡ä»¶ï¼Œå°è¯•æ‰‹åŠ¨åŠ è½½
            local_weight = '/home/nebula/xxy/ESAM/data/open_clip_pytorch_model.bin'
            if os.path.exists(local_weight):
                try:
                    import torch
                    state_dict = torch.load(local_weight, map_location='cpu')
                    self.clip_model.load_state_dict(state_dict, strict=False)
                    print(f"âœ… æ‰‹åŠ¨åŠ è½½æœ¬åœ°CLIPæƒé‡æˆåŠŸ: {local_weight}")
                except Exception as load_e:
                    print(f"âŒ æ‰‹åŠ¨åŠ è½½æœ¬åœ°æƒé‡ä¹Ÿå¤±è´¥: {load_e}")
                    print("ğŸ”„ ç»§ç»­ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„CLIPæ¨¡å‹")
        
        self.clip_visual = self.clip_model.visual
        self.num_layers = num_layers
        self.target_resolution = target_resolution
        
        # æ™ºèƒ½å†»ç»“ç­–ç•¥
        self._setup_freezing(freeze_conv1, freeze_early_layers)
        
        # æ”¹è¿›çš„2DæŠ•å½±å¤´ï¼šæ¸è¿›å¼ç»´åº¦å‹ç¼© 768->512->256
        self.enhanced_proj_2d = EnhancedProjectionHead2D(
            input_dim=768,
            hidden_dim=512,
            output_dim=256,
            use_dropout=True,
            dropout_rate=0.1
        )
        
    def _setup_freezing(self, freeze_conv1: bool, freeze_early_layers: bool):
        """æ™ºèƒ½å†»ç»“ç­–ç•¥"""
        for name, param in self.clip_visual.named_parameters():
            if 'conv1' in name:
                param.requires_grad = not freeze_conv1
            elif 'positional_embedding' in name or 'class_embedding' in name:
                param.requires_grad = not freeze_conv1
            elif 'ln_pre' in name:
                param.requires_grad = not freeze_conv1
            elif 'transformer.resblocks' in name:
                layer_idx = int(name.split('.')[2])
                if freeze_early_layers and layer_idx < 3:
                    param.requires_grad = False
                elif layer_idx < self.num_layers:
                    param.requires_grad = True
                else:
                    param.requires_grad = False
            else:
                param.requires_grad = False
                
        # æ‰“å°å†»ç»“çŠ¶æ€
        total_params = sum(p.numel() for p in self.clip_visual.parameters())
        trainable_params = sum(p.numel() for p in self.clip_visual.parameters() if p.requires_grad)
        print(f"Enhanced CLIP: {trainable_params:,}/{total_params:,} "
              f"å‚æ•°å¯è®­ç»ƒ ({trainable_params/total_params*100:.1f}%)")
    
    def forward(self, images: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        B = images.shape[0]
        
        # Resizeåˆ°CLIPæ ‡å‡†å°ºå¯¸
        if images.shape[-2:] != (self.target_resolution, self.target_resolution):
            images = F.interpolate(images, size=(self.target_resolution, self.target_resolution), 
                                 mode='bilinear', align_corners=False)
        
        # Patch embedding
        x = self.clip_visual.conv1(images)  # (B, 768, 14, 14)
        spatial_raw = x
        
        # Reshape for transformer
        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)  # (B, 196, 768)
        
        # Add class token and positional embedding
        class_token = self.clip_visual.class_embedding.to(x.dtype) + torch.zeros(
            x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device
        )
        x = torch.cat([class_token, x], dim=1)  # (B, 197, 768)
        x = x + self.clip_visual.positional_embedding.to(x.dtype)
        x = self.clip_visual.ln_pre(x)
        
        # é€šè¿‡å‰num_layerså±‚Transformer
        x = x.permute(1, 0, 2)  # NLD -> LND
        # ä½¿ç”¨ type: ignore æ¥å¤„ç†CLIPå†…éƒ¨ç»“æ„çš„ç±»å‹æ£€æŸ¥é—®é¢˜
        for i in range(self.num_layers):
            x = self.clip_visual.transformer.resblocks[i](x)  # type: ignore
        x = x.permute(1, 0, 2)  # LND -> NLD
        
        # é‡å»ºç©ºé—´ç‰¹å¾
        patch_tokens = x[:, 1:, :].permute(0, 2, 1).reshape(B, 768, 14, 14)
        fused_spatial = patch_tokens + spatial_raw  # æ®‹å·®è¿æ¥
        spatial_feat = self.enhanced_proj_2d.forward_spatial(fused_spatial)  # (B, 256, 14, 14)
        
        # å…¨å±€ç‰¹å¾
        cls_token = x[:, 0, :]  # (B, 768)
        global_feat = self.enhanced_proj_2d.forward_global(cls_token)  # (B, 256)
        
        # L2å½’ä¸€åŒ–åˆ°å•ä½çƒé¢ (æŒ‰ç…§ä¼˜åŒ–è„šæœ¬è¦æ±‚)
        global_feat = F.normalize(global_feat, dim=-1)
        # å¯¹ç©ºé—´ç‰¹å¾çš„æ¯ä¸ªä½ç½®è¿›è¡ŒL2å½’ä¸€åŒ–
        B, C, H, W = spatial_feat.shape
        spatial_feat = F.normalize(spatial_feat.view(B, C, -1), dim=1).view(B, C, H, W)
        
        return spatial_feat, global_feat


class LiteFusionGate(nn.Module):
    """Lite Fusion Gate - è½»é‡çº§èåˆé—¨æ§æœºåˆ¶
    
    æŒ‰ç…§ä¼˜åŒ–è„šæœ¬è¦æ±‚ï¼šç‚¹çº§èåˆ + é€šé“çº§SEæ³¨æ„åŠ›
    å‚æ•°é‡çº¦0.12Mï¼Œè¿œä½äºåŸEnhancedGate
    """
    
    def __init__(self, 
                 feat_dim: int = 256,
                 early_steps: int = 3000):
        super().__init__()
        
        self.feat_dim = feat_dim
        self.early_steps = early_steps
        self.training_step = 0
        
        # ç‚¹çº§èåˆæƒé‡MLP: Linear(512â†’64â†’1) + Sigmoid
        self.point_mlp = nn.Sequential(
            nn.Linear(feat_dim * 2, 64),  # 256*2 -> 64
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # SEé€šé“æ³¨æ„åŠ›æ¨¡å—
        self.se_module = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Conv1d(feat_dim, feat_dim // 16, 1),
            nn.ReLU(),
            nn.Conv1d(feat_dim // 16, feat_dim, 1),
            nn.Sigmoid()
        )
        
        # æ—©æœŸå†»ç»“ï¼šå‰3000æ­¥Î±å›ºå®šä¸º0.5
        self.register_buffer('frozen_alpha', torch.tensor(0.5))
        
    def forward(self, 
                f2d: torch.Tensor, 
                f3d: torch.Tensor, 
                valid_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            f2d: 2Dç‰¹å¾ (B, N, 256)
            f3d: 3Dç‰¹å¾ (B, N, 256) 
            valid_mask: æœ‰æ•ˆæŠ•å½±æ©ç  (B, N)
        Returns:
            fused_feat: èåˆç‰¹å¾ (B, N, 256)
            confidence: èåˆç½®ä¿¡åº¦ (B, N, 1)
        """
        B, N, C = f2d.shape
        
        # 1. è®¡ç®—ç‚¹çº§èåˆæƒé‡Î±
        if self.training and self.training_step < self.early_steps:
            # æ—©æœŸå†»ç»“é˜¶æ®µï¼šÎ± = 0.5
            alpha_raw = self.frozen_alpha.expand(B, N, 1)
        else:
            # æ­£å¸¸è®­ç»ƒé˜¶æ®µï¼šÎ± = Ïƒ(MLP([fâ‚‚Dâ€–fâ‚ƒD]))
            feat_concat = torch.cat([f2d, f3d], dim=-1)  # (B, N, 512)
            alpha_raw = self.point_mlp(feat_concat)  # (B, N, 1)
        
        # 2. åº”ç”¨æœ‰æ•ˆæ©ç è°ƒæ•´ï¼šÎ± = Î±*valid + 0.1*(1-valid)
        valid_mask_expanded = valid_mask.unsqueeze(-1).float()  # (B, N, 1)
        alpha = alpha_raw * valid_mask_expanded + 0.1 * (1 - valid_mask_expanded)
        
        # 3. ç‚¹çº§èåˆï¼šf_mix = Î±Â·fâ‚‚D + (1-Î±)Â·fâ‚ƒD
        f_mix = alpha * f2d + (1 - alpha) * f3d  # (B, N, 256)
        
        # 4. SEé€šé“é‡åŠ æƒï¼šw = Ïƒ(SE(f_mix))
        # è°ƒæ•´ç»´åº¦é€‚é…SEæ¨¡å—ï¼š(B, N, C) -> (B, C, N)
        f_mix_transposed = f_mix.permute(0, 2, 1)  # (B, 256, N)
        se_weights = self.se_module(f_mix_transposed)  # (B, 256, 1)
        se_weights = se_weights.permute(0, 2, 1)  # (B, 1, 256)
        
        # 5. åº”ç”¨SEæƒé‡ï¼šfused = wâŠ™f_mix
        fused_feat = se_weights * f_mix  # (B, N, 256)
        
        # è¿”å›èåˆç‰¹å¾å’Œç½®ä¿¡åº¦
        confidence = alpha  # èåˆæƒé‡å¯ä½œä¸ºç½®ä¿¡åº¦
        
        # æ”¶é›†èåˆç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self, "collect_stats") and self.collect_stats:
            fusion_2d_ratio = alpha.mean().item()
            fusion_3d_ratio = (1 - alpha).mean().item()
            avg_confidence = confidence.mean().item()
            valid_points_ratio = valid_mask.float().mean().item()
            
            # å¯ä»¥é€šè¿‡å…¨å±€å˜é‡æˆ–æ—¥å¿—è®°å½•è¿™äº›ç»Ÿè®¡ä¿¡æ¯
            if not hasattr(self, "_stats_buffer"):
                self._stats_buffer = []
            self._stats_buffer.append({
                "fusion_2d_ratio": fusion_2d_ratio,
                "fusion_3d_ratio": fusion_3d_ratio,
                "avg_confidence": avg_confidence,
                "valid_points_ratio": valid_points_ratio
            })
        return fused_feat, confidence
    
    def update_training_step(self, step: int):
        """æ›´æ–°è®­ç»ƒæ­¥æ•°"""
        self.training_step = step


class EnhancedFusionGate(nn.Module):
    """å¢å¼ºçš„èåˆGateæœºåˆ¶"""
    
    def __init__(self, 
                 feat_dim: int = 96,
                 use_spatial_attention: bool = True,
                 spatial_k: int = 16):
        super().__init__()
        
        self.feat_dim = feat_dim
        self.use_spatial_attention = use_spatial_attention
        self.spatial_k = spatial_k
        
        # åŸºç¡€Gate
        self.base_gate = nn.Sequential(
            nn.Linear(feat_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        # ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
        if use_spatial_attention:
            self.spatial_attn = nn.Sequential(
                nn.Conv1d(feat_dim * 2, 64, 1),
                nn.ReLU(),
                nn.Conv1d(64, 64, 1),
                nn.ReLU(),
                nn.Conv1d(64, 1, 1),
                nn.Sigmoid()
            )
        
        # å‡ ä½•ä¸€è‡´æ€§æ¨¡å—
        self.geo_encoder = nn.Sequential(
            nn.Linear(6, 32),  # xyz + normal
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        self.consistency_mlp = nn.Sequential(
            nn.Linear(feat_dim * 2 + 16, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        # Gateèåˆ
        num_gates = 2 if use_spatial_attention else 1
        self.gate_fusion = nn.Sequential(
            nn.Linear(num_gates + 1, 16),  # base + spatial + geometry
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
        # ç½®ä¿¡åº¦é¢„æµ‹
        self.confidence_mlp = nn.Sequential(
            nn.Linear(feat_dim * 2, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def _estimate_normals(self, xyz: torch.Tensor, k: int = 8) -> torch.Tensor:
        """ç®€å•çš„æ³•å‘é‡ä¼°è®¡"""
        N = xyz.shape[0]
        device = xyz.device
        
        with torch.no_grad():
            dist = torch.cdist(xyz, xyz)
            _, knn_idx = torch.topk(dist, k+1, dim=1, largest=False)
            knn_idx = knn_idx[:, 1:]  # å»æ‰è‡ªå·±
        
        neighbors = xyz[knn_idx]  # (N, k, 3)
        center = xyz.unsqueeze(1)  # (N, 1, 3)
        centered = neighbors - center  # (N, k, 3)
        cov = torch.bmm(centered.transpose(1, 2), centered)  # (N, 3, 3)
        
        try:
            eigenvalues, eigenvectors = torch.linalg.eigh(cov)
            normals = eigenvectors[:, :, 0]  # æœ€å°ç‰¹å¾å€¼å¯¹åº”çš„å‘é‡
        except:
            normals = torch.randn(N, 3, device=device)
        
        normals = F.normalize(normals, dim=1)
        return normals
    
    def forward(self, 
                f2d: torch.Tensor, 
                f3d: torch.Tensor,
                xyz: torch.Tensor,
                valid_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            f2d: (N, C) 2Dç‰¹å¾
            f3d: (N, C) 3Dç‰¹å¾
            xyz: (N, 3) 3Dåæ ‡
            valid_mask: (N,) æŠ•å½±æœ‰æ•ˆæ€§
            
        Returns:
            fused_feat: (N, C) èåˆç‰¹å¾
            confidence: (N, 1) èåˆç½®ä¿¡åº¦
        """
        N, C = f2d.shape
        
        # åŸºç¡€Gate
        base_input = torch.cat([f2d, f3d], dim=1)
        base_gate = self.base_gate(base_input)  # (N, 1)
        
        gates = [base_gate]
        
        # ç©ºé—´æ³¨æ„åŠ›Gate
        if self.use_spatial_attention:
            with torch.no_grad():
                dist = torch.cdist(xyz, xyz)
                _, knn_idx = torch.topk(dist, self.spatial_k, dim=1, largest=False)
            
            f2d_neighbors = f2d[knn_idx]  # (N, k, C)
            f3d_neighbors = f3d[knn_idx]  # (N, k, C)
            f2d_local = f2d_neighbors.mean(dim=1)  # (N, C)
            f3d_local = f3d_neighbors.mean(dim=1)  # (N, C)
            
            fusion_input = torch.cat([f2d + f2d_local, f3d + f3d_local], dim=1)  # (N, 2C)
            fusion_input = fusion_input.unsqueeze(0).transpose(1, 2)  # (1, 2C, N)
            spatial_gate = self.spatial_attn(fusion_input).transpose(1, 2).squeeze(0)  # (N, 1)
            gates.append(spatial_gate)
        
        # å‡ ä½•ä¸€è‡´æ€§Gate (ä¸å—valid_maskå½±å“ï¼Œä»…ä½œä¸ºå‡ ä½•å…ˆéªŒ)
        normals = self._estimate_normals(xyz)
        geo_feat = torch.cat([xyz, normals], dim=1)  # (N, 6)
        geo_encoded = self.geo_encoder(geo_feat)  # (N, 16)
        consistency_input = torch.cat([f2d, f3d, geo_encoded], dim=1)
        geometry_gate = self.consistency_mlp(consistency_input)  # (N, 1)
        # å‡ ä½•Gateä¸å—valid_maskç›´æ¥å½±å“ï¼Œè€Œæ˜¯ä½œä¸ºå‡ ä½•å…ˆéªŒ
        gates.append(geometry_gate)
        
        # èåˆå¤šä¸ªGate
        gate_concat = torch.cat(gates, dim=1)  # (N, num_gates)
        final_gate = self.gate_fusion(gate_concat)  # (N, 1)
        
        # åº”ç”¨æœ‰æ•ˆæ€§çº¦æŸ - è¿™é‡Œæ‰è€ƒè™‘valid_mask
        valid_weight = valid_mask.float().unsqueeze(1)
        # å¯¹äºæ— æ•ˆæŠ•å½±ç‚¹ï¼Œä½¿ç”¨è¾ƒå°çš„2Dæƒé‡ä½†ä¸å®Œå…¨æ¸…é›¶
        final_gate = final_gate * valid_weight + final_gate * 0.1 * (1 - valid_weight)
        
        # ç‰¹å¾èåˆ
        fused_feat = final_gate * f2d + (1 - final_gate) * f3d
        
        # ç½®ä¿¡åº¦ä¼°è®¡
        confidence = self.confidence_mlp(base_input)
        # ç½®ä¿¡åº¦å—valid_maskå½±å“ï¼Œæ— æ•ˆç‚¹ç½®ä¿¡åº¦è¾ƒä½
        confidence = confidence * (valid_weight * 0.9 + 0.1)  # æœ€ä½ä¿æŒ10%ç½®ä¿¡åº¦
        
        # æ”¶é›†èåˆç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self, "collect_stats") and self.collect_stats:
            fusion_2d_ratio = alpha.mean().item()
            fusion_3d_ratio = (1 - alpha).mean().item()
            avg_confidence = confidence.mean().item()
            valid_points_ratio = valid_mask.float().mean().item()
            
            # å¯ä»¥é€šè¿‡å…¨å±€å˜é‡æˆ–æ—¥å¿—è®°å½•è¿™äº›ç»Ÿè®¡ä¿¡æ¯
            if not hasattr(self, "_stats_buffer"):
                self._stats_buffer = []
            self._stats_buffer.append({
                "fusion_2d_ratio": fusion_2d_ratio,
                "fusion_3d_ratio": fusion_3d_ratio,
                "avg_confidence": avg_confidence,
                "valid_points_ratio": valid_points_ratio
            })
        return fused_feat, confidence


class FiLMModulation(nn.Module):
    """FiLMè°ƒåˆ¶æœºåˆ¶ - å‡ ä½•ä½ç½®ç¼–ç æ³¨å…¥
    
    æŒ‰ç…§ä¼˜åŒ–è„šæœ¬è¦æ±‚ï¼šLinear(64â†’128) + SiLU + Linear(128â†’512) â†’ Î³, Î² (å„256)
    FiLM: (1+Î³) âŠ™ feat + Î² åŒæ—¶ä½œç”¨äº fâ‚‚D, fâ‚ƒD
    """
    
    def __init__(self, 
                 pe_dim: int = 64,
                 hidden_dim: int = 128,
                 feat_dim: int = 256):
        super().__init__()
        
        self.feat_dim = feat_dim
        
        # PEåˆ°FiLMå‚æ•°çš„æ˜ å°„
        self.pe_to_film = nn.Sequential(
            nn.Linear(pe_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, feat_dim * 2)  # Î³ + Î²
        )
        
        # åˆå§‹åŒ–ï¼šÎ³æ¥è¿‘0ï¼ˆä¿æŒåŸç‰¹å¾ï¼‰ï¼ŒÎ²æ¥è¿‘0ï¼ˆä¸å¢åŠ åç½®ï¼‰
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡ï¼šç¡®ä¿åˆå§‹æ—¶FiLMè°ƒåˆ¶æ¥è¿‘æ’ç­‰å˜æ¢"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
        # æœ€åä¸€å±‚çš„Î³éƒ¨åˆ†åˆå§‹åŒ–ä¸ºå°å€¼ï¼ŒÎ²éƒ¨åˆ†åˆå§‹åŒ–ä¸º0
        with torch.no_grad():
            final_layer = self.pe_to_film[-1]
            # å‰åŠéƒ¨åˆ†æ˜¯Î³ï¼ŒååŠéƒ¨åˆ†æ˜¯Î²
            final_layer.weight.data[:self.feat_dim] *= 0.01  # Î³æ¥è¿‘0
            final_layer.weight.data[self.feat_dim:] *= 0.01  # Î²æ¥è¿‘0
    
    def forward(self, features: torch.Tensor, pe: torch.Tensor) -> torch.Tensor:
        """
        Args:
            features: è¾“å…¥ç‰¹å¾ (B, N, feat_dim) æˆ– (N, feat_dim)
            pe: ä½ç½®ç¼–ç  (B, N, pe_dim) æˆ– (N, pe_dim)
        Returns:
            è°ƒåˆ¶åçš„ç‰¹å¾ (ä¸è¾“å…¥å½¢çŠ¶ç›¸åŒ)
        """
        # è·å–FiLMå‚æ•°
        film_params = self.pe_to_film(pe)  # (..., feat_dim*2)
        
        # åˆ†ç¦»Î³å’ŒÎ²
        gamma, beta = torch.chunk(film_params, 2, dim=-1)  # å„è‡ª (..., feat_dim)
        
        # åº”ç”¨FiLMè°ƒåˆ¶: (1+Î³) âŠ™ feat + Î²
        modulated_features = (1 + gamma) * features + beta
        
        return modulated_features


# æ³¨å†Œåˆ° MMEngine MODELSï¼Œä¾¿äºåœ¨é…ç½®ä¸­ç›´æ¥å¼•ç”¨
@MODELS.register_module()
class TinySANeck(nn.Module):
    """Two-layer Tiny Self-Attention neck implemented by stacking TinySAModule.

    Args:
        dim (int): feature dimension.
        num_heads (int): number of attention heads for each TinySA layer.
        radius (float): ball query radius.
        max_k (int): max neighbours per center.
        sample_ratio (float): ratio of sampled center points.
        num_layers (int): number of TinySA layers to stack. Default 2 as in paper spec.
    """
    def __init__(self,
                 dim: int = 128,
                 num_heads: int = 4,
                 radius: float = 0.3,
                 max_k: int = 32,
                 sample_ratio: float = 0.25,
                 num_layers: int = 2):
        super().__init__()
        self.layers = nn.ModuleList([
            TinySAModule(dim=dim,
                          num_heads=num_heads,
                          radius=radius,
                          max_k=max_k,
                          sample_ratio=sample_ratio)
            for _ in range(num_layers)
        ])

    def forward(self, x, feats: Optional[torch.Tensor] = None, voxel_size: float = 0.02):
        """Forward æ”¯æŒä¸¤ç§è¾“å…¥ï¼š

        1. `x` ä¸º MinkowskiEngine SparseTensorï¼ˆæ¥è‡ª 3D Backboneï¼‰ã€‚
        2. `x` ä¸º (N,3) xyz åæ ‡å¼ é‡ï¼Œéœ€åŒæ—¶ä¼ å…¥ `feats` (N,C)ã€‚
        è¿”å›ä¸è¾“å…¥ç±»å‹ä¸€è‡´çš„æ•°æ®ç»“æ„ã€‚
        """
        import MinkowskiEngine as ME  # é¿å…å¾ªç¯ä¾èµ–

        # Case 1: SparseTensor
        if isinstance(x, ME.SparseTensor):
            sp_tensor = x
            xyz = sp_tensor.coordinates[:, 1:].float() * voxel_size  # å»æ‰æ‰¹ç´¢å¼•
            feats_in = sp_tensor.features
            updated_feats = self._apply_sa(xyz, feats_in)
            return ME.SparseTensor(
                updated_feats,
                coordinate_map_key=sp_tensor.coordinate_map_key,
                coordinate_manager=sp_tensor.coordinate_manager)

        # Case 2: xyz + feats
        if feats is None:
            raise ValueError('When first argument is xyz Tensor, feats must not be None.')
        return self._apply_sa(x, feats)

    # === æ–°å¢å†…éƒ¨å‡½æ•°ï¼šç»Ÿä¸€æ‰§è¡Œ TinySA å †å  ===
    def _apply_sa(self, xyz: torch.Tensor, feats: torch.Tensor):
        """Apply stacked TinySA layers.

        Args:
            xyz (Tensor): (N,3) coordinates.
            feats (Tensor): (N,C) features.
        Returns:
            Tensor: (N,C) updated features.
        """
        for sa in self.layers:
            feats = sa(xyz, feats)
        return feats


@MODELS.register_module()
class BiFusionEncoder(nn.Module):
    """Enhanced Bi-Fusion Encoder combining 2D CLIP visual features and 3D Sparse features."""

    def __init__(self,
                 clip_pretrained: str = 'openai',
                 voxel_size: float = 0.02,
                 freeze_blocks: int = 0,  # æ§åˆ¶CLIPå†»ç»“å±‚æ•°
                 use_amp: bool = True,
                 use_tiny_sa_2d: bool = False,
                 # Enhanced CLIPé…ç½®
                 clip_num_layers: int = 6,
                 freeze_clip_conv1: bool = False,
                 freeze_clip_early_layers: bool = True,
                 # Enhanced Gateé…ç½®
                 use_enhanced_gate: bool = True,
                 use_spatial_attention: bool = True,
                 spatial_k: int = 16,
                 # TinySAæ§åˆ¶
                 use_tiny_sa_3d: bool = False):  # æ–°å¢å‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨TinySA
        super().__init__()
        
        # Enhanced CLIPç¼–ç å™¨
        self.enhanced_clip = EnhancedCLIPEncoder(
            clip_pretrained=clip_pretrained,
            num_layers=clip_num_layers,
            freeze_conv1=freeze_clip_conv1,
            freeze_early_layers=freeze_clip_early_layers
        )
        
        # 2Dç‰¹å¾å¤„ç†
        self.lin2d = nn.Sequential(nn.Linear(256, 256), nn.ReLU())
        self.ln2d = nn.LayerNorm(256)
        
        # 3D encoder - ä¿æŒåŸå§‹96ç»´ä»¥å…¼å®¹é¢„è®­ç»ƒæƒé‡ï¼Œç„¶åä½¿ç”¨æŠ•å½±å¤´åˆ°256ç»´
        cfg_backbone = SimpleNamespace(dilations=[1, 1, 1, 1], bn_momentum=0.02, conv1_kernel_size=5)
        self.backbone3d = Res16UNet34C(in_channels=3, out_channels=96, config=cfg_backbone, D=3)
        
        # 3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´ (æ›¿ä»£ç®€å•çš„é€‚é…å±‚)
        self.proj_3d = EnhancedProjectionHead3D(
            input_dim=96,
            output_dim=256,
            use_dropout=True,
            dropout_rate=0.1
        )
        
        # æ¡ä»¶æ€§åœ°ä½¿ç”¨TinySAæˆ–ç®€å•çš„çº¿æ€§å±‚ - åœ¨æŠ•å½±åçš„256ç»´ä¸Šæ“ä½œ
        self.use_tiny_sa_3d = use_tiny_sa_3d
        
        if use_tiny_sa_3d:
            # ä½¿ç”¨TinySAï¼ˆå¦‚æœæ˜ç¡®å¯ç”¨ï¼‰
            self.tiny_sa_neck = TinySANeck(dim=256, num_heads=8, radius=0.3, max_k=32, sample_ratio=0.25, num_layers=2)
        else:
            # ä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚æ›¿ä»£TinySA
            self.simple_neck = nn.Sequential(
                nn.Linear(256, 256),
                nn.ReLU(),
                nn.LayerNorm(256),
                nn.Linear(256, 256),
                nn.ReLU(),
                nn.LayerNorm(256)
            )
            
        # 3Dç‰¹å¾æœ€ç»ˆå¤„ç†å±‚ï¼ˆåœ¨256ç»´ä¸Šæ“ä½œï¼‰
        self.lin3d = nn.Sequential(nn.Linear(256, 256), nn.ReLU())
        self.ln3d = nn.LayerNorm(256)

        # PE mapping with FiLM modulation
        self.pe_mlp = nn.Sequential(nn.Linear(64, 64), nn.ReLU())
        self.film_modulation = FiLMModulation(pe_dim=64, hidden_dim=128, feat_dim=256)
        
        # ç‰¹å¾å¯¹é½ - è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…256ç»´è¾“å‡ºï¼ˆä¸å†éœ€è¦PEæ‹¼æ¥ï¼Œå› ä¸ºä½¿ç”¨FiLMè°ƒåˆ¶ï¼‰
        self.lin2d_final = nn.Linear(256, 256)  # 256 -> 256 (ç§»é™¤PEæ‹¼æ¥)
        self.lin3d_final = nn.Linear(256, 256)  # 256 -> 256 (ç§»é™¤PEæ‹¼æ¥)

        # èåˆæœºåˆ¶é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨LiteFusionGate
        self.use_enhanced_gate = use_enhanced_gate
        self.use_lite_gate = True  # é»˜è®¤ä½¿ç”¨è½»é‡çº§é—¨æ§
        
        if self.use_lite_gate:
            # ä½¿ç”¨è½»é‡çº§LiteFusionGate
            self.fusion_gate = LiteFusionGate(
                feat_dim=256,
                early_steps=3000
            )
        elif use_enhanced_gate:
            self.fusion_gate = EnhancedFusionGate(
                feat_dim=256,
                use_spatial_attention=use_spatial_attention,
                spatial_k=spatial_k
            )
        else:
            # å›é€€åˆ°ç®€å•Gate - è°ƒæ•´è¾“å…¥ç»´åº¦ä»¥åŒ¹é…256ç»´ç‰¹å¾
            self.gate_mlp = nn.Sequential(
                nn.Linear(512, 128),  # 256*2 -> 128
                nn.ReLU(),
                nn.Linear(128, 1),
                nn.Sigmoid()
            )
        
        self.voxel_size = voxel_size
        self.use_amp = use_amp
        
        # ğŸ” ç»Ÿè®¡ä¿¡æ¯æ”¶é›†é…ç½®
        self._collect_fusion_stats = True   # å¯ç”¨èåˆç»Ÿè®¡æ”¶é›†
        self._collect_gradient_stats = True  # å¯ç”¨æ¢¯åº¦ç»Ÿè®¡æ”¶é›†
        self._fusion_stats = {}  # å­˜å‚¨èåˆç»Ÿè®¡ä¿¡æ¯
        self._stats_history = []  # å†å²ç»Ÿè®¡ä¿¡æ¯

    def update_training_step(self, step: int):
        """æ›´æ–°è®­ç»ƒæ­¥æ•°ï¼Œç”¨äºLiteFusionGateçš„æ—©æœŸå†»ç»“ç­–ç•¥"""
        if self.use_lite_gate and hasattr(self.fusion_gate, 'update_training_step'):
            self.fusion_gate.update_training_step(step)
    
    def _collect_fusion_statistics(self, conf: torch.Tensor, valid: torch.Tensor, 
                                 f2d: torch.Tensor, f3d: torch.Tensor):
        """æ”¶é›†èåˆé—¨æ§ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with torch.no_grad():
                # åŸºç¡€ç»Ÿè®¡
                if conf.dim() == 2:  # (N, 1)
                    conf_values = conf.squeeze(-1)  # (N,)
                else:
                    conf_values = conf
                
                # è®¡ç®—èåˆæ¯”ä¾‹
                fusion_2d_ratio = conf_values.mean().item()
                fusion_3d_ratio = 1.0 - fusion_2d_ratio
                avg_confidence = conf_values.mean().item()
                valid_points_ratio = valid.float().mean().item()
                
                # ç‰¹å¾è´¨é‡ç»Ÿè®¡
                f2d_norm = torch.norm(f2d, dim=-1).mean().item()
                f3d_norm = torch.norm(f3d, dim=-1).mean().item()
                
                # ç‰¹å¾ç›¸ä¼¼åº¦
                cos_sim = F.cosine_similarity(f2d, f3d, dim=-1).mean().item()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._fusion_stats = {
                    'fusion_2d_ratio': fusion_2d_ratio,
                    'fusion_3d_ratio': fusion_3d_ratio, 
                    'avg_confidence': avg_confidence,
                    'valid_points_ratio': valid_points_ratio,
                    'f2d_norm_avg': f2d_norm,
                    'f3d_norm_avg': f3d_norm,
                    'feature_similarity': cos_sim,
                    'total_points': conf_values.numel()
                }
                
                # ä¿ç•™å†å²è®°å½•ï¼ˆæœ€å¤š100æ¡ï¼‰
                self._stats_history.append(self._fusion_stats.copy())
                if len(self._stats_history) > 100:
                    self._stats_history.pop(0)
                    
        except Exception as e:
            print(f"Warning: Failed to collect fusion stats: {e}")
    
    def get_fusion_statistics(self):
        """è·å–èåˆç»Ÿè®¡ä¿¡æ¯"""
        return self._fusion_stats.copy() if self._fusion_stats else {}
    
    def get_statistics_summary(self, last_n: int = 10):
        """è·å–æœ€è¿‘Næ¬¡çš„ç»Ÿè®¡æ‘˜è¦"""
        if not self._stats_history:
            return {}
            
        recent_stats = self._stats_history[-last_n:]
        summary = {}
        
        for key in recent_stats[0].keys():
            if key != 'total_points':
                values = [stats[key] for stats in recent_stats if key in stats]
                if values:
                    summary[f'{key}_mean'] = sum(values) / len(values)
                    summary[f'{key}_std'] = (sum((x - summary[f'{key}_mean'])**2 for x in values) / len(values))**0.5
        
        return summary

    def build_uv_index(self, xyz_cam, intr, img_shape):
        return _build_uv_index(xyz_cam, intr, img_shape)

    def sample_img_feat(self, feat_map, uv):
        return _sample_img_feat(feat_map, uv)
    
    def _improved_projection(self, xyz_cam, intr, img_shape):
        """æ”¹è¿›çš„æŠ•å½±æœºåˆ¶ï¼šå¢åŠ è§†è·è£å‰ªå’Œä¼˜å…ˆçº§è¿‡æ»¤"""
        # 1. åŸºç¡€æŠ•å½±
        fx, fy, cx, cy = intr
        x, y, z = xyz_cam[:, 0], xyz_cam[:, 1], xyz_cam[:, 2]
        
        # 2. è§†è·è£å‰ªï¼šScanNetå®¤å†…åœºæ™¯åˆç†æ·±åº¦èŒƒå›´ (0.05m åˆ° 20m)
        depth_valid = (z > 0.05) & (z < 20.0)
        
        # 3. è®¡ç®—æŠ•å½±åæ ‡  
        u = fx * x / (z + 1e-8) + cx  # æ·»åŠ å°å€¼é¿å…é™¤é›¶
        v = fy * y / (z + 1e-8) + cy
        
        # 4. è¾¹ç•Œæ£€æŸ¥
        H, W = img_shape
        boundary_valid = (u >= 0) & (u < W) & (v >= 0) & (v < H)
        
        # 5. ç»„åˆæ‰€æœ‰æ¡ä»¶
        valid = depth_valid & boundary_valid
        
        # 6. å¦‚æœæœ‰æ•ˆç‚¹å¤ªå°‘ï¼Œè¿›ä¸€æ­¥æ”¾å®½æ·±åº¦é™åˆ¶  
        if valid.sum() < 500:  # é™ä½é˜ˆå€¼åˆ°500ä¸ªç‚¹
            depth_valid_relaxed = (z > 0.01) & (z < 50.0)  # è¿›ä¸€æ­¥æ”¾å®½åˆ°0.01m-50m
            valid = depth_valid_relaxed & boundary_valid
            if self._collect_fusion_stats:
                print(f"ğŸ” æ·±åº¦æ”¾å®½ - åˆå§‹æœ‰æ•ˆç‚¹: {depth_valid.sum()}, æ”¾å®½å: {valid.sum()}")
            
        # 7. ä¼˜å…ˆä¿ç•™è·ç¦»ç›¸æœºä¸­å¿ƒè¾ƒè¿‘çš„ç‚¹
        if valid.sum() > 5000:  # å¦‚æœæœ‰æ•ˆç‚¹è¿‡å¤šï¼Œè¿›è¡Œç­›é€‰
            center_u, center_v = W/2, H/2
            dist_to_center = torch.sqrt((u - center_u)**2 + (v - center_v)**2)
            # æŒ‰è·ç¦»ä¸­å¿ƒçš„è·ç¦»æ’åºï¼Œä¿ç•™å‰5000ä¸ª
            valid_indices = torch.where(valid)[0]
            center_distances = dist_to_center[valid_indices]
            _, sorted_indices = torch.sort(center_distances)
            keep_indices = valid_indices[sorted_indices[:5000]]
            
            # é‡æ–°è®¾ç½®valid mask
            new_valid = torch.zeros_like(valid)
            new_valid[keep_indices] = True
            valid = new_valid
        
        return valid, torch.stack([u, v], dim=-1)

    def _process_single(self, points: torch.Tensor, img: torch.Tensor, cam_meta: Dict,
                        feat2d_map: Optional[torch.Tensor] = None,
                        clip_global: Optional[torch.Tensor] = None):
        """å¤„ç†å•å¸§æ•°æ®ï¼Œä½¿ç”¨å¢å¼ºçš„CLIPå’Œèåˆæœºåˆ¶"""
        # æå–åŸºç¡€ä¿¡æ¯
        xyz_cam = points[:, :3]
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¯¹äºcoord_type='DEPTH'çš„æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨ç›¸æœºåæ ‡
        # ScanNet DEPTHåæ ‡å·²ç»åœ¨ç›¸æœºåæ ‡ç³»ä¸­ï¼Œæ— éœ€å˜æ¢
        xyz_cam_proj = xyz_cam  # ç›´æ¥ç”¨äºæŠ•å½±çš„åæ ‡
        
        # è®¡ç®—ä¸–ç•Œåæ ‡ï¼ˆä»…ç”¨äºå‡ ä½•ç‰¹å¾PEï¼Œä¸ç”¨äºæŠ•å½±ï¼‰
        xyz_world = xyz_cam  # é»˜è®¤å€¼ï¼šä¸–ç•Œåæ ‡=ç›¸æœºåæ ‡
        
        if cam_meta.get('extrinsics', None) is not None:
            try:
                extr = cam_meta['extrinsics']
                if not torch.is_tensor(extr):
                    extr = torch.as_tensor(extr, dtype=xyz_cam.dtype, device=xyz_cam.device)
                
                # å¤„ç†ç»´åº¦å’Œæ ¼å¼
                if isinstance(extr, (list, tuple)):
                    extr = torch.tensor(extr, dtype=xyz_cam.dtype, device=xyz_cam.device)
                
                if extr.dim() == 3:
                    extr = extr[0]
                elif extr.dim() == 1:
                    if extr.numel() == 12:
                        extr = extr.view(3, 4)
                    elif extr.numel() == 16:
                        extr = extr.view(4, 4)
                
                if extr.shape == (3, 4):
                    extr = torch.cat([extr, extr.new_tensor([[0, 0, 0, 1]])], dim=0)
                
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if torch.any(torch.isnan(extr)) or torch.any(torch.isinf(extr)):
                    print(f"Warning: Invalid extrinsics detected, using camera coordinates")
                    xyz_world = xyz_cam
                else:
                    # ä»…ä¸ºå‡ ä½•ç‰¹å¾è®¡ç®—ä¸–ç•Œåæ ‡ï¼ˆcam2worldå˜æ¢ï¼‰
                    T_cam2world = extr
                    xyz_h_cam = torch.cat([xyz_cam, xyz_cam.new_ones(xyz_cam.size(0), 1)], dim=-1)
                    xyz_world_h = torch.matmul(xyz_h_cam, T_cam2world.T)
                    xyz_world = xyz_world_h[:, :3]
                    
                    # éªŒè¯å˜æ¢ç»“æœ
                    if torch.any(torch.isnan(xyz_world)) or torch.any(torch.isinf(xyz_world)):
                        print(f"Warning: Invalid world coordinates, using camera coordinates")
                        xyz_world = xyz_cam
            except Exception as e:
                print(f"Warning: Extrinsics processing failed: {e}, using camera coordinates")
                xyz_world = xyz_cam

        # å‡ ä½•PE
        bbox_size = torch.zeros_like(xyz_world)
        pose_delta = torch.zeros(9, device=xyz_world.device, dtype=xyz_world.dtype)
        height = xyz_world[:, 2:3]
        pe = self.pe_mlp(build_geo_pe(xyz_world, bbox_size, pose_delta, height))

        # 3Dåˆ†æ”¯
        coords_int = torch.round(xyz_world / self.voxel_size).to(torch.int32)
        coords = torch.cat([torch.zeros(coords_int.size(0), 1, dtype=torch.int32, device=coords_int.device),
                             coords_int], dim=1)
        feats = points[:, 3:6].contiguous()
        field = ME.TensorField(coordinates=coords, features=feats)
        sparse_tensor = field.sparse()
        feat3d_sparse = self.backbone3d(sparse_tensor)
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨sliceæ“ä½œå°†ç¨€ç–ç‰¹å¾æ˜ å°„å›åŸå§‹ç‚¹äº‘
        feat3d = feat3d_sparse.slice(field).features
        
        # éªŒè¯ç‰¹å¾æ•°é‡åŒ¹é…ï¼ˆç°åœ¨åº”è¯¥åŒ¹é…äº†ï¼‰
        if feat3d.shape[0] != points.shape[0]:
            raise RuntimeError(f"3D features shape mismatch: got {feat3d.shape[0]}, expected {points.shape[0]}")
        
        # åº”ç”¨3DæŠ•å½±å¤´ï¼š96ç»´ -> 256ç»´ï¼Œå¹¶L2å½’ä¸€åŒ–
        feat3d = self.proj_3d(feat3d)  # (N, 96) -> (N, 256)
        feat3d = F.normalize(feat3d + 1e-8, dim=-1, eps=1e-8)  # L2å½’ä¸€åŒ–åˆ°å•ä½çƒé¢
        
        # å¯é€‰çš„TinySAæˆ–ç®€å•neckå¤„ç†
        if self.use_tiny_sa_3d:
            feat3d = self.tiny_sa_neck(xyz_world, feat3d)
        else:
            feat3d = self.simple_neck(feat3d)
            
        feat3d = self.lin3d(feat3d)
        feat3d = self.ln3d(feat3d)

        # 2Dåˆ†æ”¯ - ä½¿ç”¨Enhanced CLIP
        if feat2d_map is None or clip_global is None:
            with torch.no_grad():
                amp_ctx = torch.cuda.amp.autocast(enabled=self.use_amp and img.is_cuda)
                with amp_ctx:
                    feat2d_map, clip_global = self.enhanced_clip(img.unsqueeze(0))
                    if feat2d_map is not None:  # æ·»åŠ Noneæ£€æŸ¥
                        feat2d_map = feat2d_map.squeeze(0)  # Remove batch dim

        # æŠ•å½±é‡‡æ · - å¤„ç†å†…å‚æ ¼å¼ï¼ˆå¢å¼ºå®¹é”™ï¼‰
        intr = cam_meta['intrinsics']
        if not torch.is_tensor(intr):
            intr = torch.as_tensor(intr, dtype=xyz_cam.dtype, device=xyz_cam.device)
        
        # ç¡®ä¿intrinsicsæ˜¯1D tensor (4,) - å¢å¼ºå¤„ç†é€»è¾‘
        if intr.dim() == 2:  # (1, 4) æˆ– (B, 4)
            if intr.shape[-1] == 4:
                intr = intr[0]  # å–ç¬¬ä¸€ä¸ª
            elif intr.shape[0] == 4:
                intr = intr[:, 0] if intr.shape[1] == 1 else intr.flatten()
        elif intr.dim() == 0:  # æ ‡é‡
            # ä½¿ç”¨é»˜è®¤ScanNetå†…å‚
            intr = torch.tensor([577.870605, 577.870605, 319.5, 239.5], 
                              dtype=xyz_cam.dtype, device=xyz_cam.device)
        elif intr.dim() > 2:  # å¤šç»´tensorï¼Œå°è¯•å±•å¹³
            intr = intr.flatten()
        
        # ç¡®ä¿æ˜¯4ä¸ªå…ƒç´ ï¼Œå¦‚æœä¸æ˜¯åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if intr.numel() != 4:
            # è®°å½•å¼‚å¸¸intrinsicsç”¨äºè°ƒè¯•
            if intr.numel() == 1:
                # å¯èƒ½æ˜¯é”™è¯¯çš„å•å€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼
                intr = torch.tensor([577.870605, 577.870605, 319.5, 239.5], 
                                  dtype=xyz_cam.dtype, device=xyz_cam.device)
            elif intr.numel() > 4:
                # å–å‰4ä¸ªå…ƒç´ 
                intr = intr[:4]
            else:
                # å…¶ä»–æƒ…å†µï¼ŒæŠ›å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
                raise ValueError(f"intrinsicså¼‚å¸¸: æœŸæœ›4ä¸ªå…ƒç´ [fx,fy,cx,cy], å®é™…å¾—åˆ°{intr.numel()}ä¸ªå…ƒç´ ï¼Œ"
                               f"å€¼ä¸º{intr.tolist() if intr.numel() <= 10 else 'å¤ªå¤šå…ƒç´ '}, "
                               f"åŸå§‹å½¢çŠ¶: {intr.shape}, cam_meta: {cam_meta}")
        
        # æœ€ç»ˆéªŒè¯
        assert intr.numel() == 4, f"å†…å‚å¤„ç†åä»ç„¶å¼‚å¸¸: {intr.shape}"
        
        # ğŸ” è°ƒè¯•cam_metaå†…å®¹ï¼Œå¯»æ‰¾çœŸå®å›¾åƒå°ºå¯¸
        if self._collect_fusion_stats:
            print(f"ğŸ” è°ƒè¯• cam_meta å†…å®¹: {list(cam_meta.keys())}")
            if 'img_shape' in cam_meta:
                print(f"ğŸ” å‘ç° img_shape: {cam_meta['img_shape']}")
            else:
                print(f"ğŸ” cam_meta å®Œæ•´å†…å®¹: {cam_meta}")
        
        # ç¡®ä¿feat2d_mapä¸ä¸ºNone
        if feat2d_map is None:
            # å¦‚æœfeat2d_mapä»ç„¶ä¸ºNoneï¼Œåˆ›å»ºé»˜è®¤çš„ç‰¹å¾å›¾ï¼ˆå·²ç»ä¸Šé‡‡æ ·åˆ°40Ã—30ï¼‰
            feat2d_map = torch.zeros((256, 30, 40), dtype=xyz_cam.dtype, device=xyz_cam.device)
        
        # ğŸ’¡ ä¿®å¤æŠ•å½±ç¼©æ”¾é—®é¢˜ï¼šCLIPç‰¹å¾å›¾æ˜¯14x14ï¼Œéœ€è¦ç¼©æ”¾å†…å‚
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸Šé‡‡æ ·CLIPç‰¹å¾å›¾åˆ°åˆç†åˆ†è¾¨ç‡
        # å½“å‰14Ã—14 â†’ ç›®æ ‡40Ã—30 (stride=16å¯¹åº”åˆ†è¾¨ç‡)
        original_h, original_w = feat2d_map.shape[-2:]
        
        # è®¡ç®—ç›®æ ‡åˆ†è¾¨ç‡ (stride=16)
        target_w = 640 // 16  # 40
        target_h = 480 // 16  # 30
        
        if original_h != target_h or original_w != target_w:
            # ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·
            feat2d_map = F.interpolate(
                feat2d_map.unsqueeze(0),  # æ·»åŠ batchç»´åº¦
                size=(target_h, target_w),
                mode='bilinear',
                align_corners=True
            ).squeeze(0)  # ç§»é™¤batchç»´åº¦
            
            if self._collect_fusion_stats:
                print(f"ğŸ”§ ç‰¹å¾å›¾ä¸Šé‡‡æ ·: {original_h}Ã—{original_w} â†’ {target_h}Ã—{target_w}")
        
        # ä½¿ç”¨ä¸Šé‡‡æ ·åçš„ç‰¹å¾å›¾å°ºå¯¸
        feat_h, feat_w = feat2d_map.shape[-2:]
        
        # ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„ScanNetå›¾åƒå°ºå¯¸ï¼š640x480
        # å†…å‚ cx=319.5, cy=239.5 å¯¹åº” 640x480 å›¾åƒ
        original_w = 640
        original_h = 480
        
        # ğŸ”§ ä¿®å¤å†…å‚ç¼©æ”¾ï¼šä½¿ç”¨æ­£ç¡®çš„ScanNetå›¾åƒå°ºå¯¸
        # ScanNetæ ‡å‡†ï¼š640Ã—480ï¼ŒCLIPç‰¹å¾ï¼š40Ã—30 (ä¸Šé‡‡æ ·å)
        scale_x = feat_w / original_w  # 40 / 640 = 0.0625
        scale_y = feat_h / original_h  # 30 / 480 = 0.0625
        
        # ç¼©æ”¾å†…å‚ä»¥åŒ¹é…ç‰¹å¾å›¾å°ºå¯¸
        scaled_intr = intr.clone()
        scaled_intr[0] *= scale_x  # fx: 577.87 â†’ 12.59
        scaled_intr[1] *= scale_y  # fy: 577.87 â†’ 16.85  
        scaled_intr[2] *= scale_x  # cx: 319.5 â†’ 6.99
        scaled_intr[3] *= scale_y  # cy: 239.5 â†’ 6.99
        
        if self._collect_fusion_stats:
            print(f"ğŸ” å†…å‚ç¼©æ”¾ - åŸå§‹: fx={intr[0]:.1f}, fy={intr[1]:.1f}, cx={intr[2]:.1f}, cy={intr[3]:.1f}")
            print(f"ğŸ” å†…å‚ç¼©æ”¾ - ç¼©æ”¾: fx={scaled_intr[0]:.2f}, fy={scaled_intr[1]:.2f}, cx={scaled_intr[2]:.2f}, cy={scaled_intr[3]:.2f}")
            print(f"ğŸ” å†…å‚ç¼©æ”¾ - æ¯”ä¾‹: scale_x={scale_x:.6f}, scale_y={scale_y:.6f}")
        
        print(f"ğŸ” æŠ•å½±è°ƒè¯• - ç¼©æ”¾å†…å‚: {scaled_intr.tolist()}")
        print(f"ğŸ” æŠ•å½±è°ƒè¯• - ç¼©æ”¾æ¯”ä¾‹: x={scale_x:.4f}, y={scale_y:.4f}")
        print(f"ğŸ” æŠ•å½±è°ƒè¯• - åŸå§‹å›¾åƒå°ºå¯¸: {original_w}x{original_h} â†’ ç‰¹å¾å›¾å°ºå¯¸: {feat_w}x{feat_h}")
        
        # ğŸ”§ æ”¹è¿›çš„æŠ•å½±æœºåˆ¶ï¼šå¢åŠ è§†è·è£å‰ªå’Œç©ºé—´ä¼˜å…ˆçº§
        valid, uv = self._improved_projection(xyz_cam_proj, scaled_intr, (feat_h, feat_w))
        
        # ğŸ” è°ƒè¯•æŠ•å½±é—®é¢˜
        if self._collect_fusion_stats:
            with torch.no_grad():
                z_values = xyz_cam_proj[:, 2]
                print(f"ğŸ” æŠ•å½±è°ƒè¯• - æ·±åº¦ç»Ÿè®¡: min={z_values.min().item():.3f}, max={z_values.max().item():.3f}, "
                      f"mean={z_values.mean().item():.3f}, æ­£æ·±åº¦æ¯”ä¾‹={((z_values > 0).float().mean()*100):.1f}%")
                print(f"ğŸ” æŠ•å½±è°ƒè¯• - åŸå§‹å†…å‚: {intr.tolist()}")
                print(f"ğŸ” æŠ•å½±è°ƒè¯• - ç¼©æ”¾å†…å‚: {scaled_intr.tolist()}")
                print(f"ğŸ” æŠ•å½±è°ƒè¯• - ç¼©æ”¾æ¯”ä¾‹: x={scale_x:.4f}, y={scale_y:.4f}")
                print(f"ğŸ” æŠ•å½±è°ƒè¯• - åŸå§‹å›¾åƒå°ºå¯¸: {original_w}x{original_h} â†’ ç‰¹å¾å›¾å°ºå¯¸: {feat_w}x{feat_h}")
                if valid.any():
                    uv_valid = uv[valid]
                    print(f"âœ… æ”¹è¿›æŠ•å½± - æœ‰æ•ˆæŠ•å½±: {valid.sum().item()}/{valid.numel()}, "
                          f"uvèŒƒå›´: u[{uv_valid[:, 0].min().item():.1f}, {uv_valid[:, 0].max().item():.1f}], "
                          f"v[{uv_valid[:, 1].min().item():.1f}, {uv_valid[:, 1].max().item():.1f}]")
                else:
                    print(f"ğŸš¨ æ”¹è¿›æŠ•å½± - ä»ç„¶æ— æœ‰æ•ˆæŠ•å½±ç‚¹ï¼")
        
        sampled2d = xyz_cam.new_zeros((xyz_cam.shape[0], 256))  # å·²ç»æ˜¯256ç»´ï¼Œæ¥è‡ªenhanced_clipçš„è¾“å‡º
        if valid.any() and feat2d_map is not None:
            # ç¡®ä¿uvå’Œfeat2d_mapçš„æ•°æ®ç±»å‹åŒ¹é…
            if uv.dtype != feat2d_map.dtype:
                uv = uv.to(feat2d_map.dtype)
            f2d_vis = self.sample_img_feat(feat2d_map.unsqueeze(0), uv[valid])
            sampled2d[valid] = f2d_vis.to(sampled2d.dtype)  # ç¡®ä¿è¾“å‡ºç±»å‹ä¸€è‡´
        
        # 2Dç‰¹å¾åå¤„ç†ï¼ˆå·²ç»æ˜¯256ç»´ï¼Œæ— éœ€é¢å¤–æŠ•å½±ï¼‰
        feat2d = self.lin2d(sampled2d)  # 256 -> 256
        feat2d = self.ln2d(feat2d)
        # åº”ç”¨L2å½’ä¸€åŒ–ç¡®ä¿ç‰¹å¾åœ¨å•ä½çƒé¢
        feat2d = F.normalize(feat2d + 1e-8, dim=-1, eps=1e-8)

        # åº”ç”¨FiLMè°ƒåˆ¶è€Œéç‰¹å¾æ‹¼æ¥
        feat2d_modulated = self.film_modulation(feat2d, pe)
        feat3d_modulated = self.film_modulation(feat3d, pe)
        
        # ç‰¹å¾æœ€ç»ˆæŠ•å½±ï¼ˆä¸å†éœ€è¦PEæ‹¼æ¥ï¼‰
        f2d_final = self.lin2d_final(feat2d_modulated)
        f3d_final = self.lin3d_final(feat3d_modulated)
        
        # ç¡®ä¿æœ€ç»ˆç‰¹å¾å½’ä¸€åŒ–åˆ°å•ä½çƒé¢ï¼ˆåœ¨èåˆå‰ï¼‰
        f2d_final = F.normalize(f2d_final + 1e-8, dim=-1, eps=1e-8)
        f3d_final = F.normalize(f3d_final + 1e-8, dim=-1, eps=1e-8)

        # ä½¿ç”¨LiteFusionGateæˆ–Enhanced Gateè¿›è¡Œèåˆ
        if self.use_lite_gate:
            # æ·»åŠ æ‰¹é‡ç»´åº¦ä»¥é€‚é…LiteFusionGate
            f2d_batch = f2d_final.unsqueeze(0)  # (1, N, 256)
            f3d_batch = f3d_final.unsqueeze(0)  # (1, N, 256)
            valid_batch = valid.unsqueeze(0)    # (1, N)
            
            fused_batch, conf_batch = self.fusion_gate(f2d_batch, f3d_batch, valid_batch)
            fused = fused_batch.squeeze(0)      # (N, 256)
            conf = conf_batch.squeeze(0)        # (N, 1)
            
            # ğŸ” æ”¶é›†èåˆç»Ÿè®¡ä¿¡æ¯
            if self._collect_fusion_stats:
                self._collect_fusion_statistics(conf, valid, f2d_final, f3d_final)
            
        elif self.use_enhanced_gate:
            fused, conf = self.fusion_gate(f2d_final, f3d_final, xyz_world, valid)
            
            # ğŸ” æ”¶é›†èåˆç»Ÿè®¡ä¿¡æ¯
            if self._collect_fusion_stats:
                self._collect_fusion_statistics(conf, valid, f2d_final, f3d_final)
        else:
            # å›é€€åˆ°ç®€å•çš„gateæœºåˆ¶
            gate_input = torch.cat([f2d_final, f3d_final], dim=-1)
            gate = self.gate_mlp(gate_input)
            valid_weight = valid.float().unsqueeze(-1)
            gate = gate * valid_weight + 0.2 * (1 - valid_weight)
            fused = gate * f2d_final + (1 - gate) * f3d_final
            conf = gate
            
            # ğŸ” æ”¶é›†èåˆç»Ÿè®¡ä¿¡æ¯
            if self._collect_fusion_stats:
                self._collect_fusion_statistics(conf, valid, f2d_final, f3d_final)

        return fused, conf, pe, clip_global, valid

    def forward(self, points_list, imgs, cam_info):
        """æ”¯æŒ List æˆ– batched Tensor è¾“å…¥ï¼Œç»Ÿä¸€è¿”å› List ç»“æœã€‚"""
        # å…¼å®¹æ€§å¤„ç†
        if torch.is_tensor(points_list):
            points_list = list(points_list)
        if torch.is_tensor(imgs):
            imgs = list(imgs)
        
        # æ£€æŸ¥è¾“å…¥é•¿åº¦
        n_points = len(points_list)
        n_imgs = len(imgs)
        
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥ï¼ˆæ•°æ®é¢„å¤„ç†å™¨åº”è¯¥å·²ç»å¤„ç†äº†tupleå±•å¼€ï¼‰
        if n_points != n_imgs:
            raise RuntimeError(f"Length mismatch after preprocessing: points_list ({n_points}) != imgs ({n_imgs})")
        
        # å¤„ç†cam_infoæ ¼å¼
        if isinstance(cam_info, dict):
            # å•ä¸ªå­—å…¸ï¼Œå¤åˆ¶ç»™æ‰€æœ‰æ ·æœ¬
            cam_info = [cam_info for _ in range(n_points)]
        elif isinstance(cam_info, list):
            # å·²ç»æ˜¯åˆ—è¡¨ï¼Œç¡®ä¿é•¿åº¦åŒ¹é…
            if len(cam_info) != n_points:
                # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå…ƒç´ å¡«å……
                first_info = cam_info[0] if cam_info else {}
                cam_info = [first_info for _ in range(n_points)]
        else:
            # å…¶ä»–æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼
            default_info = {'intrinsics': [577.870605, 577.870605, 319.5, 239.5], 'extrinsics': None}
            cam_info = [default_info for _ in range(n_points)]

        # æœ€ç»ˆé•¿åº¦éªŒè¯
        assert len(points_list) == len(imgs) == len(cam_info), \
            f"Final length check failed: points={len(points_list)}, imgs={len(imgs)}, cam_info={len(cam_info)}"

        # æ‰¹é‡CLIPå¤„ç†ï¼ˆå¦‚æœå›¾åƒå°ºå¯¸ä¸€è‡´ï¼‰
        feat2d_maps, clip_globals = None, None
        try:
            if all(img.shape == imgs[0].shape for img in imgs):
                imgs_batch = torch.stack(imgs, dim=0)
                with torch.no_grad():
                    amp_ctx = torch.cuda.amp.autocast(enabled=self.use_amp and imgs_batch.is_cuda)
                    with amp_ctx:
                        feat2d_maps, clip_globals = self.enhanced_clip(imgs_batch)
        except Exception as e:
            # æ‰¹é‡å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å•ç‹¬å¤„ç†
            feat2d_maps = clip_globals = None

        # é€æ ·æœ¬å¤„ç†
        feat_fusion_list, conf_list, pe_list, clip_global_list, valid_mask_list = [], [], [], [], []
        for idx, (pts, img, meta) in enumerate(zip(points_list, imgs, cam_info)):
            try:
                # ç¡®ä¿metaæ˜¯å­—å…¸æ ¼å¼
                if not isinstance(meta, dict):
                    meta = {'intrinsics': [577.870605, 577.870605, 319.5, 239.5], 'extrinsics': None}
                
                # ç¡®ä¿intrinsicså­˜åœ¨
                if 'intrinsics' not in meta:
                    meta['intrinsics'] = [577.870605, 577.870605, 319.5, 239.5]  # ScanNeté»˜è®¤å†…å‚
                
                fmap = feat2d_maps[idx:idx+1] if feat2d_maps is not None else None
                cglb = clip_globals[idx] if clip_globals is not None else None
                fused, conf, pe, clip_global, valid_mask = self._process_single(pts, img, meta, 
                                                                  fmap.squeeze(0) if fmap is not None else None, 
                                                                  cglb)
                feat_fusion_list.append(fused)
                conf_list.append(conf)
                pe_list.append(pe)
                clip_global_list.append(clip_global)
                valid_mask_list.append(valid_mask)
            
            except Exception as e:
                raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¸è¦è·³è¿‡æ ·æœ¬
        
        return {
            'feat_fusion': feat_fusion_list,
            'conf_2d': conf_list,
            'pe_xyz': pe_list,
            'clip_global': clip_global_list,
            'valid_projection_mask': valid_mask_list  # æ–°å¢æœ‰æ•ˆæŠ•å½±æ©ç 
        } 