import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import contextlib
from typing import List, Dict, Optional, Tuple, Union, cast
import warnings

import MinkowskiEngine as ME
import copy
from mmdet3d.registry import MODELS
from .mink_unet import Res16UNet34C
from .projection_utils import unified_projection_and_sample
from types import SimpleNamespace
from collections import deque, defaultdict

@MODELS.register_module()
class Conv3DFusionModule(nn.Module):
    """3DMVÂºè3DÂç∑ÁßØËûçÂêàÊ®°Âùó
    
    ‰ªøÁÖß3DMVÊû∂ÊûÑËÆæËÆ°ÔºåÈÄöËøá3DÂç∑ÁßØÂÆûÁé∞Á©∫Èó¥‰∏ÄËá¥ÊÄßÁöÑ2D-3DÁâπÂæÅËûçÂêàÔºö
    - features3d: Â§ÑÁêÜ3DÂá†‰ΩïÁâπÂæÅÔºå96Áª¥ ‚Üí 64Áª¥
    - features2d: Â§ÑÁêÜÊäïÂΩ±ÂêéÁöÑ2DÁâπÂæÅÔºå256Áª¥ ‚Üí 32Áª¥  
    - features_fusion: ËûçÂêà‰∏§ÁßçÁâπÂæÅÔºå96Áª¥(64+32) ‚Üí 128Áª¥
    
    ‰∏éÂéüÁÇπÁ∫ßËûçÂêàÁõ∏ÊØîÔºå3DÂç∑ÁßØËÉΩÊõ¥Â•ΩÂú∞Âà©Áî®Á©∫Èó¥ÈÇªÂüü‰ø°ÊÅØËøõË°åÁâπÂæÅËûçÂêà
    """
    
    def __init__(self, 
                 feat3d_dim: int = 96,      # 3DÁâπÂæÅÁª¥Â∫¶ÔºàMinkUNetËæìÂá∫Ôºâ
                 feat2d_dim: int = 256,     # 2DÁâπÂæÅÁª¥Â∫¶ÔºàCLIPÊäïÂΩ±ÂêéÔºâ
                 output_dim: int = 128,     # ÊúÄÁªàËæìÂá∫Áª¥Â∫¶
                 enable_debug: bool = False,
                 collect_gradient_stats: bool = True):
        super().__init__()
        
        self.feat3d_dim = feat3d_dim
        self.feat2d_dim = feat2d_dim
        self.output_dim = output_dim
        self.enable_debug = enable_debug
        self.collect_gradient_stats = collect_gradient_stats
        
        # ‰ªøÁÖß3DMVÁöÑfeatures3dÔºöÂ§ÑÁêÜ3DÂá†‰ΩïÁâπÂæÅ (96 ‚Üí 64Áª¥)
        self.features3d = nn.Sequential(
            # Á¨¨‰∏ÄÈò∂ÊÆµÔºöÁâπÂæÅÊâ©Â±ïÂíåÁ©∫Èó¥ÊÑüÁü•
            ME.MinkowskiConvolution(feat3d_dim, 64, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(64),
            ME.MinkowskiReLU(True),
            # 1x1x1Á≤æÁÇºÂç∑ÁßØÔºöÊèêÂèñÊõ¥ÊäΩË±°ÁöÑÁâπÂæÅË°®Á§∫
            ME.MinkowskiConvolution(64, 64, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(64),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(0.1),
            
            # Á¨¨‰∫åÈò∂ÊÆµÔºö‰øùÊåÅ64Áª¥ÔºåËøõ‰∏ÄÊ≠•ÁâπÂæÅÊäΩË±°
            ME.MinkowskiConvolution(64, 64, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(64),
            ME.MinkowskiReLU(True),
            ME.MinkowskiConvolution(64, 64, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(64),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(0.1)
        )
        
        # ‰ªøÁÖß3DMVÁöÑfeatures2dÔºöÂ§ÑÁêÜÊäïÂΩ±ÂêéÁöÑ2DÁâπÂæÅ (256 ‚Üí 32Áª¥)
        self.features2d = nn.Sequential(
            # Á¨¨‰∏ÄÈò∂ÊÆµÔºöÁª¥Â∫¶ÂéãÁº© 256 ‚Üí 64
            ME.MinkowskiConvolution(feat2d_dim, 64, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(64),
            ME.MinkowskiReLU(True),
            # 1x1x1Á≤æÁÇºÂç∑ÁßØ
            ME.MinkowskiConvolution(64, 64, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(64),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(0.1),
            
            # Á¨¨‰∫åÈò∂ÊÆµÔºöËøõ‰∏ÄÊ≠•ÂéãÁº© 64 ‚Üí 32
            ME.MinkowskiConvolution(64, 32, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(32),
            ME.MinkowskiReLU(True),
            ME.MinkowskiConvolution(32, 32, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(32),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(0.1)
        )
        
        # ‰ªøÁÖß3DMVÁöÑfeaturesÔºöÂ§öÊ®°ÊÄÅÁâπÂæÅËûçÂêà (96Áª¥=64+32 ‚Üí 128Áª¥)
        self.features_fusion = nn.Sequential(
            # ËûçÂêàÈò∂ÊÆµÔºöÂ§ÑÁêÜconcatenatedÁâπÂæÅ
            ME.MinkowskiConvolution(96, 128, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(128),
            ME.MinkowskiReLU(True),
            # 1x1x1Á≤æÁÇºÂç∑ÁßØÔºöÊ∑±Â±ÇÁâπÂæÅÊäΩË±°
            ME.MinkowskiConvolution(128, 128, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(128),
            ME.MinkowskiReLU(True),
            ME.MinkowskiConvolution(128, output_dim, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(output_dim),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(0.1)
        )
        
        self._last_monitor = {}
        self._last_feats = None
        self._grad_feature_norms = {}
        self._prev_grad_stats = {}

        if self.enable_debug:
            print(f"üîß ÂàùÂßãÂåñConv3DFusionModule: 3D({feat3d_dim}‚Üí64) + 2D({feat2d_dim}‚Üí32) ‚Üí ËûçÂêà({96}‚Üí{output_dim})")

    def forward(self, feat3d_sparse: ME.SparseTensor, feat2d_sparse: ME.SparseTensor) -> ME.SparseTensor:
        """
        3DÂç∑ÁßØËûçÂêàÂâçÂêë‰º†Êí≠
        
        Args:
            feat3d_sparse: ME.SparseTensorÔºå3DÁâπÂæÅ (N, feat3d_dim)
            feat2d_sparse: ME.SparseTensorÔºå2DÁâπÂæÅ (N, feat2d_dim)
        Returns:
            fused_sparse: ME.SparseTensorÔºåËûçÂêàÁâπÂæÅ (N, output_dim)
        """
        if self.enable_debug:
            print(f"üîç Conv3DËûçÂêàËæìÂÖ•: 3DÁâπÂæÅ{feat3d_sparse.features.shape}, 2DÁâπÂæÅ{feat2d_sparse.features.shape}")
        
        # ÂàÜÂà´Â§ÑÁêÜ3DÂíå2DÁâπÂæÅÔºöÊ®°‰ªø3DMVÁöÑÂèåÂàÜÊîØËÆæËÆ°
        f3d_processed = self.features3d(feat3d_sparse)      # 96 ‚Üí 64Áª¥
        f2d_processed = self.features2d(feat2d_sparse)      # 256 ‚Üí 32Áª¥
        
        f3d_feats = f3d_processed.features
        f2d_feats = f2d_processed.features

        if self.enable_debug:
            print(f"üîç ÂàÜÊîØÂ§ÑÁêÜÂêé: 3DÁâπÂæÅ{f3d_feats.shape}, 2DÁâπÂæÅ{f2d_feats.shape}")

        monitor = {}
        with torch.no_grad():
            monitor['feat3d_mean_abs'] = f3d_feats.abs().mean().item()
            monitor['feat3d_std'] = f3d_feats.std().item()
            monitor['feat3d_nonzero_ratio'] = (f3d_feats.abs() > 1e-3).float().mean().item()

            monitor['feat2d_mean_abs'] = f2d_feats.abs().mean().item()
            monitor['feat2d_std'] = f2d_feats.std().item()
            monitor['feat2d_nonzero_ratio'] = (f2d_feats.abs() > 1e-3).float().mean().item()

        if self.collect_gradient_stats:
            prev_norms = getattr(self, '_grad_feature_norms', None)
            self._prev_grad_stats = prev_norms.copy() if prev_norms else {}
            self._grad_feature_norms = {}
        else:
            self._prev_grad_stats = {}
        
        # ÁâπÂæÅÊãºÊé•ÔºöÂú®ÈÄöÈÅìÁª¥Â∫¶concat (64+32=96Áª¥)
        # ÊçïÊçâ3DÂùêÊ†áÈ°∫Â∫èÂπ∂ÂØπÈΩê2DÁâπÂæÅ
        coord_manager = f3d_processed.coordinate_manager
        coords3d = f3d_processed.C.float()

        try:
            f2d_aligned = f2d_processed.features_at_coordinates(coords3d)
        except RuntimeError as err:
            if self.enable_debug:
                print(f"‚ö†Ô∏è features_at_coordinates ÂºÇÂ∏∏: {err}")
            f2d_aligned = f3d_processed.features.new_zeros(
                f3d_processed.features.shape[0], f2d_processed.features.shape[1])

        if not torch.isfinite(f2d_aligned).all():
            invalid_mask = ~torch.isfinite(f2d_aligned)
            if self.enable_debug:
                invalid_count = invalid_mask.sum().item()
                print(f"‚ö†Ô∏è ÂØπÈΩêÂêéÁöÑ2DÁâπÂæÅÂá∫Áé∞NaN/InfÔºåÂ∑≤ÁΩÆÈõ∂ÔºåÊï∞Èáè: {invalid_count}")
            f2d_aligned = f2d_aligned.masked_fill(invalid_mask, 0)

        if self.collect_gradient_stats:
            def _capture(name):
                def hook(grad):
                    if grad is None:
                        return
                    if not hasattr(self, '_grad_feature_norms'):
                        self._grad_feature_norms = {}
                    with torch.no_grad():
                        self._grad_feature_norms[f'grad_norm_{name}'] = grad.detach().norm().item()
                return hook

            f3d_feats.register_hook(_capture('feat3d'))
            f2d_aligned.register_hook(_capture('feat2d'))

        if self.collect_gradient_stats:
            def _capture(name):
                key = f'grad_norm_{name}_raw'

                def hook(grad):
                    if grad is None:
                        return
                    if not hasattr(self, '_grad_feature_norms'):
                        self._grad_feature_norms = {}
                    with torch.no_grad():
                        self._grad_feature_norms[key] = grad.detach().norm().item()
                return hook

            if f3d_feats.requires_grad:
                f3d_feats.register_hook(_capture('feat3d'))
            if f2d_aligned.requires_grad:
                f2d_aligned.register_hook(_capture('feat2d'))

        # Â≠òÂÇ®ÂØπÈΩêÂêéÁöÑÁâπÂæÅÂø´ÁÖßÔºå‰æø‰∫éË∞ÉËØïÁõëÊéß
        self._last_monitor = monitor
        self._last_feats = {
            'f3d_feats': f3d_feats.detach(),
            'f2d_feats': f2d_aligned.detach()
        }

        manual_features = torch.cat([f3d_feats, f2d_aligned], dim=1)
        if self.collect_gradient_stats and manual_features.requires_grad:
            manual_features.register_hook(_capture('fusion'))
        fused_sparse = ME.SparseTensor(
            features=manual_features,
            coordinate_map_key=f3d_processed.coordinate_map_key,
            coordinate_manager=coord_manager
        )

        if self.enable_debug:
            print(f"üîç ÊâãÂä®ÁâπÂæÅÊãºÊé•ÊàêÂäü: {fused_sparse.features.shape}")

        # ÊúÄÁªàËûçÂêàÂç∑ÁßØÔºö96 ‚Üí output_dimÁª¥
        output_sparse = self.features_fusion(fused_sparse)

        self._last_monitor = monitor
        self._last_feats = {'f3d_feats': f3d_feats, 'f2d_feats': f2d_feats}

        if self.enable_debug:
            print(f"üîç Conv3DËûçÂêàËæìÂá∫: {output_sparse.features.shape}")

        return output_sparse


# LiteFusionGateÁ±ªÂ∑≤Âà†Èô§ - ‰∏ìÈó®‰ΩøÁî®3DÂç∑ÁßØËûçÂêà


@MODELS.register_module(name='BiFusionEncoder3DMV')
class BiFusionEncoder(nn.Module):
    """Enhanced Bi-Fusion Encoder combining 2D CLIP visual features and 3D Sparse features.
    
    üî• Êñ∞Â¢û3DMVÂºè3DÂç∑ÁßØËûçÂêàÊîØÊåÅÔºö
    
    Êû∂ÊûÑËÆæËÆ°Ôºö
    - ‰º†ÁªüÊ®°ÂºèÔºöÁÇπÁ∫ßËûçÂêàÔºàLiteFusionGateÔºâ
    - Â¢ûÂº∫Ê®°ÂºèÔºö3DÂç∑ÁßØËûçÂêàÔºàConv3DFusionModuleÔºâ
    - Ê∑∑ÂêàÊ®°ÂºèÔºö‰∏§ÁßçËûçÂêàÊñπÂºèÁªìÂêà
    
    ‰ΩøÁî®ÊñπÊ≥ïÔºö
    1. Á∫ØÁÇπÁ∫ßËûçÂêàÔºàÈªòËÆ§ÔºâÔºö
       use_conv3d_fusion=False, fusion_mode="point_only"
       
    2. Á∫Ø3DÂç∑ÁßØËûçÂêàÔºö
       use_conv3d_fusion=True, fusion_mode="conv3d_only"
       
    3. Ê∑∑ÂêàËûçÂêàÔºö
       use_conv3d_fusion=True, fusion_mode="hybrid"
    
    Ê†∏ÂøÉÂéüÁêÜÔºö
    - 3DÂàÜÊîØÔºöMinkUNet(96Áª¥) ‚Üí Conv3DÂ§ÑÁêÜ ‚Üí 64Áª¥
    - 2DÂàÜÊîØÔºöCLIPÁâπÂæÅ(256Áª¥) ‚Üí Conv3DÂ§ÑÁêÜ ‚Üí 32Áª¥  
    - ËûçÂêàÔºöConcat(96Áª¥) ‚Üí Conv3D ‚Üí ÊúÄÁªàÁâπÂæÅ
    
    Áõ∏ÊØîÁÇπÁ∫ßËûçÂêàÁöÑ‰ºòÂäøÔºö
    - Á©∫Èó¥‰∏ÄËá¥ÊÄßÔºöÂà©Áî®3DÂç∑ÁßØÁöÑÁ©∫Èó¥ÈÇªÂüü‰ø°ÊÅØ
    - Â±ÇÊ¨°ËûçÂêàÔºöÂú®Âç∑ÁßØÁâπÂæÅÂ±ÇÁ∫ßËøõË°åËûçÂêàÔºåÊõ¥Ê∑±ÂÖ•
    - Á´ØÂà∞Á´ØÂ≠¶‰π†ÔºöÊï¥‰∏™ËøáÁ®ãÂèØÂæÆÂàÜÔºåÊîØÊåÅÊ¢ØÂ∫¶Âèç‰º†
    """

    def __init__(self,
                 voxel_size: float = 0.02,
                 use_amp: bool = True,
                 # üéØ ÁâπÂæÅÂüüÈÖçÁΩÆÔºàÁÆÄÂåñ‰∏∫‰ªÖÊîØÊåÅ60√ó80È¢ÑËÆ°ÁÆóÔºâ
                 feat_space: str = "precomp_60x80",      # Âõ∫ÂÆö‰∏∫È¢ÑËÆ°ÁÆóÁâπÂæÅ
                 use_precomp_2d: bool = True,            # ÈªòËÆ§ÂêØÁî®È¢ÑËÆ°ÁÆóÁâπÂæÅ
                 # üî• 3DÂç∑ÁßØËûçÂêàÈÖçÁΩÆÔºà‰∏ìÈó®‰ΩøÁî®Conv3DÔºâ
                 conv3d_output_dim: int = 256,           # 3DÂç∑ÁßØËûçÂêàËæìÂá∫Áª¥Â∫¶ÔºåÈªòËÆ§256‰øùÊåÅÂÖºÂÆπ
                 # Ë∞ÉËØïÊ®°ÂºèÊéßÂà∂
                 debug: bool = False,
                 collect_gradient_stats: bool = True,
                 **kwargs):  # Êé•Êî∂ÂÖ∂‰ªñÊú™Áü•ÂèÇÊï∞
        super().__init__()
        
        # üîß ‰øÆÂ§çÔºöÂ¶ÇÊûúvoxel_sizeÊòØÂ≠óÂÖ∏Ôºàconfig‰º†ÂÖ•ÈîôËØØÔºâÔºåÊèêÂèñÊàñ‰ΩøÁî®ÈªòËÆ§ÂÄº
        if isinstance(voxel_size, dict):
            print(f"‚ö†Ô∏è Ë≠¶Âëä: voxel_size‰º†ÂÖ•‰∫ÜÂ≠óÂÖ∏Ôºå‰ΩøÁî®ÈªòËÆ§ÂÄº0.02")
            voxel_size = 0.02
        
        # üéØ ÁâπÂæÅÂüüÈÖçÁΩÆ
        self.feat_space = feat_space
        self.use_precomp_2d = use_precomp_2d
        self.debug = debug
        
        # üî• 3DÂç∑ÁßØËûçÂêàÈÖçÁΩÆÔºà‰∏ìÈó®‰ΩøÁî®Conv3DÔºâ
        self.conv3d_output_dim = conv3d_output_dim

        # üéØ Ê†πÊçÆÁâπÂæÅÂüüËÆæÁΩÆÔºàÁÆÄÂåñÔºåÂè™ÊîØÊåÅ60√ó80È¢ÑËÆ°ÁÆóÔºâ
        if feat_space != "precomp_60x80":
            print(f"Ë≠¶Âëä: ÂΩìÂâç‰ªÖÊîØÊåÅprecomp_60x80ÁâπÂæÅÂüüÔºåËá™Âä®ÂàáÊç¢Âà∞precomp_60x80")
            feat_space = "precomp_60x80"
        
        # Âà†Èô§Enhanced CLIPÁºñÁ†ÅÂô®Ôºà‰∏çÂÜçÈúÄË¶ÅÔºâ
        # self.enhanced_clip = None
        
        # 3D encoder - ‰øùÊåÅÂéüÂßã96Áª¥‰ª•ÂÖºÂÆπÈ¢ÑËÆ≠ÁªÉÊùÉÈáç
        cfg_backbone = SimpleNamespace(dilations=[1, 1, 1, 1], bn_momentum=0.02, conv1_kernel_size=5)
        self.backbone3d = Res16UNet34C(in_channels=3, out_channels=96, config=cfg_backbone, D=3)
        
        # üî• 3DÂç∑ÁßØËûçÂêàÊ®°ÂùóÔºö‰∏ìÈó®‰ΩøÁî®Conv3DËûçÂêà
        self.conv3d_fusion = Conv3DFusionModule(
            feat3d_dim=96,          # MinkUNetËæìÂá∫Áª¥Â∫¶
            feat2d_dim=256,         # 2DÁâπÂæÅÁª¥Â∫¶ÔºàÈÄÇÈÖçÂêéÔºâ
            output_dim=self.conv3d_output_dim,  # ÂèØÈÖçÁΩÆËæìÂá∫Áª¥Â∫¶
            enable_debug=self.debug,
            collect_gradient_stats=collect_gradient_stats
        )
        self.cos_proj3d = nn.Linear(64, 32, bias=False)
        self.cos_proj2d = nn.Identity()
        for p in self.cos_proj3d.parameters():
            p.requires_grad_(False)
        with torch.no_grad():
            self.cos_proj3d.weight.zero_()
            self.cos_proj3d.weight[:, :32] = torch.eye(32)
        if self.debug:
            print(f"üîß ÂàùÂßãÂåñ3DÂç∑ÁßØËûçÂêàÊ®°Âùó: ËæìÂá∫Áª¥Â∫¶={self.conv3d_output_dim}")
        
        # üéØ È¢ÑËÆ°ÁÆóÁâπÂæÅÈÄÇÈÖçÂô®ÔºàÊÉ∞ÊÄßÂàùÂßãÂåñÔºâ
        self.precomp_adapter = None
        
        # üéØ AlphaÂõûÈÄÄÂÄºÔºàÂèØÂ≠¶‰π†ÂèÇÊï∞Ôºâ
        
        # üéØ ÊçüÂ§±ÂéÜÂè≤ËÆ∞ÂΩïÔºàÁî®‰∫éÊäñÂä®ÂàÜÊûêÔºâ
        self._loss_hist = deque(maxlen=100)

        # Âü∫Êú¨ËøêË°å/Ë∞ÉËØïÂºÄÂÖ≥ÂíåÁªüËÆ°ÁªìÊûÑ
        self.voxel_size = voxel_size
        self.use_amp = use_amp
        self.standard_scannet_intrinsics = (577.870605, 577.870605, 319.5, 239.5)
        self.align_corners = True  # ‰∏éÊäïÂΩ±ÈááÊ†∑‰øùÊåÅ‰∏ÄËá¥
        self.max_depth = 20.0
        self._collect_fusion_stats = True
        self._collect_gradient_stats = collect_gradient_stats  # Ê¢ØÂ∫¶ÁªüËÆ°ËæìÂá∫Áã¨Á´ã‰∫édebug
        self._fusion_stats = {}
        self._stats_history = []

        self._param_grad_sums = defaultdict(float)
        self._param_grad_groups = {}
        self._registered_param_ids = set()
        self._last_param_grad_norms = {}

        # üî• ËæìÂá∫ÈÖçÁΩÆ‰ø°ÊÅØ
        self._print_config_summary()

        if self._collect_gradient_stats:
            self._register_grad_param_hooks()
    
    def _print_config_summary(self):
        """ÊâìÂç∞ÂΩìÂâçÈÖçÁΩÆÊëòË¶Å"""
        print("=" * 60)
        print("üî• BiFusionEncoderÈÖçÁΩÆÊëòË¶Å - ‰∏ìÁî®3DÂç∑ÁßØËûçÂêàÁâàÊú¨")
        print("=" * 60)
        print(f"ÁâπÂæÅÂüü: {self.feat_space}")
        print(f"‰ΩøÁî®È¢ÑËÆ°ÁÆó2DÁâπÂæÅ: {self.use_precomp_2d}")
        print(f"‰ΩìÁ¥†Â§ßÂ∞è: {self.voxel_size}")
        print(f"Ë∞ÉËØïÊ®°Âºè: {self.debug}")
        print("-" * 40)
        print("üéØ ËûçÂêàÈÖçÁΩÆ:")
        print(f"  ËûçÂêàÊ®°Âºè: ‰∏ìÁî®3DÂç∑ÁßØËûçÂêà")
        print(f"  3DÂç∑ÁßØËæìÂá∫Áª¥Â∫¶: {self.conv3d_output_dim}")
        print(f"  3DÂç∑ÁßØÊ®°Âùó: {'Â∑≤ÂàùÂßãÂåñ' if self.conv3d_fusion is not None else 'Êú™ÂàùÂßãÂåñ'}")
        print(f"  Ê¢ØÂ∫¶ÁõëÊéß: {'ÂêØÁî®' if self._collect_gradient_stats else 'ÂÖ≥Èó≠'}")
        print("-" * 40)
        print("üìä Êû∂ÊûÑËØ¥Êòé:")
        print("  Ê®°Âºè: 3DMVÂºè3DÂç∑ÁßØËûçÂêà")  
        print("  ÁâπÁÇπ: Á©∫Èó¥‰∏ÄËá¥ÊÄßÂº∫ÔºåÂà©Áî®ÈÇªÂüü‰ø°ÊÅØÔºåÁ´ØÂà∞Á´ØÂ≠¶‰π†")
        print("  ÊµÅÁ®ã: 3D(96Áª¥)‚Üí64Áª¥ + 2D(256Áª¥)‚Üí32Áª¥ ‚Üí Concat(96Áª¥) ‚Üí ËûçÂêàËæìÂá∫")
        print("=" * 60)
    
    @classmethod
    def create_conv3d_config(cls, **kwargs):
        """ÂàõÂª∫3DÂç∑ÁßØËûçÂêàÈÖçÁΩÆÁöÑ‰æøÊç∑ÊñπÊ≥ï
        
        Á§∫‰æã:
        # ÈªòËÆ§ÈÖçÁΩÆÔºà256Áª¥ËæìÂá∫Ôºâ
        encoder = BiFusionEncoder.create_conv3d_config()
        
        # Ëá™ÂÆö‰πâËæìÂá∫Áª¥Â∫¶
        encoder = BiFusionEncoder.create_conv3d_config(
            conv3d_output_dim=128,
            debug=True
        )
        """
        default_config = {
            'conv3d_output_dim': 256,
            'debug': False,
            'collect_gradient_stats': True
        }
        default_config.update(kwargs)
        return cls(**default_config)

    def _create_sparse_tensor_from_features(self, 
                                             features: torch.Tensor, 
                                             coordinates: torch.Tensor,
                                             coord_manager=None) -> ME.SparseTensor:
        """
        Â∞ÜÁâπÂæÅÂíåÂùêÊ†áËΩ¨Êç¢‰∏∫MinkowskiEngineÁ®ÄÁñèÂº†Èáè
        
        Args:
            features: (N, C) ÁâπÂæÅÂº†Èáè
            coordinates: (N, 3) ÂùêÊ†áÂº†ÈáèÔºà‰∏ñÁïåÂùêÊ†áÁ≥ªÔºâ
            coord_manager: ÂùêÊ†áÁÆ°ÁêÜÂô®ÔºåÁî®‰∫éÁ°Æ‰øùÁ®ÄÁñèÂº†ÈáèÂÖºÂÆπÊÄß
        Returns:
            ME.SparseTensor: Á®ÄÁñèÂº†Èáè
        """
        # ÂùêÊ†áÈáèÂåñÔºö‰∏ñÁïåÂùêÊ†á ‚Üí ‰ΩìÁ¥†ÂùêÊ†á
        coords_int = torch.round(coordinates / self.voxel_size).to(torch.int32)
        
        # Ê∑ªÂä†batchÁª¥Â∫¶Ôºö(N, 3) ‚Üí (N, 4)ÔºåÁ¨¨‰∏ÄÂàó‰∏∫batch_index=0
        coords_with_batch = torch.cat([
            torch.zeros(coords_int.size(0), 1, dtype=torch.int32, device=coords_int.device),
            coords_int
        ], dim=1)
        
        sparse_kwargs = {
            'features': features.float(),
            'coordinates': coords_with_batch,
            'device': features.device
        }
        if coord_manager is not None:
            sparse_kwargs['coordinate_manager'] = coord_manager

        sparse_tensor = ME.SparseTensor(**sparse_kwargs)
        
        if self.debug:
            print(f"üîß ÂàõÂª∫Á®ÄÁñèÂº†Èáè: ÁâπÂæÅ{features.shape} ‚Üí ÂùêÊ†á{coords_with_batch.shape}")
        
        return sparse_tensor
    
    def _convert_2d_features_to_sparse(self, 
                                       feat2d: torch.Tensor, 
                                       xyz_world: torch.Tensor,
                                       valid_mask: torch.Tensor,
                                       reference_sparse: ME.SparseTensor) -> ME.SparseTensor:
        """Â∞Ü 2D ÁâπÂæÅÈáçÊñ∞ÊéíÂàó‰∏∫‰∏é 3D Á®ÄÁñèÂº†Èáè‰∏ÄËá¥ÁöÑÂùêÊ†á„ÄÇ"""
        feat2d_filled = feat2d.clone()
        if feat2d_filled.numel() > 0:
            feat2d_filled[~valid_mask] = 0

        coords_ref = reference_sparse.C  # (M, 4)
        device = feat2d_filled.device
        feature_dim = feat2d_filled.shape[1]

        ordered_features = feat2d_filled.new_zeros((coords_ref.shape[0], feature_dim))
        hit_counts = feat2d_filled.new_zeros((coords_ref.shape[0],), dtype=feat2d_filled.dtype)

        coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_ref.cpu().tolist())}
        coords_full = torch.cat([
            torch.zeros(xyz_world.size(0), 1, dtype=torch.int32, device=xyz_world.device),
            torch.round(xyz_world / self.voxel_size).to(torch.int32)
        ], dim=1)

        coords_full_list = coords_full.cpu().tolist()
        idx_list = [coord_to_idx.get(tuple(coord), -1) for coord in coords_full_list]
        idx_tensor = torch.tensor(idx_list, device=device, dtype=torch.long)

        valid_idx_mask = idx_tensor >= 0
        if not valid_idx_mask.all():
            missing = (~valid_idx_mask).sum().item()
            if missing > 0:
                warnings.warn(f"{missing} points not found in sparse coordinate map; skipped.", stacklevel=2)

        if valid_idx_mask.any():
            idx_tensor_valid = idx_tensor[valid_idx_mask]
            feat_selected = feat2d_filled[valid_idx_mask]
            ordered_features.index_add_(0, idx_tensor_valid, feat_selected)
            hit_counts.index_add_(
                0,
                idx_tensor_valid,
                torch.ones(idx_tensor_valid.shape[0], device=device, dtype=hit_counts.dtype)
            )

        ordered_features = ordered_features / hit_counts.clamp_min(1.0).unsqueeze(-1)

        return ME.SparseTensor(
            features=ordered_features.float(),
            coordinate_manager=reference_sparse.coordinate_manager,
            coordinate_map_key=reference_sparse.coordinate_map_key
        )
    
    def _extract_features_from_sparse(self, 
                                      sparse_tensor: ME.SparseTensor, 
                                      target_coordinates: torch.Tensor,
                                      target_size: int) -> torch.Tensor:
        """
        ‰ªéÁ®ÄÁñèÂº†Èáè‰∏≠ÊèêÂèñÁõÆÊ†áÂùêÊ†áÂØπÂ∫îÁöÑÁâπÂæÅ
        
        Args:
            sparse_tensor: ME.SparseTensor ËæìÂÖ•Á®ÄÁñèÂº†Èáè
            target_coordinates: (N, 3) ÁõÆÊ†áÂùêÊ†áÔºà‰∏ñÁïåÂùêÊ†áÁ≥ªÔºâ
            target_size: int ÁõÆÊ†áÁâπÂæÅÊï∞Èáè
        Returns:
            torch.Tensor: (target_size, C) ÊèêÂèñÁöÑÁâπÂæÅ
        """
        # ÈáèÂåñÁõÆÊ†áÂùêÊ†á
        target_coords_int = torch.round(target_coordinates / self.voxel_size).to(torch.int32)
        target_coords_with_batch = torch.cat([
            torch.zeros(target_coords_int.size(0), 1, dtype=torch.int32, device=target_coords_int.device),
            target_coords_int
        ], dim=1)
        
        # ‰ΩøÁî®features_at_coordsÊñπÊ≥ïÊèêÂèñÁâπÂæÅ
        try:
            extracted = sparse_tensor.features_at_coordinates(target_coords_with_batch.float())
        except Exception as err:
            warnings.warn(f"Failed to look up sparse features: {err}", stacklevel=2)
            extracted = torch.zeros(
                target_size,
                sparse_tensor.features.shape[1],
                device=sparse_tensor.device,
                dtype=sparse_tensor.features.dtype
            )
            return extracted
        
        if extracted.shape[0] != target_size:
            padded = torch.zeros(
                target_size,
                sparse_tensor.features.shape[1],
                device=sparse_tensor.device,
                dtype=sparse_tensor.features.dtype
            )
            copy_len = min(extracted.shape[0], target_size)
            padded[:copy_len] = extracted[:copy_len]
            extracted = padded
        
        return extracted

        # üîß ‰øÆÊ≠£ÔºöÁ°Æ‰øùÁº©ÊîæÊñπÂêëÊ≠£Á°Æ
        # ÂéüÂßãScanNet: 640√ó480 (W√óH)
        # ÁâπÂæÅÂõæ: Wf√óHf
        scale_w = Wf / 640.0  # ÂÆΩÂ∫¶Áº©Êîæ
        scale_h = Hf / 480.0  # È´òÂ∫¶Áº©Êîæ

        # ÂÜÖÂèÇÁº©ÊîæÔºö‰øùÊåÅx/yÊñπÂêëÂØπÂ∫îÂÖ≥Á≥ª
        fx_feat = fx0 * scale_w  # xÊñπÂêëÁÑ¶Ë∑ùÈöèÂÆΩÂ∫¶Áº©Êîæ
        fy_feat = fy0 * scale_h  # yÊñπÂêëÁÑ¶Ë∑ùÈöèÈ´òÂ∫¶Áº©Êîæ
        cx_feat = cx0 * scale_w  # xÊñπÂêë‰∏ªÁÇπÈöèÂÆΩÂ∫¶Áº©Êîæ
        cy_feat = cy0 * scale_h  # yÊñπÂêë‰∏ªÁÇπÈöèÈ´òÂ∫¶Áº©Êîæ

        if self.debug:
            print(f"üîß ÂÜÖÂèÇÁº©Êîæ: ÂÆΩÂ∫¶Áº©Êîæ={scale_w:.3f}, È´òÂ∫¶Áº©Êîæ={scale_h:.3f}")
            print(f"üîß ËÆ°ÁÆóÁªìÊûú: fx={fx_feat:.1f}, fy={fy_feat:.1f}, cx={cx_feat:.1f}, cy={cy_feat:.1f}")

        return (fx_feat, fy_feat, cx_feat, cy_feat)


    def get_pose_pick_stats(self):
        """‰øùÁïôÊé•Âè£ÔºåÂΩìÂâçÂÆûÁé∞‰∏çÁªüËÆ°ËØ•‰ø°ÊÅØ„ÄÇ"""
        return {}

    def reset_pose_pick_stats(self):
        """‰øùÁïôÊé•Âè£ÔºåÊó†ÈúÄÊâßË°åÈ¢ùÂ§ñÊìç‰Ωú„ÄÇ"""
        return None
    
    #ÔºüÔºü
    def _ensure_precomp_adapter(self, c_in: int):
        """ÊÉ∞ÊÄßÂàùÂßãÂåñÈ¢ÑËÆ°ÁÆóÁâπÂæÅÈÄÇÈÖçÂô®Ôºö512 ‚Üí 256"""
        if (self.precomp_adapter is None) or (self.precomp_adapter[0].in_features != c_in):
            # ÊåâÁÖß‰ºòÂåñÊåáÂçóË¶ÅÊ±ÇÔºöLinear(512‚Üí256) + LayerNorm
            self.precomp_adapter = nn.Sequential(
                nn.Linear(c_in, 256),
                nn.LayerNorm(256)
            ).to(next(self.parameters()).device)
            if self.debug:
                print(f"üîß ÂàùÂßãÂåñÈ¢ÑËÆ°ÁÆóÈÄÇÈÖçÂô®: {c_in} ‚Üí 256 (‰ºòÂåñÁâàÊú¨)")
            if self._collect_gradient_stats:
                self._register_params_to_group('feat2d', self.precomp_adapter.parameters())

    # ------------------------------------------------------------------
    # Ê¢ØÂ∫¶ÁõëÊéßËæÖÂä©ÂáΩÊï∞
    # ------------------------------------------------------------------
    def _make_param_hook(self, group_key: str):
        def _hook(grad: torch.Tensor):
            if grad is None or not self._collect_gradient_stats:
                return
            self._param_grad_sums[group_key] += grad.detach().pow(2).sum().item()

        return _hook

    def _register_params_to_group(self, group_key: str, params):
        if not self._collect_gradient_stats:
            return

        group_list = self._param_grad_groups.setdefault(group_key, [])
        for param in params:
            if (param is None) or (not getattr(param, 'requires_grad', False)):
                continue
            param_id = id(param)
            if param_id in self._registered_param_ids:
                continue
            group_list.append(param)
            param.register_hook(self._make_param_hook(group_key))
            self._registered_param_ids.add(param_id)

    def _register_grad_param_hooks(self):
        # Âü∫Á°ÄÂàÜÊîØ
        self._register_params_to_group('feat3d', list(self.backbone3d.parameters()))
        self._register_params_to_group('feat3d', list(self.conv3d_fusion.features3d.parameters()))
        self._register_params_to_group('feat2d', list(self.conv3d_fusion.features2d.parameters()))
        self._register_params_to_group('fusion', list(self.conv3d_fusion.features_fusion.parameters()))

        # Ëß£Á†ÅÂô®
        if hasattr(self, 'decoder') and self.decoder is not None:
            self._register_params_to_group('decoder', list(self.decoder.parameters()))

    def _pop_param_grad_norms(self) -> Dict[str, float]:
        if not self._collect_gradient_stats:
            return {}

        norms = {}
        for group_key, sq_sum in self._param_grad_sums.items():
            norms[f'grad_params_{group_key}'] = sq_sum ** 0.5 if sq_sum > 0 else 0.0

        self._param_grad_sums = defaultdict(float)
        return norms
    
    def update_loss_stat(self, loss_val: float):
        """Êõ¥Êñ∞ÊçüÂ§±ÂéÜÂè≤ËÆ∞ÂΩï"""
        self._loss_hist.append(float(loss_val))
    
    def get_loss_var(self):
        """Ëé∑ÂèñÊçüÂ§±ÊªëÁ™óÊñπÂ∑Æ"""
        if len(self._loss_hist) < 20:
            return None
        arr = torch.tensor(list(self._loss_hist))
        return float(arr.var(unbiased=False))
    
    # ÁÆÄÂåñÁöÑÁªüËÆ°ÊñπÊ≥ïÂ∑≤ÈõÜÊàêÂú®_process_single‰∏≠
    
    def get_fusion_statistics(self):
        """Ëé∑ÂèñËûçÂêàÁªüËÆ°‰ø°ÊÅØ"""
        return self._fusion_stats.copy() if self._fusion_stats else {}
    
    def get_fusion_ratios(self):
        """‰∏ìÈó®Ëé∑ÂèñËûçÂêàÊØî‰æãÁªüËÆ° - ‰æõHook‰ΩøÁî®"""
        if not self._fusion_stats:
            return {}

        keys = [
            'avg_confidence',
            'valid_ratio',
            'norm_ratio_2d_over_3d',
            'cos_2d3d_mean',
            'cos_2d3d_mean_ln'
        ]
        return {k: self._fusion_stats.get(k, 0.0) for k in keys if k in self._fusion_stats}
    
    # ËûçÂêàÂπ≥Ë°°ÊçüÂ§±Áõ∏ÂÖ≥ÊñπÊ≥ïÂ∑≤Âà†Èô§ - ‰∏ìÁî®Conv3D‰∏çÈúÄË¶Å
    
    def get_statistics_summary(self, last_n: int = 10):
        """Ëé∑ÂèñÊúÄËøëNÊ¨°ÁöÑÁªüËÆ°ÊëòË¶Å"""
        if not self._stats_history:
            return {}
            
        recent_stats = self._stats_history[-last_n:]
        summary = {}
        
        for key in recent_stats[0].keys():
            if key != 'total_points':
                values = [stats[key] for stats in recent_stats if key in stats]
                if values:
                    summary[f'{key}_mean'] = sum(values) / len(values)
                    summary[f'{key}_std'] = (sum((x - summary[f'{key}_mean'])**2 for x in values) / len(values))**0.5
        
        return summary

    # Âà†Èô§‰∫ÜÂ§çÊùÇÁöÑ _improved_projection_with_geometry ÂáΩÊï∞Ôºå
    # Áªü‰∏Ä‰ΩøÁî® unified_projection_and_sample

    def _extract_pose_matrix(self, cam_meta: Dict, sample_idx: int = 0):
        """‰ªé cam_info ‰∏≠ÊèêÂèñÂçïÂ∏ß pose Áü©ÈòµÔºàcam2worldÔºâ„ÄÇ"""
        if not isinstance(cam_meta, dict):
            return None

        pose = cam_meta.get('pose')
        if pose is None:
            return None

        if isinstance(pose, (list, tuple)):
            poses = [p for p in pose if p is not None]
            if not poses:
                return None
            index = min(sample_idx, len(poses) - 1)
            pose = poses[index]

        if isinstance(pose, torch.Tensor):
            return pose.to(dtype=torch.float32)
        if isinstance(pose, np.ndarray):
            return torch.from_numpy(pose).float()

        warnings.warn(f"Unsupported pose type {type(pose)}; ignoring pose.", stacklevel=2)
        return None

    def _transform_coordinates(self, xyz: torch.Tensor, pose_matrix: Optional[torch.Tensor]) -> Optional[torch.Tensor]:
        """Â∞Ü‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÁÇπËΩ¨Êç¢Âà∞Áõ∏Êú∫ÂùêÊ†áÁ≥ª„ÄÇ"""
        if pose_matrix is None:
            return None

        if pose_matrix.shape != (4, 4):
            warnings.warn(f"Unexpected pose shape {pose_matrix.shape}; ignoring pose.", stacklevel=2)
            return None

        pose = pose_matrix.to(device=xyz.device, dtype=xyz.dtype)
        if not torch.isfinite(pose).all():
            warnings.warn("Pose matrix contains NaN/Inf values; ignoring pose.", stacklevel=2)
            return None

        try:
            w2c = torch.inverse(pose)
        except RuntimeError as err:
            warnings.warn(f"Pose inversion failed: {err}; ignoring pose.", stacklevel=2)
            return None

        homo = torch.ones((xyz.shape[0], 1), device=xyz.device, dtype=xyz.dtype)
        xyz_cam = torch.cat([xyz, homo], dim=1) @ w2c.t()
        xyz_cam = xyz_cam[:, :3]

        if not torch.isfinite(xyz_cam).all():
            warnings.warn("Projected camera coordinates contain NaN/Inf; ignoring pose.", stacklevel=2)
            return None

        positive_depth_ratio = (xyz_cam[:, 2] > 0).float().mean().item()
        if positive_depth_ratio < 0.1:
            warnings.warn(f"Too few points with positive depth ({positive_depth_ratio:.3f}); ignoring pose.", stacklevel=2)
            return None

        return xyz_cam

    def _process_single(self, points: torch.Tensor, img: List[torch.Tensor], cam_meta: Dict, sample_idx: int = 0):
        """Â§ÑÁêÜÂçïÂ∏ß 2D-3D ËûçÂêàÊµÅÁ®ã„ÄÇ"""
        xyz = points[:, :3].contiguous()
        dev = xyz.device

        pose_matrix = self._extract_pose_matrix(cam_meta, sample_idx=sample_idx)
        xyz_cam_proj = self._transform_coordinates(xyz, pose_matrix)

        coords_int = torch.round(xyz / self.voxel_size).to(torch.int32)
        coords = torch.cat(
            [torch.zeros(coords_int.size(0), 1, dtype=torch.int32, device=coords_int.device), coords_int],
            dim=1)
        feats = points[:, 3:6].contiguous()
        field = ME.TensorField(coordinates=coords, features=feats)
        feat3d_sparse = self.backbone3d(field.sparse())

        clip_data = cam_meta.get('clip_pix') if isinstance(cam_meta, dict) else None
        if isinstance(clip_data, (list, tuple)):
            clip_candidates = [c for c in clip_data if c is not None]
            if clip_candidates:
                clip_data = clip_candidates[min(sample_idx, len(clip_candidates) - 1)]
            else:
                clip_data = None

        if xyz_cam_proj is None or clip_data is None:
            if clip_data is None:
                warnings.warn("Missing clip_pix feature; falling back to zero 2D features.", stacklevel=2)
            feat2d_raw = torch.zeros((points.shape[0], 256), device=dev, dtype=torch.float32)
            valid = torch.zeros(points.shape[0], device=dev, dtype=torch.bool)
        else:
            if isinstance(clip_data, torch.Tensor):
                feat_map = clip_data.to(device=dev, dtype=torch.float32)
            elif isinstance(clip_data, np.ndarray):
                feat_map = torch.from_numpy(clip_data).to(device=dev, dtype=torch.float32)
            else:
                warnings.warn(f"Unsupported clip_pix type {type(clip_data)}; using zero features.", stacklevel=2)
                feat_map = None

            if feat_map is None:
                feat2d_raw = torch.zeros((points.shape[0], 256), device=dev, dtype=torch.float32)
                valid = torch.zeros(points.shape[0], device=dev, dtype=torch.bool)
            else:
                feat2d_raw, valid = unified_projection_and_sample(
                    xyz_cam=xyz_cam_proj,
                    feat_map=feat_map.unsqueeze(0),
                    max_depth=self.max_depth,
                    align_corners=self.align_corners,
                    standard_intrinsics=self.standard_scannet_intrinsics,
                    debug=self.debug,
                    debug_prefix=f'[BiFusion3DMV] sample={sample_idx}'
                )
                if feat2d_raw.shape[-1] != 256:
                    self._ensure_precomp_adapter(feat2d_raw.shape[-1])
                    feat2d_raw = self.precomp_adapter(feat2d_raw) if self.precomp_adapter else feat2d_raw

        try:
            feat2d_sparse = self._convert_2d_features_to_sparse(
                feat2d_raw,
                xyz,
                valid,
                reference_sparse=feat3d_sparse
            )

            cos_mean = 0.0
            try:
                feat3d_base = feat3d_sparse.features
                feat2d_base = feat2d_sparse.features
                min_dim = min(feat3d_base.shape[1], feat2d_base.shape[1])
                if min_dim > 0:
                    cos_mean = float(F.cosine_similarity(
                        F.normalize(feat3d_base[:, :min_dim], dim=1),
                        F.normalize(feat2d_base[:, :min_dim], dim=1),
                        dim=1).mean().item())
            except Exception as err:
                warnings.warn(f"Failed to compute feature similarity: {err}", stacklevel=2)

            fused_sparse = self.conv3d_fusion(feat3d_sparse, feat2d_sparse)

            monitor_stats = getattr(self.conv3d_fusion, '_last_monitor', {}).copy()
            feat_dict = getattr(self.conv3d_fusion, '_last_feats', None)
            if feat_dict is not None:
                with torch.no_grad():
                    proj3d = self.cos_proj3d(feat_dict['f3d_feats'].detach().to(self.cos_proj3d.weight.device))
                    proj2d = self.cos_proj2d(feat_dict['f2d_feats'].detach().to(self.cos_proj3d.weight.device))
                    proj3d_ln = F.layer_norm(proj3d, proj3d.shape[-1:])
                    proj2d_ln = F.layer_norm(proj2d, proj2d.shape[-1:])
                    monitor_stats['cos_2d3d_mean_ln'] = F.cosine_similarity(proj3d_ln, proj2d_ln, dim=1).mean().item()
            else:
                monitor_stats = monitor_stats or {}

            if self._collect_gradient_stats:
                grad_stats = getattr(self.conv3d_fusion, '_prev_grad_stats', None)
                if grad_stats:
                    monitor_stats.update(grad_stats)
                if getattr(self, '_last_param_grad_norms', None):
                    monitor_stats.update(self._last_param_grad_norms)
                    g2d = self._last_param_grad_norms.get('grad_params_feat2d', 0.0)
                    g3d = self._last_param_grad_norms.get('grad_params_feat3d', 0.0)
                    monitor_stats['grad_ratio_2d_over_3d'] = g2d / (g3d + 1e-12)
            self.conv3d_fusion._last_feats = None

            fused = self._extract_features_from_sparse(fused_sparse, xyz, points.shape[0])

            if fused.shape[-1] != self.conv3d_output_dim:
                if fused.shape[-1] < self.conv3d_output_dim:
                    padding = torch.zeros(
                        fused.shape[0],
                        self.conv3d_output_dim - fused.shape[-1],
                        device=fused.device,
                        dtype=fused.dtype
                    )
                    fused = torch.cat([fused, padding], dim=-1)
                else:
                    fused = fused[:, :self.conv3d_output_dim]

            valid_ratio = valid.float().mean().item()
            conf_value = max(0.3, min(0.9, valid_ratio))
            conf = torch.full((points.shape[0], 1), conf_value, device=dev, dtype=torch.float32)

            if self.debug:
                print(f"[BiFusion3DMV] sample={sample_idx} valid_ratio={valid_ratio:.3f} cos_mean={cos_mean:.3f}")

        except Exception as e:
            warnings.warn(f"Conv3D fusion failed; using zero fallback. Details: {e}", stacklevel=2)
            fused = torch.zeros((points.shape[0], self.conv3d_output_dim), device=dev, dtype=torch.float32)
            conf = torch.full((points.shape[0], 1), 0.1, device=dev, dtype=torch.float32)
            self.conv3d_fusion._last_monitor = {}
            self.conv3d_fusion._last_feats = None
            self.conv3d_fusion._prev_grad_stats = {}
            if hasattr(self.conv3d_fusion, '_grad_feature_norms'):
                self.conv3d_fusion._grad_feature_norms = {}
            monitor_stats = {}
            self._last_param_grad_norms = {}

        # L2ÂΩí‰∏ÄÂåñÁ°Æ‰øùÁâπÂæÅÁ®≥ÂÆöÊÄßÔºåÂêåÊó∂ËÆ∞ÂΩïÂΩí‰∏ÄÂåñÂâçÁöÑÂπÖÂ∫¶‰ø°ÊÅØ
        fused_pre_norm = fused.detach()
        fused = F.normalize(fused, dim=-1, eps=1e-6)
        
        # ÁÆÄÂåñÁöÑÁªüËÆ°‰ø°ÊÅØÊî∂ÈõÜ
        if self._collect_fusion_stats:
            try:
                valid_ratio = valid.float().mean().item()
                feat2d_norm = feat2d_raw.norm(dim=-1).clamp_min(1e-6).mean().item()
                
                feat3d_norm = feat3d_sparse.features.norm(dim=-1).clamp_min(1e-6).mean().item()
                norm_ratio = feat2d_norm / max(feat3d_norm, 1e-6)

                with torch.no_grad():
                    monitor_stats['fused_mean_abs_raw'] = fused_pre_norm.abs().mean().item()
                    monitor_stats['fused_std_raw'] = fused_pre_norm.std().item()
                    monitor_stats['fused_norm_mean_raw'] = fused_pre_norm.norm(dim=-1).mean().item()
                    monitor_stats['fused_mean_abs'] = fused.abs().mean().item()
                    monitor_stats['fused_std'] = fused.std().item()

                self._fusion_stats = {
                    'valid_ratio': valid_ratio,
                    'valid_points_ratio': valid_ratio,
                    'avg_confidence': conf_value,
                    'norm_ratio_2d_over_3d': norm_ratio,
                    'cos_2d3d_mean': cos_mean,
                    'norm_2d_mean': feat2d_norm,
                    'norm_3d_mean': feat3d_norm
                }
                self._fusion_stats.update(monitor_stats)
                
                if self.debug:
                    print(f"üìä ËûçÂêàÁªüËÆ°: ÊúâÊïàÊØî‰æã={valid_ratio:.3f}, 2DÁâπÂæÅËåÉÊï∞={feat2d_norm:.3f}")
            except Exception as e:
                if self.debug:
                    print(f"‚ö†Ô∏è ÁªüËÆ°Êî∂ÈõÜÂ§±Ë¥•: {e}")
        
        return fused, conf, valid

    def forward(self, points_list, imgs, cam_info):
        """ÁÆÄÂåñÁöÑforwardÂáΩÊï∞ÔºöÊâπÈáèÂ§ÑÁêÜ3D-2DËûçÂêà"""
        # 1. ËæìÂÖ•Ê†ºÂºèÊ†áÂáÜÂåñ
        if self.debug:
            print(f"üîç forwardËæìÂÖ•Ê¶ÇËßà | points_list: {type(points_list)} | imgs: {type(imgs)} | cam_info: {type(cam_info)}")

        if self._collect_gradient_stats:
            self._last_param_grad_norms = self._pop_param_grad_norms()
        else:
            self._last_param_grad_norms = {}

        if not isinstance(points_list, list):
            raise TypeError(f"points_list must be list[Tensor], got {type(points_list)}")
        if not isinstance(imgs, list):
            raise TypeError(f"imgs must be list[Tensor], got {type(imgs)}")
        if not isinstance(cam_info, list):
            raise TypeError(f"cam_info must be list[dict], got {type(cam_info)}")

        batch_size = len(points_list)
        if len(imgs) != batch_size:
            if len(imgs) == 1:
                single_img = imgs[0]
                if torch.is_tensor(single_img):
                    imgs = [single_img.clone() for _ in range(batch_size)]
                else:
                    imgs = [copy.deepcopy(single_img) for _ in range(batch_size)]
            else:
                raise RuntimeError(f"points({batch_size}) and imgs({len(imgs)}) length mismatch")
        if len(cam_info) != batch_size:
            if len(cam_info) == 1:
                single_meta = cam_info[0]
                cam_info = [copy.deepcopy(single_meta) for _ in range(batch_size)]
            else:
                raise RuntimeError(f"cam_info({len(cam_info)}) and points({batch_size}) length mismatch")
        
        # 3. ÈÄêÊ†∑Êú¨Â§ÑÁêÜ
        feat_fusion_list, conf_list, valid_mask_list = [], [], []
        
        for idx, (pts, img, meta) in enumerate(zip(points_list, imgs, cam_info)):
            # ÁÆÄÂåñmeta‰ø°ÊÅØÂ§ÑÁêÜÔºöPKLÊñá‰ª∂ÊòØÂ∏ßÁ∫ßÁªÑÁªáÔºåÁõ¥Êé•Â§çÂà∂
            meta_std = meta if meta is not None else {}
            
            # Â§ÑÁêÜÂçï‰∏™Ê†∑Êú¨Ôºå‰º†ÈÄíÊ†∑Êú¨Á¥¢Âºï
            fused, conf, valid_mask = self._process_single(pts, img, meta_std, idx)
            
            feat_fusion_list.append(fused)
            conf_list.append(conf)
            valid_mask_list.append(valid_mask)
        
        return {
            'feat_fusion': feat_fusion_list,
            'conf_2d': conf_list,
            'valid_projection_mask': valid_mask_list
        }
