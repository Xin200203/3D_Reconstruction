import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import contextlib
from typing import List, Dict, Optional, Tuple, Union, cast
import warnings

import MinkowskiEngine as ME
import copy
from mmdet3d.registry import MODELS
from .mink_unet import Res16UNet34C
from .projection_utils import unified_projection_and_sample
from types import SimpleNamespace
from collections import deque, defaultdict

@MODELS.register_module()
class Conv3DFusionModule(nn.Module):
    """3DMVå¼3Då·ç§¯èåˆæ¨¡å—
    
    ä»¿ç…§3DMVæ¶æ„è®¾è®¡ï¼Œé€šè¿‡3Då·ç§¯å®ç°ç©ºé—´ä¸€è‡´æ€§çš„2D-3Dç‰¹å¾èåˆï¼š
    - features3d: å¤„ç†3Då‡ ä½•ç‰¹å¾ï¼Œ96ç»´ â†’ 64ç»´
    - features2d: å¤„ç†æŠ•å½±åçš„2Dç‰¹å¾ï¼Œ256ç»´ â†’ 32ç»´  
    - features_fusion: èåˆä¸¤ç§ç‰¹å¾ï¼Œ96ç»´(64+32) â†’ 128ç»´
    
    ä¸åŸç‚¹çº§èåˆç›¸æ¯”ï¼Œ3Då·ç§¯èƒ½æ›´å¥½åœ°åˆ©ç”¨ç©ºé—´é‚»åŸŸä¿¡æ¯è¿›è¡Œç‰¹å¾èåˆ
    """
    
    def __init__(self, 
                 feat3d_dim: int = 96,      # 3Dç‰¹å¾ç»´åº¦ï¼ˆMinkUNetè¾“å‡ºï¼‰
                 feat2d_dim: int = 256,     # 2Dç‰¹å¾ç»´åº¦ï¼ˆCLIPæŠ•å½±åï¼‰
                 output_dim: int = 128,     # æœ€ç»ˆè¾“å‡ºç»´åº¦
                 enable_debug: bool = False,
                 collect_gradient_stats: bool = True,
                 dropout: float = 0.1):
        super().__init__()
        
        self.feat3d_dim = feat3d_dim
        self.feat2d_dim = feat2d_dim
        self.output_dim = output_dim
        self.enable_debug = enable_debug
        self.collect_gradient_stats = collect_gradient_stats
        self.dropout = float(max(0.0, min(1.0, dropout)))
        
        # ä»¿ç…§3DMVçš„features3dï¼šå¤„ç†3Då‡ ä½•ç‰¹å¾ (96 â†’ 64ç»´)
        self.features3d = nn.Sequential(
            # ç¬¬ä¸€é˜¶æ®µï¼šç‰¹å¾æ‰©å±•å’Œç©ºé—´æ„ŸçŸ¥
            ME.MinkowskiConvolution(feat3d_dim, 64, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(64, momentum=0.02),
            ME.MinkowskiReLU(True),
            # 1x1x1ç²¾ç‚¼å·ç§¯ï¼šæå–æ›´æŠ½è±¡çš„ç‰¹å¾è¡¨ç¤º
            ME.MinkowskiConvolution(64, 64, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(64, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(self.dropout),
            
            # ç¬¬äºŒé˜¶æ®µï¼šä¿æŒ64ç»´ï¼Œè¿›ä¸€æ­¥ç‰¹å¾æŠ½è±¡
            ME.MinkowskiConvolution(64, 64, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(64, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiConvolution(64, 64, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(64, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(self.dropout)
        )
        
        # ä»¿ç…§3DMVçš„features2dï¼šå¤„ç†æŠ•å½±åçš„2Dç‰¹å¾ (256 â†’ 32ç»´)
        self.features2d = nn.Sequential(
            # ç¬¬ä¸€é˜¶æ®µï¼šç»´åº¦å‹ç¼© 256 â†’ 64
            ME.MinkowskiConvolution(feat2d_dim, 64, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(64, momentum=0.02),
            ME.MinkowskiReLU(True),
            # 1x1x1ç²¾ç‚¼å·ç§¯
            ME.MinkowskiConvolution(64, 64, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(64, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(self.dropout),
            
            # ç¬¬äºŒé˜¶æ®µï¼šè¿›ä¸€æ­¥å‹ç¼© 64 â†’ 32
            ME.MinkowskiConvolution(64, 32, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(32, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiConvolution(32, 32, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(32, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(self.dropout)
        )
        
        # 3D-only é˜¶æ®µçš„é€šé“æ‰©å±•ï¼šå°† 3D åˆ†æ”¯ 64 é€šé“æ‰©å±•åˆ° 96 é€šé“ï¼ˆhead64 + shadow32ï¼‰
        self.expand3d_64to96 = nn.Sequential(
            ME.MinkowskiConvolution(64, 96, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(96, momentum=0.02),
            ME.MinkowskiReLU(True)
        )

        # ä»¿ç…§3DMVçš„featuresï¼šå¤šæ¨¡æ€ç‰¹å¾èåˆ (96ç»´=64+32 â†’ 128ç»´)
        self.features_fusion = nn.Sequential(
            # èåˆé˜¶æ®µï¼šå¤„ç†concatenatedç‰¹å¾
            ME.MinkowskiConvolution(96, 128, kernel_size=3, stride=1, dimension=3),
            ME.MinkowskiBatchNorm(128, momentum=0.02),
            ME.MinkowskiReLU(True),
            # 1x1x1ç²¾ç‚¼å·ç§¯ï¼šæ·±å±‚ç‰¹å¾æŠ½è±¡
            ME.MinkowskiConvolution(128, 128, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(128, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiConvolution(128, output_dim, kernel_size=1, dimension=3),
            ME.MinkowskiBatchNorm(output_dim, momentum=0.02),
            ME.MinkowskiReLU(True),
            ME.MinkowskiDropout(self.dropout)
        )
        
        self._last_monitor = {}
        self._last_feats = None
        self._grad_feature_norms = {}
        self._prev_grad_stats = {}

        if self.enable_debug:
            print(f"ğŸ”§ åˆå§‹åŒ–Conv3DFusionModule: 3D({feat3d_dim}â†’64) + 2D({feat2d_dim}â†’32) â†’ èåˆ({96}â†’{output_dim})")

    def forward(self, feat3d_sparse: ME.SparseTensor, feat2d_sparse: ME.SparseTensor) -> ME.SparseTensor:
        """
        3Då·ç§¯èåˆå‰å‘ä¼ æ’­
        
        Args:
            feat3d_sparse: ME.SparseTensorï¼Œ3Dç‰¹å¾ (N, feat3d_dim)
            feat2d_sparse: ME.SparseTensorï¼Œ2Dç‰¹å¾ (N, feat2d_dim)
        Returns:
            fused_sparse: ME.SparseTensorï¼Œèåˆç‰¹å¾ (N, output_dim)
        """
        if self.enable_debug:
            print(f"ğŸ” Conv3Dèåˆè¾“å…¥: 3Dç‰¹å¾{feat3d_sparse.features.shape}, 2Dç‰¹å¾{feat2d_sparse.features.shape}")
        
        # åˆ†åˆ«å¤„ç†3Då’Œ2Dç‰¹å¾ï¼šæ¨¡ä»¿3DMVçš„åŒåˆ†æ”¯è®¾è®¡
        f3d_processed = self.features3d(feat3d_sparse)      # 96 â†’ 64ç»´
        f3d_feats = f3d_processed.features                  # (N, 64)

        # å°† 3D 64 é€šé“æ‰©å±•åˆ° 96 é€šé“ï¼ˆhead64 + shadow32ï¼‰
        proj96_sparse = self.expand3d_64to96(f3d_processed)  # (N, 96)
        proj96_feats = proj96_sparse.features
        head64 = proj96_feats[:, :64]
        shadow32 = proj96_feats[:, 64:]

        # è¯»å– alphaï¼ˆè‹¥æœªè®¾ç½®åˆ™è§†ä¸º 0.0ï¼‰
        alpha = float(getattr(self, 'alpha_for_blend', 0.0))

        # ä»…å½“ alpha > 0 æ—¶æ‰è®¡ç®— 2D åˆ†æ”¯ï¼Œé¿å… Phase A é¢å¤–å¼€é”€
        if alpha > 0.0:
            f2d_processed = self.features2d(feat2d_sparse)  # (N, 32)
            f2d_feats = f2d_processed.features
        else:
            f2d_processed = None
            f2d_feats = None

        if self.enable_debug:
            print(f"ğŸ” åˆ†æ”¯å¤„ç†å: 3Dç‰¹å¾{f3d_feats.shape}, 2Dç‰¹å¾{f2d_feats.shape}")

        monitor = {}
        with torch.no_grad():
            monitor['feat3d_mean_abs'] = f3d_feats.abs().mean().item()
            monitor['feat3d_std'] = f3d_feats.std().item()
            monitor['feat3d_nonzero_ratio'] = (f3d_feats.abs() > 1e-3).float().mean().item()

            if f2d_feats is not None:
                monitor['feat2d_mean_abs'] = f2d_feats.abs().mean().item()
                monitor['feat2d_std'] = f2d_feats.std().item()
                monitor['feat2d_nonzero_ratio'] = (f2d_feats.abs() > 1e-3).float().mean().item()
            else:
                monitor['feat2d_mean_abs'] = 0.0
                monitor['feat2d_std'] = 0.0
                monitor['feat2d_nonzero_ratio'] = 0.0

        if self.collect_gradient_stats:
            prev_norms = getattr(self, '_grad_feature_norms', None)
            self._prev_grad_stats = prev_norms.copy() if prev_norms else {}
            self._grad_feature_norms = {}
        else:
            self._prev_grad_stats = {}
        
        # ç‰¹å¾æ‹¼æ¥ï¼šåœ¨é€šé“ç»´åº¦concat (64+32=96ç»´)
        # æ•æ‰3Dåæ ‡é¡ºåºå¹¶å¯¹é½2Dç‰¹å¾ï¼ˆæˆ–ä½¿ç”¨ shadow32ï¼‰
        coord_manager = f3d_processed.coordinate_manager
        coords3d = f3d_processed.C.float()

        if alpha > 0.0 and f2d_processed is not None:
            try:
                # å°† 2D åˆ†æ”¯ç‰¹å¾æŒ‰ç…§ 3D æ´»è·ƒåæ ‡é¡ºåºå¯¹é½
                f2d_aligned = f2d_processed.features_at_coordinates(coords3d)
            except RuntimeError as err:
                if self.enable_debug:
                    print(f"âš ï¸ features_at_coordinates å¼‚å¸¸: {err}")
                f2d_aligned = f3d_processed.features.new_zeros(
                    f3d_processed.features.shape[0], 32)

            if not torch.isfinite(f2d_aligned).all():
                invalid_mask = ~torch.isfinite(f2d_aligned)
                if self.enable_debug:
                    invalid_count = invalid_mask.sum().item()
                    print(f"âš ï¸ å¯¹é½åçš„2Dç‰¹å¾å‡ºç°NaN/Infï¼Œå·²ç½®é›¶ï¼Œæ•°é‡: {invalid_count}")
                f2d_aligned = f2d_aligned.masked_fill(invalid_mask, 0)
        else:
            # Phase A æˆ– alpha=0ï¼šä¸ä½¿ç”¨ 2D åˆ†æ”¯
            f2d_aligned = None

        if self.collect_gradient_stats:
            def _capture(name):
                def hook(grad):
                    if grad is None:
                        return
                    if not hasattr(self, '_grad_feature_norms'):
                        self._grad_feature_norms = {}
                    with torch.no_grad():
                        self._grad_feature_norms[f'grad_norm_{name}'] = grad.detach().norm().item()
                return hook

            # ä»…åœ¨éœ€è¦æ¢¯åº¦æ—¶æ³¨å†Œhookï¼Œé¿å…åœ¨eval/æ— æ¢¯åº¦æ—¶æŠ›å‡ºå¼‚å¸¸
            if f3d_feats.requires_grad:
                f3d_feats.register_hook(_capture('feat3d'))
            if f2d_aligned is not None and f2d_aligned.requires_grad:
                f2d_aligned.register_hook(_capture('feat2d'))

        if self.collect_gradient_stats:
            def _capture(name):
                key = f'grad_norm_{name}_raw'

                def hook(grad):
                    if grad is None:
                        return
                    if not hasattr(self, '_grad_feature_norms'):
                        self._grad_feature_norms = {}
                    with torch.no_grad():
                        self._grad_feature_norms[key] = grad.detach().norm().item()
                return hook

            if f3d_feats.requires_grad:
                f3d_feats.register_hook(_capture('feat3d'))
            if f2d_aligned is not None and f2d_aligned.requires_grad:
                f2d_aligned.register_hook(_capture('feat2d'))

        # è®°å½•ç›‘æ§ä¿¡æ¯ï¼›å…·ä½“ç‰¹å¾å¿«ç…§åœ¨åç»­æ„å»º tail32 åç»Ÿä¸€å­˜å‚¨
        self._last_monitor = monitor

        # æ„é€  tail32ï¼šPhase A ä½¿ç”¨ shadow32ï¼›Phase B ä½¿ç”¨ shadow32 ä¸ f2d_aligned çš„çº¿æ€§æ··åˆ
        if f2d_aligned is None:
            tail32 = shadow32
        else:
            # ä¿è¯å½¢çŠ¶åŒ¹é… (N, 32)
            if f2d_aligned.shape[1] != 32:
                if self.enable_debug:
                    print(f"âš ï¸ f2d_aligned é€šé“ç»´ä¸ä¸º32ï¼Œå½“å‰ {f2d_aligned.shape[1]}ï¼Œå°†æˆªæ–­æˆ–è¡¥é›¶")
                if f2d_aligned.shape[1] > 32:
                    f2d_aligned = f2d_aligned[:, :32]
                else:
                    pad = f2d_aligned.new_zeros(f2d_aligned.size(0), 32 - f2d_aligned.size(1))
                    f2d_aligned = torch.cat([f2d_aligned, pad], dim=1)
            tail32 = (1.0 - alpha) * shadow32 + alpha * f2d_aligned

        manual_features = torch.cat([head64, tail32], dim=1)
        if self.collect_gradient_stats and manual_features.requires_grad:
            manual_features.register_hook(_capture('fusion'))
        fused_sparse = ME.SparseTensor(
            features=manual_features,
            coordinate_map_key=f3d_processed.coordinate_map_key,
            coordinate_manager=coord_manager
        )

        if self.enable_debug:
            print(f"ğŸ” æ‰‹åŠ¨ç‰¹å¾æ‹¼æ¥æˆåŠŸ: {fused_sparse.features.shape}")

        # æœ€ç»ˆèåˆå·ç§¯ï¼š96 â†’ output_dimç»´
        output_sparse = self.features_fusion(fused_sparse)

        self._last_monitor = monitor
        # è®°å½•èåˆå‰ç”¨äºç›¸ä¼¼åº¦çš„ç‰¹å¾ï¼ˆä¿æŒé”®åä¸å˜ï¼‰ã€‚è‹¥æ— 2Dï¼Œåˆ™ç”¨ shadow32 ä»£æ›¿ï¼Œç”¨äºä¸Šå±‚ç»Ÿè®¡ã€‚
        if f2d_feats is None:
            # ä¼ªé€ ä¸€ä¸ªä¸ tail32 åŒå½¢çš„ç‰¹å¾ä¾›ä¸Šå±‚å–ç”¨
            f2d_record = tail32.detach()
        else:
            f2d_record = f2d_feats.detach()
        self._last_feats = {'f3d_feats': f3d_feats, 'f2d_feats': f2d_record}

        if self.enable_debug:
            print(f"ğŸ” Conv3Dèåˆè¾“å‡º: {output_sparse.features.shape}")

        return output_sparse

@MODELS.register_module(name='BiFusionEncoder3DMV')
class BiFusionEncoder(nn.Module):
    """Enhanced Bi-Fusion Encoder combining 2D CLIP visual features and 3D Sparse features.
    
    ğŸ”¥ æ–°å¢3DMVå¼3Då·ç§¯èåˆæ”¯æŒï¼š
    
    æ¶æ„è®¾è®¡ï¼š
    - ä¼ ç»Ÿæ¨¡å¼ï¼šç‚¹çº§èåˆï¼ˆLiteFusionGateï¼‰
    - å¢å¼ºæ¨¡å¼ï¼š3Då·ç§¯èåˆï¼ˆConv3DFusionModuleï¼‰
    - æ··åˆæ¨¡å¼ï¼šä¸¤ç§èåˆæ–¹å¼ç»“åˆ
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    1. çº¯ç‚¹çº§èåˆï¼ˆé»˜è®¤ï¼‰ï¼š
       use_conv3d_fusion=False, fusion_mode="point_only"
       
    2. çº¯3Då·ç§¯èåˆï¼š
       use_conv3d_fusion=True, fusion_mode="conv3d_only"
       
    3. æ··åˆèåˆï¼š
       use_conv3d_fusion=True, fusion_mode="hybrid"
    
    æ ¸å¿ƒåŸç†ï¼š
    - 3Dåˆ†æ”¯ï¼šMinkUNet(96ç»´) â†’ Conv3Då¤„ç† â†’ 64ç»´
    - 2Dåˆ†æ”¯ï¼šCLIPç‰¹å¾(256ç»´) â†’ Conv3Då¤„ç† â†’ 32ç»´  
    - èåˆï¼šConcat(96ç»´) â†’ Conv3D â†’ æœ€ç»ˆç‰¹å¾
    
    ç›¸æ¯”ç‚¹çº§èåˆçš„ä¼˜åŠ¿ï¼š
    - ç©ºé—´ä¸€è‡´æ€§ï¼šåˆ©ç”¨3Då·ç§¯çš„ç©ºé—´é‚»åŸŸä¿¡æ¯
    - å±‚æ¬¡èåˆï¼šåœ¨å·ç§¯ç‰¹å¾å±‚çº§è¿›è¡Œèåˆï¼Œæ›´æ·±å…¥
    - ç«¯åˆ°ç«¯å­¦ä¹ ï¼šæ•´ä¸ªè¿‡ç¨‹å¯å¾®åˆ†ï¼Œæ”¯æŒæ¢¯åº¦åä¼ 
    """

    def __init__(self,
                 voxel_size: float = 0.02,
                 use_amp: bool = True,
                 # ğŸ¯ ç‰¹å¾åŸŸé…ç½®ï¼ˆç®€åŒ–ä¸ºä»…æ”¯æŒ60Ã—80é¢„è®¡ç®—ï¼‰
                 feat_space: str = "precomp_60x80",      # å›ºå®šä¸ºé¢„è®¡ç®—ç‰¹å¾
                 use_precomp_2d: bool = True,            # é»˜è®¤å¯ç”¨é¢„è®¡ç®—ç‰¹å¾
                 # ğŸ”¥ 3Då·ç§¯èåˆé…ç½®ï¼ˆä¸“é—¨ä½¿ç”¨Conv3Dï¼‰
                 conv3d_output_dim: int = 256,           # 3Då·ç§¯èåˆè¾“å‡ºç»´åº¦ï¼Œé»˜è®¤256ä¿æŒå…¼å®¹
                 conv3d_dropout: float = 0.1,            # 3Då·ç§¯èåˆä¸­çš„Dropoutæ¯”ä¾‹ï¼ˆå¯ä¸º0å…³é—­ï¼‰
                 # è°ƒè¯•æ¨¡å¼æ§åˆ¶
                 debug: bool = False,
                 collect_gradient_stats: bool = True,
                 freeze_2d_branch: bool = False,
                 **kwargs):  # æ¥æ”¶å…¶ä»–æœªçŸ¥å‚æ•°
        super().__init__()
        self.freeze_2d_branch = freeze_2d_branch
        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœvoxel_sizeæ˜¯å­—å…¸ï¼ˆconfigä¼ å…¥é”™è¯¯ï¼‰ï¼Œæå–æˆ–ä½¿ç”¨é»˜è®¤å€¼
        if isinstance(voxel_size, dict):
            print(f"âš ï¸ è­¦å‘Š: voxel_sizeä¼ å…¥äº†å­—å…¸ï¼Œä½¿ç”¨é»˜è®¤å€¼0.02")
            voxel_size = 0.02
        
        # ğŸ¯ ç‰¹å¾åŸŸé…ç½®
        self.feat_space = feat_space
        self.use_precomp_2d = use_precomp_2d
        self.debug = debug
        
        # ğŸ”¥ 3Då·ç§¯èåˆé…ç½®ï¼ˆä¸“é—¨ä½¿ç”¨Conv3Dï¼‰
        self.conv3d_output_dim = conv3d_output_dim

        # ğŸ¯ æ ¹æ®ç‰¹å¾åŸŸè®¾ç½®ï¼ˆç®€åŒ–ï¼Œåªæ”¯æŒ60Ã—80é¢„è®¡ç®—ï¼‰
        if feat_space != "precomp_60x80":
            print(f"è­¦å‘Š: å½“å‰ä»…æ”¯æŒprecomp_60x80ç‰¹å¾åŸŸï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°precomp_60x80")
            feat_space = "precomp_60x80"
        
        # åˆ é™¤Enhanced CLIPç¼–ç å™¨ï¼ˆä¸å†éœ€è¦ï¼‰
        # self.enhanced_clip = None
        
        # 3D encoder - ä¿æŒåŸå§‹96ç»´ä»¥å…¼å®¹é¢„è®­ç»ƒæƒé‡
        cfg_backbone = SimpleNamespace(dilations=[1, 1, 1, 1], bn_momentum=0.02, conv1_kernel_size=5)
        self.backbone3d = Res16UNet34C(in_channels=3, out_channels=96, config=cfg_backbone, D=3)
        
        # ğŸ”¥ 3Då·ç§¯èåˆæ¨¡å—ï¼šä¸“é—¨ä½¿ç”¨Conv3Dèåˆ
        self.alpha_2d = 0.0
        self.conv3d_fusion = Conv3DFusionModule(
            feat3d_dim=96,          # MinkUNetè¾“å‡ºç»´åº¦
            feat2d_dim=256,         # 2Dç‰¹å¾ç»´åº¦ï¼ˆé€‚é…åï¼‰
            output_dim=self.conv3d_output_dim,  # å¯é…ç½®è¾“å‡ºç»´åº¦
            enable_debug=self.debug,
            collect_gradient_stats=collect_gradient_stats,
            dropout=float(max(0.0, min(1.0, conv3d_dropout)))
        )
        self.align_dim = 64
        self.cos_proj3d = nn.Sequential(
            nn.Linear(64, self.align_dim),
            nn.LayerNorm(self.align_dim)
        )
        self.cos_proj2d = nn.Sequential(
            nn.Linear(32, self.align_dim),
            nn.LayerNorm(self.align_dim)
        )
        if self.debug:
            print(f"ğŸ”§ åˆå§‹åŒ–3Då·ç§¯èåˆæ¨¡å—: è¾“å‡ºç»´åº¦={self.conv3d_output_dim}")
        
        # ğŸ¯ é¢„è®¡ç®—ç‰¹å¾é€‚é…å™¨ï¼ˆæƒ°æ€§åˆå§‹åŒ–ï¼‰
        self.precomp_adapter = None
        
        # ğŸ¯ Alphaå›é€€å€¼ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰
        
        # ğŸ¯ æŸå¤±å†å²è®°å½•ï¼ˆç”¨äºæŠ–åŠ¨åˆ†æï¼‰
        self._loss_hist = deque(maxlen=100)

        # åŸºæœ¬è¿è¡Œ/è°ƒè¯•å¼€å…³å’Œç»Ÿè®¡ç»“æ„
        self.voxel_size = voxel_size
        self.use_amp = use_amp
        self.standard_scannet_intrinsics = (577.870605, 577.870605, 319.5, 239.5)
        self.align_corners = True  # ä¸æŠ•å½±é‡‡æ ·ä¿æŒä¸€è‡´
        self.max_depth = 20.0
        self._collect_fusion_stats = True
        self._collect_gradient_stats = collect_gradient_stats  # æ¢¯åº¦ç»Ÿè®¡è¾“å‡ºç‹¬ç«‹äºdebug
        self._fusion_stats = {}
        self._stats_history = []

        self._param_grad_sums = defaultdict(float)
        self._param_grad_groups = {}
        self._registered_param_ids = set()
        self._last_param_grad_norms = {}

        # ğŸ”¥ è¾“å‡ºé…ç½®ä¿¡æ¯
        self._print_config_summary()

        if self._collect_gradient_stats:
            self._register_grad_param_hooks()

        if self.freeze_2d_branch:
            self._freeze_2d_parameters()

    def _freeze_2d_parameters(self):
        """Freeze 2D projection branch during Phase A."""
        modules_to_freeze = []
        if hasattr(self.conv3d_fusion, 'features2d'):
            modules_to_freeze.append(self.conv3d_fusion.features2d)
        modules_to_freeze.append(self.cos_proj2d)
        for module in modules_to_freeze:
            module.eval()
            for param in module.parameters():
                param.requires_grad = False
        # features_fusion ä¹Ÿä¼šæ¥æ”¶åˆ°2Dæ”¯è·¯è¾“å‡ºï¼Œæ­¤å¤„ä¸å†»ç»“ä»¥ä¿ç•™å­¦ä¹ èƒ½åŠ›ã€‚

    def train(self, mode: bool = True):
        super().train(mode)
        if self.freeze_2d_branch and mode:
            # å†æ¬¡æ–½åŠ å†»ç»“ï¼Œé¿å…å¤–éƒ¨ train() è°ƒç”¨æ¢å¤ 2D åˆ†æ”¯ BN/æƒé‡
            self._freeze_2d_parameters()
        return self
    
    def _print_config_summary(self):
        """æ‰“å°å½“å‰é…ç½®æ‘˜è¦"""
        print("=" * 60)
        print("ğŸ”¥ BiFusionEncoderé…ç½®æ‘˜è¦ - ä¸“ç”¨3Då·ç§¯èåˆç‰ˆæœ¬")
        print("=" * 60)
        print(f"ç‰¹å¾åŸŸ: {self.feat_space}")
        print(f"ä½¿ç”¨é¢„è®¡ç®—2Dç‰¹å¾: {self.use_precomp_2d}")
        print(f"ä½“ç´ å¤§å°: {self.voxel_size}")
        print(f"è°ƒè¯•æ¨¡å¼: {self.debug}")
        print("-" * 40)
        print("ğŸ¯ èåˆé…ç½®:")
        print(f"  èåˆæ¨¡å¼: ä¸“ç”¨3Då·ç§¯èåˆ")
        print(f"  3Då·ç§¯è¾“å‡ºç»´åº¦: {self.conv3d_output_dim}")
        print(f"  3Då·ç§¯æ¨¡å—: {'å·²åˆå§‹åŒ–' if self.conv3d_fusion is not None else 'æœªåˆå§‹åŒ–'}")
        print(f"  æ¢¯åº¦ç›‘æ§: {'å¯ç”¨' if self._collect_gradient_stats else 'å…³é—­'}")
        print("-" * 40)
        print("ğŸ“Š æ¶æ„è¯´æ˜:")
        print("  æ¨¡å¼: 3DMVå¼3Då·ç§¯èåˆ")  
        print("  ç‰¹ç‚¹: ç©ºé—´ä¸€è‡´æ€§å¼ºï¼Œåˆ©ç”¨é‚»åŸŸä¿¡æ¯ï¼Œç«¯åˆ°ç«¯å­¦ä¹ ")
        print("  æµç¨‹: 3D(96ç»´)â†’64ç»´ + 2D(256ç»´)â†’32ç»´ â†’ Concat(96ç»´) â†’ èåˆè¾“å‡º")
        print("=" * 60)
    
    @classmethod
    def create_conv3d_config(cls, **kwargs):
        """åˆ›å»º3Då·ç§¯èåˆé…ç½®çš„ä¾¿æ·æ–¹æ³•
        
        ç¤ºä¾‹:
        # é»˜è®¤é…ç½®ï¼ˆ256ç»´è¾“å‡ºï¼‰
        encoder = BiFusionEncoder.create_conv3d_config()
        
        # è‡ªå®šä¹‰è¾“å‡ºç»´åº¦
        encoder = BiFusionEncoder.create_conv3d_config(
            conv3d_output_dim=128,
            debug=True
        )
        """
        default_config = {
            'conv3d_output_dim': 256,
            'debug': False,
            'collect_gradient_stats': True
        }
        default_config.update(kwargs)
        return cls(**default_config)

    def _create_sparse_tensor_from_features(self, 
                                             features: torch.Tensor, 
                                             coordinates: torch.Tensor,
                                             coord_manager=None) -> ME.SparseTensor:
        """
        å°†ç‰¹å¾å’Œåæ ‡è½¬æ¢ä¸ºMinkowskiEngineç¨€ç–å¼ é‡
        
        Args:
            features: (N, C) ç‰¹å¾å¼ é‡
            coordinates: (N, 3) åæ ‡å¼ é‡ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
            coord_manager: åæ ‡ç®¡ç†å™¨ï¼Œç”¨äºç¡®ä¿ç¨€ç–å¼ é‡å…¼å®¹æ€§
        Returns:
            ME.SparseTensor: ç¨€ç–å¼ é‡
        """
        # åæ ‡é‡åŒ–ï¼šä¸–ç•Œåæ ‡ â†’ ä½“ç´ åæ ‡
        coords_int = torch.round(coordinates / self.voxel_size).to(torch.int32)
        
        # æ·»åŠ batchç»´åº¦ï¼š(N, 3) â†’ (N, 4)ï¼Œç¬¬ä¸€åˆ—ä¸ºbatch_index=0
        coords_with_batch = torch.cat([
            torch.zeros(coords_int.size(0), 1, dtype=torch.int32, device=coords_int.device),
            coords_int
        ], dim=1)
        
        sparse_kwargs = {
            'features': features.float(),
            'coordinates': coords_with_batch,
            'device': features.device
        }
        if coord_manager is not None:
            sparse_kwargs['coordinate_manager'] = coord_manager

        sparse_tensor = ME.SparseTensor(**sparse_kwargs)
        
        if self.debug:
            print(f"ğŸ”§ åˆ›å»ºç¨€ç–å¼ é‡: ç‰¹å¾{features.shape} â†’ åæ ‡{coords_with_batch.shape}")
        
        return sparse_tensor
    
    def _convert_2d_features_to_sparse(self, 
                                       feat2d: torch.Tensor, 
                                       xyz_world: torch.Tensor,
                                       valid_mask: torch.Tensor,
                                       reference_sparse: ME.SparseTensor) -> ME.SparseTensor:
        """å°† 2D ç‰¹å¾é‡æ–°æ’åˆ—ä¸ºä¸ 3D ç¨€ç–å¼ é‡ä¸€è‡´çš„åæ ‡ã€‚"""
        feat2d_filled = feat2d.clone()
        if feat2d_filled.numel() > 0:
            feat2d_filled[~valid_mask] = 0

        coords_ref = reference_sparse.C  # (M, 4)
        device = feat2d_filled.device
        feature_dim = feat2d_filled.shape[1]

        ordered_features = feat2d_filled.new_zeros((coords_ref.shape[0], feature_dim))
        hit_counts = feat2d_filled.new_zeros((coords_ref.shape[0],), dtype=feat2d_filled.dtype)

        coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_ref.cpu().tolist())}
        coords_full = torch.cat([
            torch.zeros(xyz_world.size(0), 1, dtype=torch.int32, device=xyz_world.device),
            torch.round(xyz_world / self.voxel_size).to(torch.int32)
        ], dim=1)

        coords_full_list = coords_full.cpu().tolist()
        idx_list = [coord_to_idx.get(tuple(coord), -1) for coord in coords_full_list]
        idx_tensor = torch.tensor(idx_list, device=device, dtype=torch.long)

        valid_idx_mask = idx_tensor >= 0
        if not valid_idx_mask.all():
            missing = (~valid_idx_mask).sum().item()
            if missing > 0:
                warnings.warn(f"{missing} points not found in sparse coordinate map; skipped.", stacklevel=2)

        if valid_idx_mask.any():
            idx_tensor_valid = idx_tensor[valid_idx_mask]
            feat_selected = feat2d_filled[valid_idx_mask]
            ordered_features.index_add_(0, idx_tensor_valid, feat_selected)
            hit_counts.index_add_(
                0,
                idx_tensor_valid,
                torch.ones(idx_tensor_valid.shape[0], device=device, dtype=hit_counts.dtype)
            )

        ordered_features = ordered_features / hit_counts.clamp_min(1.0).unsqueeze(-1)

        return ME.SparseTensor(
            features=ordered_features.float(),
            coordinate_manager=reference_sparse.coordinate_manager,
            coordinate_map_key=reference_sparse.coordinate_map_key
        )
    
    def _extract_features_from_sparse(self, 
                                      sparse_tensor: ME.SparseTensor, 
                                      target_coordinates: torch.Tensor,
                                      target_size: int) -> torch.Tensor:
        """
        ä»ç¨€ç–å¼ é‡ä¸­æå–ç›®æ ‡åæ ‡å¯¹åº”çš„ç‰¹å¾
        
        Args:
            sparse_tensor: ME.SparseTensor è¾“å…¥ç¨€ç–å¼ é‡
            target_coordinates: (N, 3) ç›®æ ‡åæ ‡ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
            target_size: int ç›®æ ‡ç‰¹å¾æ•°é‡
        Returns:
            torch.Tensor: (target_size, C) æå–çš„ç‰¹å¾
        """
        # é‡åŒ–ç›®æ ‡åæ ‡
        target_coords_int = torch.round(target_coordinates / self.voxel_size).to(torch.int32)
        target_coords_with_batch = torch.cat([
            torch.zeros(target_coords_int.size(0), 1, dtype=torch.int32, device=target_coords_int.device),
            target_coords_int
        ], dim=1)
        
        # ä½¿ç”¨features_at_coordsæ–¹æ³•æå–ç‰¹å¾
        try:
            extracted = sparse_tensor.features_at_coordinates(target_coords_with_batch.float())
        except Exception as err:
            warnings.warn(f"Failed to look up sparse features: {err}", stacklevel=2)
            extracted = torch.zeros(
                target_size,
                sparse_tensor.features.shape[1],
                device=sparse_tensor.device,
                dtype=sparse_tensor.features.dtype
            )
            return extracted
        
        if extracted.shape[0] != target_size:
            padded = torch.zeros(
                target_size,
                sparse_tensor.features.shape[1],
                device=sparse_tensor.device,
                dtype=sparse_tensor.features.dtype
            )
            copy_len = min(extracted.shape[0], target_size)
            padded[:copy_len] = extracted[:copy_len]
            extracted = padded
        
        return extracted

    def get_pose_pick_stats(self):
        """ä¿ç•™æ¥å£ï¼Œå½“å‰å®ç°ä¸ç»Ÿè®¡è¯¥ä¿¡æ¯ã€‚"""
        return {}

    def reset_pose_pick_stats(self):
        """ä¿ç•™æ¥å£ï¼Œæ— éœ€æ‰§è¡Œé¢å¤–æ“ä½œã€‚"""
        return None
    
    #ï¼Ÿï¼Ÿ
    def _ensure_precomp_adapter(self, c_in: int):
        """æƒ°æ€§åˆå§‹åŒ–é¢„è®¡ç®—ç‰¹å¾é€‚é…å™¨ï¼š512 â†’ 256"""
        if (self.precomp_adapter is None) or (self.precomp_adapter[0].in_features != c_in):
            # æŒ‰ç…§ä¼˜åŒ–æŒ‡å—è¦æ±‚ï¼šLinear(512â†’256) + LayerNorm
            self.precomp_adapter = nn.Sequential(
                nn.Linear(c_in, 256),
                nn.LayerNorm(256)
            ).to(next(self.parameters()).device)
            if self.debug:
                print(f"ğŸ”§ åˆå§‹åŒ–é¢„è®¡ç®—é€‚é…å™¨: {c_in} â†’ 256 (ä¼˜åŒ–ç‰ˆæœ¬)")
            if self._collect_gradient_stats:
                self._register_params_to_group('feat2d', self.precomp_adapter.parameters())

    # ------------------------------------------------------------------
    # æ¢¯åº¦ç›‘æ§è¾…åŠ©å‡½æ•°
    # ------------------------------------------------------------------
    def _make_param_hook(self, group_key: str):
        def _hook(grad: torch.Tensor):
            if grad is None or not self._collect_gradient_stats:
                return
            self._param_grad_sums[group_key] += grad.detach().pow(2).sum().item()

        return _hook

    def _register_params_to_group(self, group_key: str, params):
        if not self._collect_gradient_stats:
            return

        group_list = self._param_grad_groups.setdefault(group_key, [])
        for param in params:
            if (param is None) or (not getattr(param, 'requires_grad', False)):
                continue
            param_id = id(param)
            if param_id in self._registered_param_ids:
                continue
            group_list.append(param)
            param.register_hook(self._make_param_hook(group_key))
            self._registered_param_ids.add(param_id)

    def _register_grad_param_hooks(self):
        # åŸºç¡€åˆ†æ”¯
        self._register_params_to_group('feat3d', list(self.backbone3d.parameters()))
        self._register_params_to_group('feat3d', list(self.conv3d_fusion.features3d.parameters()))
        self._register_params_to_group('feat2d', list(self.conv3d_fusion.features2d.parameters()))
        self._register_params_to_group('fusion', list(self.conv3d_fusion.features_fusion.parameters()))

        # è§£ç å™¨
        if hasattr(self, 'decoder') and self.decoder is not None:
            self._register_params_to_group('decoder', list(self.decoder.parameters()))

    def _pop_param_grad_norms(self) -> Dict[str, float]:
        if not self._collect_gradient_stats:
            return {}

        norms = {}
        for group_key, sq_sum in self._param_grad_sums.items():
            norms[f'grad_params_{group_key}'] = sq_sum ** 0.5 if sq_sum > 0 else 0.0

        self._param_grad_sums = defaultdict(float)
        return norms
    
    def update_loss_stat(self, loss_val: float):
        """æ›´æ–°æŸå¤±å†å²è®°å½•"""
        self._loss_hist.append(float(loss_val))
    
    def get_loss_var(self):
        """è·å–æŸå¤±æ»‘çª—æ–¹å·®"""
        if len(self._loss_hist) < 20:
            return None
        arr = torch.tensor(list(self._loss_hist))
        return float(arr.var(unbiased=False))
    
    # ç®€åŒ–çš„ç»Ÿè®¡æ–¹æ³•å·²é›†æˆåœ¨_process_singleä¸­
    
    def get_fusion_statistics(self):
        """è·å–èåˆç»Ÿè®¡ä¿¡æ¯"""
        return self._fusion_stats.copy() if self._fusion_stats else {}
    
    def get_fusion_ratios(self):
        """ä¸“é—¨è·å–èåˆæ¯”ä¾‹ç»Ÿè®¡ - ä¾›Hookä½¿ç”¨"""
        if not self._fusion_stats:
            return {}

        keys = [
            'avg_confidence',
            'valid_ratio',
            'norm_ratio_2d_over_3d',
            'cos_2d3d_mean',
            'cos_2d3d_mean_ln'
        ]
        return {k: self._fusion_stats.get(k, 0.0) for k in keys if k in self._fusion_stats}
    
    # èåˆå¹³è¡¡æŸå¤±ç›¸å…³æ–¹æ³•å·²åˆ é™¤ - ä¸“ç”¨Conv3Dä¸éœ€è¦
    
    def get_statistics_summary(self, last_n: int = 10):
        """è·å–æœ€è¿‘Næ¬¡çš„ç»Ÿè®¡æ‘˜è¦"""
        if not self._stats_history:
            return {}
            
        recent_stats = self._stats_history[-last_n:]
        summary = {}
        
        for key in recent_stats[0].keys():
            if key != 'total_points':
                values = [stats[key] for stats in recent_stats if key in stats]
                if values:
                    summary[f'{key}_mean'] = sum(values) / len(values)
                    summary[f'{key}_std'] = (sum((x - summary[f'{key}_mean'])**2 for x in values) / len(values))**0.5
        
        return summary

    # åˆ é™¤äº†å¤æ‚çš„ _improved_projection_with_geometry å‡½æ•°ï¼Œ
    # ç»Ÿä¸€ä½¿ç”¨ unified_projection_and_sample

    def _extract_pose_matrix(self, cam_meta: Dict, sample_idx: int = 0):
        """ä» cam_info ä¸­æå–å•å¸§ pose çŸ©é˜µï¼ˆcam2worldï¼‰ã€‚"""
        # !!!!!
        if not isinstance(cam_meta, dict):
            return None

        pose = cam_meta.get('pose')
        if pose is None:
            return None

        if isinstance(pose, (list, tuple)):
            poses = [p for p in pose if p is not None]
            if not poses:
                return None
            index = min(sample_idx, len(poses) - 1)
            pose = poses[index]

        if isinstance(pose, torch.Tensor):
            return pose.to(dtype=torch.float32)
        if isinstance(pose, np.ndarray):
            return torch.from_numpy(pose).float()

        warnings.warn(f"Unsupported pose type {type(pose)}; ignoring pose.", stacklevel=2)
        return None

    def _transform_coordinates(self, xyz: torch.Tensor, pose_matrix: Optional[torch.Tensor]) -> Optional[torch.Tensor]:
        """å°†ä¸–ç•Œåæ ‡ç³»çš„ç‚¹è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»ã€‚"""
        if pose_matrix is None:
            return None

        if pose_matrix.shape != (4, 4):
            warnings.warn(f"Unexpected pose shape {pose_matrix.shape}; ignoring pose.", stacklevel=2)
            return None

        pose = pose_matrix.to(device=xyz.device, dtype=xyz.dtype)
        if not torch.isfinite(pose).all():
            warnings.warn("Pose matrix contains NaN/Inf values; ignoring pose.", stacklevel=2)
            return None

        try:
            w2c = torch.inverse(pose)
        except RuntimeError as err:
            warnings.warn(f"Pose inversion failed: {err}; ignoring pose.", stacklevel=2)
            return None

        homo = torch.ones((xyz.shape[0], 1), device=xyz.device, dtype=xyz.dtype)
        xyz_cam = torch.cat([xyz, homo], dim=1) @ w2c.t()
        xyz_cam = xyz_cam[:, :3]

        if not torch.isfinite(xyz_cam).all():
            warnings.warn("Projected camera coordinates contain NaN/Inf; ignoring pose.", stacklevel=2)
            return None

        positive_depth_ratio = (xyz_cam[:, 2] > 0).float().mean().item()
        if positive_depth_ratio < 0.1:
            warnings.warn(f"Too few points with positive depth ({positive_depth_ratio:.3f}); ignoring pose.", stacklevel=2)
            return None

        return xyz_cam

    def _process_single(self, points: torch.Tensor, img: List[torch.Tensor], cam_meta: Dict, sample_idx: int = 0):
        """å¤„ç†å•å¸§ 2D-3D èåˆæµç¨‹ã€‚"""
        xyz = points[:, :3].contiguous()
        dev = xyz.device
        proj3d_points = None
        proj2d_points = None

        pose_matrix = self._extract_pose_matrix(cam_meta, sample_idx=sample_idx)
        xyz_cam_proj = self._transform_coordinates(xyz, pose_matrix)

        coords_int = torch.round(xyz / self.voxel_size).to(torch.int32)
        coords = torch.cat(
            [torch.zeros(coords_int.size(0), 1, dtype=torch.int32, device=coords_int.device), coords_int],
            dim=1)
        feats = points[:, 3:6].contiguous()
        field = ME.TensorField(coordinates=coords, features=feats)
        feat3d_sparse = self.backbone3d(field.sparse())

        clip_data = cam_meta.get('clip_pix') if isinstance(cam_meta, dict) else None
        if isinstance(clip_data, (list, tuple)):
            clip_candidates = [c for c in clip_data if c is not None]
            if clip_candidates:
                clip_data = clip_candidates[min(sample_idx, len(clip_candidates) - 1)]
            else:
                clip_data = None

        if xyz_cam_proj is None or clip_data is None:
            print("Missing xyz_cam_proj or clip_data; falling back to zero 2D features.")
            if clip_data is None:
                warnings.warn("Missing clip_pix feature; falling back to zero 2D features.", stacklevel=2)
            feat2d_raw = torch.zeros((points.shape[0], 256), device=dev, dtype=torch.float32)
            valid = torch.zeros(points.shape[0], device=dev, dtype=torch.bool)
        else:
            # !!!!
            if isinstance(clip_data, torch.Tensor):
                feat_map = clip_data.to(device=dev, dtype=torch.float32)
            elif isinstance(clip_data, np.ndarray):
                feat_map = torch.from_numpy(clip_data).to(device=dev, dtype=torch.float32)
            else:
                warnings.warn(f"Unsupported clip_pix type {type(clip_data)}; using zero features.", stacklevel=2)
                feat_map = None

            if feat_map is None:
                feat2d_raw = torch.zeros((points.shape[0], 256), device=dev, dtype=torch.float32)
                valid = torch.zeros(points.shape[0], device=dev, dtype=torch.bool)
            else:
                feat2d_raw, valid = unified_projection_and_sample(
                    xyz_cam=xyz_cam_proj,
                    feat_map=feat_map.unsqueeze(0),
                    max_depth=self.max_depth,
                    align_corners=self.align_corners,
                    standard_intrinsics=self.standard_scannet_intrinsics,
                    debug=self.debug,
                    debug_prefix=f'[BiFusion3DMV] sample={sample_idx}'
                )
                # æŠ•å½±æœ‰æ•ˆç‡è¿‡ä½å‘Šè­¦ï¼ˆå¯èƒ½ç”±ä½å§¿/å†…å‚ä¸åˆ†è¾¨ç‡é”™é…å¼•èµ·ï¼‰
                try:
                    valid_ratio_local = float(valid.float().mean().item())
                    if valid_ratio_local < 0.1:
                        warnings.warn(
                            f"Low projection valid ratio: {valid_ratio_local:.3f} (sample={sample_idx})."
                            " Check pose/intrinsics/resolution consistency.",
                            stacklevel=2)
                except Exception:
                    pass
                if feat2d_raw.shape[-1] != 256:
                    self._ensure_precomp_adapter(feat2d_raw.shape[-1])
                    feat2d_raw = self.precomp_adapter(feat2d_raw) if self.precomp_adapter else feat2d_raw

        try:
            feat2d_sparse = self._convert_2d_features_to_sparse(
                feat2d_raw,
                xyz,
                valid,
                reference_sparse=feat3d_sparse
            )

            if self.alpha_2d < 1.0:
                scale = float(max(0.0, min(1.0, self.alpha_2d)))
                if scale == 0.0:
                    scaled = feat2d_sparse.features.new_zeros(feat2d_sparse.features.shape)
                else:
                    scaled = feat2d_sparse.features * scale
                feat2d_sparse = ME.SparseTensor(
                    features=scaled,
                    coordinate_map_key=feat2d_sparse.coordinate_map_key,
                    coordinate_manager=feat2d_sparse.coordinate_manager,
                    tensor_stride=feat2d_sparse.tensor_stride
                )

            cos_mean = 0.0
            try:
                feat3d_base = feat3d_sparse.features
                feat2d_base = feat2d_sparse.features
                min_dim = min(feat3d_base.shape[1], feat2d_base.shape[1])
                if min_dim > 0:
                    cos_mean = float(F.cosine_similarity(
                        F.normalize(feat3d_base[:, :min_dim], dim=1),
                        F.normalize(feat2d_base[:, :min_dim], dim=1),
                        dim=1).mean().item())
            except Exception as err:
                warnings.warn(f"Failed to compute feature similarity: {err}", stacklevel=2)

            # å°† alpha ä¼ é€’ç»™èåˆæ¨¡å—ï¼Œç”¨äº Phase A/B ä¸‹çš„ tail32 æ„é€ ç­–ç•¥
            try:
                self.conv3d_fusion.alpha_for_blend = float(self.alpha_2d)
            except Exception:
                pass
            fused_sparse = self.conv3d_fusion(feat3d_sparse, feat2d_sparse)

            monitor_stats = getattr(self.conv3d_fusion, '_last_monitor', {}).copy()
            feat_dict = getattr(self.conv3d_fusion, '_last_feats', None)
            if feat_dict is not None:
                proj3d_points = self.cos_proj3d(feat_dict['f3d_feats'])
                proj2d_points = self.cos_proj2d(feat_dict['f2d_feats'])
                with torch.no_grad():
                    proj3d_ln = F.layer_norm(proj3d_points.detach(), proj3d_points.shape[-1:])
                    proj2d_ln = F.layer_norm(proj2d_points.detach(), proj2d_points.shape[-1:])
                    monitor_stats['cos_2d3d_mean_ln'] = F.cosine_similarity(proj3d_ln, proj2d_ln, dim=1).mean().item()
            else:
                monitor_stats = monitor_stats or {}

            if self._collect_gradient_stats:
                grad_stats = getattr(self.conv3d_fusion, '_prev_grad_stats', None)
                if grad_stats:
                    monitor_stats.update(grad_stats)
                if getattr(self, '_last_param_grad_norms', None):
                    monitor_stats.update(self._last_param_grad_norms)
                    g2d = self._last_param_grad_norms.get('grad_params_feat2d', 0.0)
                    g3d = self._last_param_grad_norms.get('grad_params_feat3d', 0.0)
                    monitor_stats['grad_ratio_2d_over_3d'] = g2d / (g3d + 1e-12)
            self.conv3d_fusion._last_feats = None

            fused = self._extract_features_from_sparse(fused_sparse, xyz, points.shape[0])

            if fused.shape[-1] != self.conv3d_output_dim:
                if fused.shape[-1] < self.conv3d_output_dim:
                    padding = torch.zeros(
                        fused.shape[0],
                        self.conv3d_output_dim - fused.shape[-1],
                        device=fused.device,
                        dtype=fused.dtype
                    )
                    fused = torch.cat([fused, padding], dim=-1)
                else:
                    fused = fused[:, :self.conv3d_output_dim]

            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼šè‹¥å‡ºç° NaN/Infï¼Œå‘Šè­¦å¹¶ç”¨3D-onlyå›é€€
            if not torch.isfinite(fused).all():
                warnings.warn("Fused features contain NaN/Inf; falling back to 3D-only features for this sample.", stacklevel=2)
                fallback_3d = self._extract_features_from_sparse(feat3d_sparse, xyz, points.shape[0])
                if fallback_3d.shape[-1] != self.conv3d_output_dim:
                    if fallback_3d.shape[-1] < self.conv3d_output_dim:
                        padding = torch.zeros(
                            fallback_3d.shape[0],
                            self.conv3d_output_dim - fallback_3d.shape[-1],
                            device=fallback_3d.device,
                            dtype=fallback_3d.dtype
                        )
                        fused = torch.cat([fallback_3d, padding], dim=-1)
                    else:
                        fused = fallback_3d[:, :self.conv3d_output_dim]
                else:
                    fused = fallback_3d

            valid_ratio = valid.float().mean().item()
            conf_value = max(0.3, min(0.9, valid_ratio))
            conf = torch.full((points.shape[0], 1), conf_value, device=dev, dtype=torch.float32)

            if self.debug:
                print(f"[BiFusion3DMV] sample={sample_idx} valid_ratio={valid_ratio:.3f} cos_mean={cos_mean:.3f}")

        except Exception as e:
            warnings.warn(f"Conv3D fusion failed; using 3D-only fallback. Details: {e}", stacklevel=2)
            # ç”¨3Dä¸»å¹²ç‰¹å¾å›é€€ï¼Œé¿å…å°†å…¨é›¶ç‰¹å¾é€å…¥è§£ç å™¨å¯¼è‡´é¢„æµ‹é€€åŒ–
            fallback_3d = self._extract_features_from_sparse(feat3d_sparse, xyz, points.shape[0])
            # è°ƒæ•´ç»´åº¦è‡³ conv3d_output_dim
            if fallback_3d.shape[-1] != self.conv3d_output_dim:
                if fallback_3d.shape[-1] < self.conv3d_output_dim:
                    padding = torch.zeros(
                        fallback_3d.shape[0],
                        self.conv3d_output_dim - fallback_3d.shape[-1],
                        device=fallback_3d.device,
                        dtype=fallback_3d.dtype
                    )
                    fused = torch.cat([fallback_3d, padding], dim=-1)
                else:
                    fused = fallback_3d[:, :self.conv3d_output_dim]
            else:
                fused = fallback_3d
            conf = torch.full((points.shape[0], 1), 0.5, device=dev, dtype=torch.float32)
            # å…³é”®ï¼šå½“éœ€è¦æ„é€ ç¨€ç–å¼ é‡æ—¶ï¼Œç‰¹å¾è¡Œæ•°å¿…é¡»ä¸æ´»è·ƒç‚¹æ•°ä¸€è‡´
            n_active_fallback = int(feat3d_sparse.features.shape[0])
            proj3d_points = torch.zeros((n_active_fallback, self.align_dim), device=dev, dtype=torch.float32)
            proj2d_points = torch.zeros((n_active_fallback, self.align_dim), device=dev, dtype=torch.float32)
            self.conv3d_fusion._last_monitor = {}
            self.conv3d_fusion._last_feats = None
            self.conv3d_fusion._prev_grad_stats = {}
            if hasattr(self.conv3d_fusion, '_grad_feature_norms'):
                self.conv3d_fusion._grad_feature_norms = {}
            monitor_stats = {}
            self._last_param_grad_norms = {}

        # è®°å½•èåˆç‰¹å¾åŸå§‹å¹…å€¼ï¼Œä¾¿äºç›‘æ§
        fused_pre_norm = fused.detach()

        # ç®€åŒ–çš„ç»Ÿè®¡ä¿¡æ¯æ”¶é›†ï¼ˆåŒæ—¶è®°å½• pre-gate ä¸ post-gate æŒ‡æ ‡ï¼Œpost-gate èƒ½åæ˜  Î± å¯¹2Dåˆ†æ”¯çš„å®é™…æŠ‘åˆ¶ç¨‹åº¦ï¼‰
        if self._collect_fusion_stats:
            try:
                valid_ratio = valid.float().mean().item()
                feat2d_norm = feat2d_raw.norm(dim=-1).clamp_min(1e-6).mean().item()
                
                feat3d_norm = feat3d_sparse.features.norm(dim=-1).clamp_min(1e-6).mean().item()
                norm_ratio = feat2d_norm / max(feat3d_norm, 1e-6)

                # è®°å½•post-gateï¼ˆç»è¿‡ Î± é—¨æ§åçš„ï¼‰2DèŒƒæ•°ä¸æ¯”å€¼
                try:
                    feat2d_post = feat2d_sparse.features
                    feat2d_norm_post = feat2d_post.norm(dim=-1).clamp_min(1e-6).mean().item()
                    norm_ratio_post = feat2d_norm_post / max(feat3d_norm, 1e-6)
                except Exception:
                    feat2d_norm_post = 0.0
                    norm_ratio_post = 0.0

                with torch.no_grad():
                    monitor_stats['fused_mean_abs_raw'] = fused_pre_norm.abs().mean().item()
                    monitor_stats['fused_std_raw'] = fused_pre_norm.std().item()
                    monitor_stats['fused_norm_mean_raw'] = fused_pre_norm.norm(dim=-1).mean().item()

                self._fusion_stats = {
                    'valid_ratio': valid_ratio,
                    'valid_points_ratio': valid_ratio,
                    'avg_confidence': conf_value,
                    'norm_ratio_2d_over_3d': norm_ratio,
                    'norm_ratio_2d_over_3d_post': norm_ratio_post,
                    'cos_2d3d_mean': cos_mean,
                    'norm_2d_mean': feat2d_norm,
                    'norm_2d_mean_post': feat2d_norm_post,
                    'norm_3d_mean': feat3d_norm
                }
                self._fusion_stats.update(monitor_stats)
                
                if self.debug:
                    print(f"ğŸ“Š èåˆç»Ÿè®¡: æœ‰æ•ˆæ¯”ä¾‹={valid_ratio:.3f}, 2Dç‰¹å¾èŒƒæ•°={feat2d_norm:.3f}")
            except Exception as e:
                if self.debug:
                    print(f"âš ï¸ ç»Ÿè®¡æ”¶é›†å¤±è´¥: {e}")
        
        # åœ¨æ„å»ºç¨€ç–å¼ é‡å‰ï¼Œç¡®ä¿ç‰¹å¾é•¿åº¦ä¸åæ ‡æ˜ å°„ä¸€è‡´ï¼›å¦‚ä¸ä¸€è‡´ï¼Œç›´æ¥æŠ¥é”™
        n_active = int(feat3d_sparse.features.shape[0])
        if proj3d_points is None or proj3d_points.shape[0] != n_active:
            got = -1 if proj3d_points is None else int(proj3d_points.shape[0])
            warnings.warn(
                f"proj3d_points invalid for SparseTensor (got rows={got}, active={n_active});"
                " filling zeros aligned to active coordinates.",
                stacklevel=2)
            proj3d_points = torch.zeros((n_active, self.align_dim), device=dev, dtype=torch.float32)
        proj3d_sparse = ME.SparseTensor(
            features=proj3d_points,
            coordinate_map_key=feat3d_sparse.coordinate_map_key,
            coordinate_manager=feat3d_sparse.coordinate_manager,
            tensor_stride=feat3d_sparse.tensor_stride
        )
        proj3d_points = self._extract_features_from_sparse(proj3d_sparse, xyz, points.shape[0])

        if proj2d_points is None or proj2d_points.shape[0] != n_active:
            got = -1 if proj2d_points is None else int(proj2d_points.shape[0])
            warnings.warn(
                f"proj2d_points invalid for SparseTensor (got rows={got}, active={n_active});"
                " filling zeros aligned to active coordinates.",
                stacklevel=2)
            proj2d_points = torch.zeros((n_active, self.align_dim), device=dev, dtype=torch.float32)
        proj2d_sparse = ME.SparseTensor(
            features=proj2d_points,
            coordinate_map_key=feat3d_sparse.coordinate_map_key,
            coordinate_manager=feat3d_sparse.coordinate_manager,
            tensor_stride=feat3d_sparse.tensor_stride
        )
        proj2d_points = self._extract_features_from_sparse(proj2d_sparse, xyz, points.shape[0])

        return fused, conf, valid, proj3d_points, proj2d_points


    def set_alpha_2d(self, value: float) -> None:
        """Set 2D branch gating value between 0 and 1."""
        self.alpha_2d = float(max(0.0, min(1.0, value)))

    def forward(self, points_list, imgs, cam_info):
        """ç®€åŒ–çš„forwardå‡½æ•°ï¼šæ‰¹é‡å¤„ç†3D-2Dèåˆ"""
        # 1. è¾“å…¥æ ¼å¼æ ‡å‡†åŒ–
        if self.debug:
            print(f"ğŸ” forwardè¾“å…¥æ¦‚è§ˆ | points_list: {type(points_list)} | imgs: {type(imgs)} | cam_info: {type(cam_info)}")

        if self._collect_gradient_stats:
            self._last_param_grad_norms = self._pop_param_grad_norms()
        else:
            self._last_param_grad_norms = {}

        if not isinstance(points_list, list):
            raise TypeError(f"points_list must be list[Tensor], got {type(points_list)}")
        if not isinstance(imgs, list):
            raise TypeError(f"imgs must be list[Tensor], got {type(imgs)}")
        if not isinstance(cam_info, list):
            raise TypeError(f"cam_info must be list[dict], got {type(cam_info)}")

        batch_size = len(points_list)
        if len(imgs) != batch_size:
            if len(imgs) == 1:
                single_img = imgs[0]
                if torch.is_tensor(single_img):
                    imgs = [single_img.clone() for _ in range(batch_size)]
                else:
                    imgs = [copy.deepcopy(single_img) for _ in range(batch_size)]
            else:
                raise RuntimeError(f"points({batch_size}) and imgs({len(imgs)}) length mismatch")
        if len(cam_info) != batch_size:
            if len(cam_info) == 1:
                single_meta = cam_info[0]
                cam_info = [copy.deepcopy(single_meta) for _ in range(batch_size)]
            else:
                raise RuntimeError(f"cam_info({len(cam_info)}) and points({batch_size}) length mismatch")
        
        # 3. é€æ ·æœ¬å¤„ç†
        feat_fusion_list, conf_list, valid_mask_list = [], [], []
        proj3d_list, proj2d_list = [], []
        
        for idx, (pts, img, meta) in enumerate(zip(points_list, imgs, cam_info)):
            # ç®€åŒ–metaä¿¡æ¯å¤„ç†ï¼šPKLæ–‡ä»¶æ˜¯å¸§çº§ç»„ç»‡ï¼Œç›´æ¥å¤åˆ¶
            meta_std = meta if meta is not None else {}
            
            # å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œä¼ é€’æ ·æœ¬ç´¢å¼•
            fused, conf, valid_mask, proj3d_pts, proj2d_pts = self._process_single(pts, img, meta_std, idx)
            
            feat_fusion_list.append(fused)
            conf_list.append(conf)
            valid_mask_list.append(valid_mask)
            proj3d_list.append(proj3d_pts)
            proj2d_list.append(proj2d_pts)
        
        return {
            'feat_fusion': feat_fusion_list,
            'conf_2d': conf_list,
            'valid_projection_mask': valid_mask_list,
            'proj_3d_points': proj3d_list,
            'proj_2d_points': proj2d_list,
            'alpha_2d': float(self.alpha_2d)
        }
