好的——我把你的投影链路“逐行对表”，给出**必须改的点**与**可选但强烈建议**的点，并附上**可直接替换/新增**的代码。目标是让：上采样 → 内参缩放 → 3D→2D 投影 → grid\_sample 归一化，这四步完全自洽。

---

# 0. 症结回顾（你现在代码里的问题）

1. 上采样 `align_corners=True`；而 `grid_sample` 通常配 `align_corners=False`。两者不一致会形成**恒定半像素漂移**。
2. 缩放内参后又把 `cx,cy` 强行改成特征图中心（`W/2,H/2`）。这会引入**系统性平移**。
3. 投影有效域判定用 `0.5/W` 的“安全边”；与 `grid_sample` 的归一化公式不配套，导致**边界点被过早丢弃**。
4. `_improved_projection` 里还有“极度放宽深度”的分支，会掩盖真正的错位问题。

---

# 1. 必改：上采样改为 `align_corners=False`

**你现在：**

```python
feat2d_map = F.interpolate(
    feat2d_map.unsqueeze(0),
    size=(target_h, target_w),
    mode='bilinear',
    align_corners=True   # ← 必须改
).squeeze(0)
```

**改成：**

```python
feat2d_map = F.interpolate(
    feat2d_map.unsqueeze(0),
    size=(target_h, target_w),
    mode='bilinear',
    align_corners=False  # ← 与 grid_sample 对齐
).squeeze(0)
```

为什么：与 `grid_sample(..., align_corners=False)` 的归一化公式严格配套（见第 4 节），避免半像素偏移。

---

# 2. 必改：缩放内参，但**不要**重置主点

你现在在缩放后又把主点重置为中心：

```python
scaled_intr[0] *= scale_x
scaled_intr[1] *= scale_y
scaled_intr[2] *= scale_x
scaled_intr[3] *= scale_y
scaled_intr[2] = feat_w / 2.0   # ❌ 不要这么做
scaled_intr[3] = feat_h / 2.0   # ❌
```

**正确做法：只缩放，不改中心：**

```python
scale_x = feat_w / original_w    # e.g., 40/640=0.0625
scale_y = feat_h / original_h    # e.g., 30/480=0.0625

scaled_intr = intr.clone()
scaled_intr[0] *= scale_x        # fx'
scaled_intr[1] *= scale_y        # fy'
scaled_intr[2] *= scale_x        # cx'
scaled_intr[3] *= scale_y        # cy'
# 不要覆盖 cx', cy'
```

---

# 3. 必改：统一 3D→2D 投影、边界判定与 z 过滤

把 `_improved_projection` **简化**为“几何闭合 + 一致边界”的版本：只要 `z>0` 且 `0≤u≤W-1, 0≤v≤H-1` 即有效。保留一个极小 `eps` 防除零。

**替换 `_improved_projection`：**

```python
def _improved_projection(self, xyz_cam: torch.Tensor,
                         intr_feat: torch.Tensor,
                         feat_hw: Tuple[int, int],
                         eps: float = 1e-6):
    """
    以“特征图像素坐标系”为目标投影，并返回像素级 u,v 与有效 mask。
    - xyz_cam: (N,3) 摄像机坐标系
    - intr_feat: (4,) = (fx', fy', cx', cy') 已按特征图尺度缩放后的内参
    - feat_hw: (H_feat, W_feat)
    """
    H, W = feat_hw
    fx, fy, cx, cy = intr_feat
    x, y, z = xyz_cam[:, 0], xyz_cam[:, 1], xyz_cam[:, 2]

    # 正深度（在相机前）
    valid_z = z > eps

    # 像素级投影（特征图尺度）
    u = fx * (x / (z + eps)) + cx
    v = fy * (y / (z + eps)) + cy

    # 与 grid_sample(align_corners=False) 配套的边界：
    # 允许像素中心落在 [0, W-1] / [0, H-1]
    valid_uv = (u >= 0.0) & (u <= (W - 1.0)) & (v >= 0.0) & (v <= (H - 1.0))

    valid = valid_z & valid_uv
    uv = torch.stack([u, v], dim=-1)  # (N,2), 像素坐标（特征图尺度）

    return valid, uv
```

**注意**：删除原来那段“0.01–50m、无效点太少就放宽到 0.005–100m”的逻辑；它只是在“错位”的情况下人为抬高 valid，但会让监督/融合发散。

---

# 4. 必改：`grid_sample` 归一化公式与边界一致

新增一个**统一的归一化函数**，确保与第 3 节边界判定一致（`align_corners=False`）：

```python
def _pixels_to_grid(self, uv_feat: torch.Tensor,
                    feat_hw: Tuple[int,int],
                    align_corners: bool = False) -> torch.Tensor:
    """
    把像素坐标 (u,v) 转为 grid_sample 需要的 [-1,1] 归一化坐标。
    - uv_feat: (M,2) 像素坐标（特征图尺度）
    - feat_hw: (H_feat, W_feat)
    - 返回: (1, M, 1, 2) 的 grid
    """
    H, W = feat_hw
    u = uv_feat[:, 0]
    v = uv_feat[:, 1]
    if not align_corners:
        # 与第 3 节边界 0..W-1/0..H-1 完全配套
        x_norm = 2.0 * (u + 0.5) / float(W) - 1.0
        y_norm = 2.0 * (v + 0.5) / float(H) - 1.0
    else:
        # 若你坚持 align_corners=True（不建议），则需改第 3 节边界
        x_norm = 2.0 * u / float(W - 1.0) - 1.0
        y_norm = 2.0 * v / float(H - 1.0) - 1.0
    grid = torch.stack([x_norm, y_norm], dim=-1).view(1, -1, 1, 2)
    return grid
```

---

# 5. 建议：统一实现 `sample_img_feat`，内部做归一化

把现在外面“先 u,v，再自己归一化”的零碎逻辑，收敛到一个函数里：输入像素级 `uv`（特征图尺度），**内部**做归一化并调用 `grid_sample`。这样可以避免任何“重复归一化/不一致”的坑。

**替换 `_sample_img_feat`（或你的 `sample_img_feat`）为：**

```python
def _sample_img_feat(self, feat_map: torch.Tensor,
                     uv_feat: torch.Tensor,
                     valid_mask: torch.Tensor,
                     align_corners: bool = False) -> torch.Tensor:
    """
    从特征图 (1,C,H,W) 采样 N 个点的特征。
    - feat_map: (1, C, H, W)
    - uv_feat:  (N, 2) 像素坐标（特征图尺度）
    - valid_mask: (N,) bool
    - 返回: (N, C)
    """
    assert feat_map.dim() == 4 and feat_map.size(0) == 1
    H, W = feat_map.shape[-2], feat_map.shape[-1]

    # 只对 valid 的点构造 grid，可以减少边界异常
    idx = torch.nonzero(valid_mask, as_tuple=False).squeeze(-1)
    if idx.numel() == 0:
        return feat_map.new_zeros((uv_feat.size(0), feat_map.size(1)))

    uv_valid = uv_feat[idx]  # (M,2)
    grid = self._pixels_to_grid(uv_valid, (H, W), align_corners=align_corners)  # 1xMx1x2

    # 采样: 得到 1xCx1xM
    sampled = F.grid_sample(
        feat_map, grid, mode='bilinear',
        align_corners=align_corners
    ).squeeze(2).transpose(1, 2)  # -> 1xMxC -> MxC

    out = feat_map.new_zeros((uv_feat.size(0), feat_map.size(1)))
    out[idx] = sampled[0]
    return out
```

> **调用方式**（见第 7 节集成）统一为：
>
> * `feat_map` 必须是 `(1, C, H, W)`
> * `uv` 必须是**像素级**坐标（特征图尺度），**不需要**提前归一化
> * `align_corners` 与第 1/4 节一致（推荐 `False`）

---

# 6. 建议：封装一个“按特征图尺度投影”的统一入口

新增一个入口，负责“缩放内参 + 投影 + 采样”的一条龙，把易错的细节封装掉：

```python
def project_and_sample(self,
                       xyz_cam: torch.Tensor,             # (N,3) 相机坐标
                       intr_fullres: torch.Tensor,         # (4,) 原图内参
                       full_hw: Tuple[int,int],            # (H_full, W_full) e.g. (480,640)
                       feat_map: torch.Tensor,             # (1,C,H_feat,W_feat)
                       align_corners: bool = False):
    # 1) 缩放内参到特征图
    H_feat, W_feat = feat_map.shape[-2], feat_map.shape[-1]
    W_full, H_full = full_hw[1], full_hw[0]
    sx, sy = W_feat/float(W_full), H_feat/float(H_full)
    intr_feat = intr_fullres.clone()
    intr_feat[0] *= sx; intr_feat[1] *= sy
    intr_feat[2] *= sx; intr_feat[3] *= sy

    # 2) 投影（得到像素级 uv_feat）与有效 mask
    valid, uv_feat = self._improved_projection(xyz_cam, intr_feat, (H_feat, W_feat))

    # 3) 采样（内部归一化）
    sampled = self._sample_img_feat(feat_map, uv_feat, valid, align_corners=align_corners)  # (N,C)
    return sampled, valid, uv_feat
```

---

# 7. 在 `_process_single` 里如何落地替换

把你原先的“上采样 + 手改 cx,cy + `_improved_projection` + `sample_img_feat`”几段换成下面这一段（用你现有变量名直接代入）：

```python
# ========= 2D: 上采样到 30x40，align_corners=False =========
target_w, target_h = 640 // 16, 480 // 16   # 40, 30
if feat2d_map is not None:
    if feat2d_map.shape[-2:] != (target_h, target_w):
        feat2d_map = F.interpolate(
            feat2d_map.unsqueeze(0), size=(target_h, target_w),
            mode='bilinear', align_corners=False
        ).squeeze(0)
else:
    feat2d_map = torch.zeros((256, target_h, target_w),
                             dtype=xyz_cam_proj.dtype, device=xyz_cam_proj.device)

# ========= 3D->2D: 缩放内参 + 投影 + 采样（自洽链路）=========
# intr: 原图 640x480 的 [fx,fy,cx,cy]，上面已处理为 tensor
feat_map_4d = feat2d_map.unsqueeze(0)  # (1,256,30,40)

sampled2d, valid, uv_feat = self.project_and_sample(
    xyz_cam=xyz_cam_proj,                      # (N,3)
    intr_fullres=intr,                         # (4,)
    full_hw=(480, 640),                        # H,W
    feat_map=feat_map_4d,                      # (1,256,30,40)
    align_corners=False
)
# sampled2d: (N,256), valid: (N,), uv_feat: (N,2)
```

然后再进入你后续的 `lin2d → ln2d → normalize → fusion gate` 等流程即可。

> 注意：这段代码假定你已经把坐标**闭合**了（两种做法二选一）
>
> * **A1 推荐**：`xyz_cam_proj` 用 **pose\_centered**（与 `unaligned_xyz` 成对）
> * A2：`xyz_cam_proj` 用**原始 pose**，但点先加回 `xyz_offset` 还原到原世界系

---

# 8.（可选）把投影有效域的小数边界“抖动”去掉

如果你仍然担心浮点误差导致的极限边界点偶发越界，可以给边界留一个极小 `eps`（比如 `1e-4`）：

```python
eps_pix = 1e-4
valid_uv = (u >= 0.0 - eps_pix) & (u <= (W - 1.0 + eps_pix)) & \
           (v >= 0.0 - eps_pix) & (v <= (H - 1.0 + eps_pix))
```

这比“0.5 像素安全边”要温和很多，不会白白损失一圈边界点。

---

# 9. 一页 Checklist（照这个逐条对）

* [ ] 上采样 `align_corners=False`
* [ ] 只缩放内参，不重置主点（不要写 `cx= W/2, cy=H/2`）
* [ ] `_improved_projection` 改为“`z>0` + `0..W-1 / 0..H-1` 边界”的干净版本
* [ ] 新增 `_pixels_to_grid`，与 `align_corners=False` 的归一化公式配套
* [ ] `sample_img_feat` 内部做归一化；输入是**像素级 uv**（特征图尺度）
* [ ] `project_and_sample` 一站式：缩放内参→投影→采样，避免散落重复代码
* [ ] 统一使用 **pose\_centered ⟺ unaligned\_xyz**（或点加回 offset ⟺ 原始 pose）
* [ ] 去掉“极度放宽深度”的权宜逻辑；只保留 `z>0`（或上限设个大值如 100m）
* [ ] 所有张量 dtype、device 保持一致（`intr`, `xyz_cam_proj`, `feat_map`）

---

做到这些，你的 valid 比例会主要由“边缘/遮挡/无效深度”这类**局部**因素决定（通常远高于 5–15%），而不会被“系统性错位”拖垮。如果你愿意，我也可以把这些函数整理成一个小的 `geometry.py`，再帮你把 `BiFusionEncoder._process_single` 里的相关段落打补丁成“可直接粘贴”的版本。
