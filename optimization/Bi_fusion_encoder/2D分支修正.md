非常好，把2D分支先“做干净、做闭环”是当前最该做的。下面我把需要改的点按模块梳理一遍，并给出“最小可行修改清单”。你提到的三件事（换官方预处理、不再用 spatial\_raw、去掉 global\_feat）之外，其实还有几处**几何与归一化相关**的小坑必须一起统一，才能把投影稳定住。

---

# 你要改的到底有哪些（按优先级）

## A. 预处理 & 几何闭环（最关键）

1. **严格使用 CLIP 官方预处理**

   * 使用 `open_clip.create_model_and_transforms` 返回的 `preprocess`（PIL bicubic + 等比缩放 + CenterCrop(224) + CLIP mean/std）。
   * 不再手写 `F.interpolate(images, size=(224,224))` 这种“非等比拉伸”。

2. **在编码器里显式回传几何参数 `geom_meta`**（供投影用）

   * 计算并返回：

     * `mode`: `"official"`（等比→居中裁剪）或 `"stretch"`（如果你以后切回自定义拉伸）
     * `s` 或 `(sx, sy)`、`(dx, dy)`、`in_size=(H0,W0)`、`crop_size=(224,224)`、`feat_size=(14,14)`
   * BiFusion 的投影把 (u,v) 先映射到 **224裁剪域**，再除以16得到 14×14，或再映射到你要采样的 30×40（见下文 F）。

> 这一步是修掉“系统性错位”的核心。几何映射**必须**按预处理真实发生的缩放/裁剪来。

---

## B. 编码器主干（ViT）输出什么

3. **只用 `patch_tokens` 做空间特征**

   * 去掉 `spatial_raw + patch_tokens` 的直接相加；这是跨阶段裸加，容易引噪。
   * 若想融合两路，改为**1×1 conv 适配 + 门控残差**（可后续加；现在先用干净的 `patch_tokens`）。

4. **“跑前 K 层 Transformer”的落地**

   * 仍然保留你“只跑前6层”的策略（可冻结更早层，微调靠后的几层），但**输出就是这些层后的 `patch_tokens`**，不做额外跨阶段相加。

---

## C. 2D 投影头（ProjectionHead2D）与 3D 对称

5. **2D/3D 适配头对称化**

   * 与 3D 的 MLP 类似：`Linear(768→512) + GELU + LayerNorm + Dropout(可选) + Linear(512→256)`。
   * **不要**在**整张 feature map**位置上做 L2；只在**点级采样后**再考虑是否 L2（见 E）。

---

## D. 归一化策略（放在哪里、放多少）

6. **取消“图上逐位置 L2”**

   * 不要对 `(B,256,H,W)` 逐位置做 L2。
   * 2D/3D 投影头内部保留 **LayerNorm**（或 RMSNorm），**不要 L2**。

7. **点级采样后的归一化**

   * 若你的损失是“余弦相似 / 对比学习”，在**采样到点级特征 `(N,256)` 后**对每个点做 `F.normalize(x, dim=-1)`；
   * 若损失不是角度型的（比如 MSE/CE），可以**不做 L2**，让幅度参与门控学习；在融合后再统一做一遍 L2 也可（保持检索/相似度稳定）。

> 简单说：**图上不L2，点上按需要加L2**；LayerNorm留在投影头里即可。

---

## E. grid\_sample 与上采样的一致性

8. **统一 `align_corners=False`**

   * 上采样 14→30×40 用 `F.interpolate(..., mode='bilinear', align_corners=False)`；
   * `grid_sample` 也用 `align_corners=False`；
   * 归一化坐标用**半像素中心**：`x = 2*(u + 0.5)/W - 1`，`y = 2*(v + 0.5)/H - 1`。

9. **边界判定用“像素中心域”**

   * 在采样前先判断 `0 ≤ u ≤ W-1` 与 `0 ≤ v ≤ H-1`（而不是 0.5 的安全边）。

---

## F. 14×14 vs 30×40：只选一种采样域，几何要配套

10. **优先直接在 14×14 上采样**（最省事，几何最清楚）

* 公式：

  * 官方预处理：`u14 = (s*u - dx) / 16`，`v14 = (s*v - dy) / 16`
  * 归一化后进 `grid_sample`。
* 如果你*确实*要在 30×40 上采：

  * `u40 = (s*u - dx) * (40/224)`，`v30 = (s*v - dy) * (30/224)`；
  * 注意依旧 `align_corners=False` 与半像素中心。

> 建议：先把所有采样都改回 **14×14**（最少一跳变换），把投影稳定住，再考虑是否 14→30×40 上采样。

---

## G. 冻结策略（白名单，避免误伤）

11. **白名单控制可训练层**

* 默认全冻，然后**显式解冻**：`conv1`（看你策略）、`pos_embed`（一般也冻），`transformer.resblocks[:num_layers]` 中最后 k 个 block。
* 避免 `else: requires_grad=False` 误伤本想训练的投影头/ln/conv。
* 训练模式：让解冻的块处于 `train()`，其他 `eval()`；CLIP ViT 没有 heavy dropout，不用担心随机性。

---

## H. 颜色/通道/数据类型

12. **颜色与数值域**

* 确保输入是 **RGB**（不是 BGR）；范围 `[0,1]`；使用 CLIP 的 mean/std（不是 ImageNet）。
* AMP 下注意精度：`preprocess` 在 CPU/PIL，入网张量为 `float32`；再由 autocast 控制中间精度。

---

## I. 输出契约（改方法签名，驱动几何闭环）

13. **改 `EnhancedCLIPEncoder.forward` 的返回**

* 现在返回：`spatial_feat_14: (B,256,14,14)`, `geom_meta`（含 s、dx、dy、in\_size、feat\_size、mode）。
* （global\_feat 你已计划删除——OK）

---

# 参考“最小可行”代码骨架（伪代码）

```python
class EnhancedCLIPEncoder(nn.Module):
    def __init__(...):
        self.clip_model, _, self.preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained=...)
        self.visual = self.clip_model.visual
        self.num_layers = 6
        # 对称化 2D 投影头
        self.proj2d = nn.Sequential(
            nn.Linear(768, 512), nn.GELU(), nn.LayerNorm(512),
            nn.Dropout(0.1),
            nn.Linear(512, 256)
        )
        # 冻结：先全冻，再白名单解冻
        for p in self.visual.parameters(): p.requires_grad = False
        for i in range(self.num_layers - 2, self.num_layers):  # 例：只解冻最后两层
            for p in self.visual.transformer.resblocks[i].parameters():
                p.requires_grad = True

    def forward(self, images_640x480):
        # 1) 官方预处理：PIL bicubic + 等比 + CenterCrop + CLIP mean/std
        #    这里建议在 dataloader / collate 前做；若在这里做，要把 s, dx, dy 算出来
        B, _, H0, W0 = images_640x480.shape   # 480, 640
        s = 224.0 / H0
        W1 = W0 * s
        dx, dy = (W1 - 224.0) / 2.0, 0.0

        # 真正入网的张量：images_224 = preprocess(images_640x480)  # 省略细节
        # 2) ViT 前 6 层
        x = self.visual.conv1(images_224)      # (B, 768, 14, 14)
        x = x.flatten(2).permute(0,2,1)        # (B, 196, 768)
        x = torch.cat([cls_tok, x], dim=1)
        x = x + pos_embed
        x = self.visual.ln_pre(x)
        x = x.permute(1,0,2)
        for i in range(self.num_layers):
            x = self.visual.transformer.resblocks[i](x)
        x = x.permute(1,0,2)
        patch = x[:,1:,:]                      # (B,196,768)

        # 3) 2D投影头（逐 token）
        patch = self.proj2d(patch)             # (B,196,256)
        spatial_feat_14 = patch.transpose(1,2).reshape(B,256,14,14)

        # 4) 返回几何信息，供投影使用
        geom_meta = dict(
            mode="official",
            s=float(s), dx=float(dx), dy=float(dy),
            in_size=(int(H0), int(W0)),
            crop_size=(224,224),
            feat_size=(14,14),
            align_corners=False
        )
        return spatial_feat_14, geom_meta
```

> BiFusion 侧：若直接用 14×14 采样，按 `u14 = (s*u - dx)/16`、`v14 = (s*v - dy)/16`，再做半像素中心归一化进 `grid_sample`。

---

# 回答你的问题：“除了三点之外，还有什么要改？”

**有**，而且是关键的：

* **把几何参数（s, dx, dy, feat\_size）从 2D 编码器传出来，并按它来投影**（最重要）。
* **去掉图上逐位置 L2**；改为**点级（采样后）按需 L2**。
* **统一 `align_corners=False` + 半像素中心归一化**，并用像素中心边界。
* **2D/3D 投影头对称**（结构和归一化一致）；2D 只用 `patch_tokens`。
* **白名单冻结策略**，避免误伤你想训的层。
* **颜色/均值方差严格用 CLIP 的**。
* **（可选）先只在 14×14 上采样**，投影几何链路越短越稳；等投影稳定后再考虑 14→30×40 上采。

把这些做完，你的 2D→3D 对齐会“立即”稳定很多；valid 比例、落点热力图、alpha 学习都会回到正常水平。之后再视情况慢慢加门控残差、再上更强的融合策略即可。
