# 1211：基于在线 DINOv2 的数据增强与 2D–3D 对齐方案（ESAM / ScanNet200-SV）

> 视角：资深 3D 场景重建 + 多模态融合科学家  
> 目标：在 ESAM 单帧 ScanNet200-SV 上，引入 **在线、几何严格对齐的 DINOv2 特征**，同时恢复 3D 几何增强以缓解过拟合；DINO 模型始终冻结。

---

## 0. 总体思路

- 训练管线回到 ESAM 基线的「强 3D 增强」设置（随机翻转、旋转、缩放、弹性形变），以提高数据多样性，缓解过拟合。
- 2D 分支不再使用离线 `clip_pix`，而是：
  - 在 **dataset pipeline** 中对 RGB 图做：  
    1）`LoadSingleImageFromFile` 加载原始 480×640 图像与 pose / intrinsics；  
    2）同步 3D 的 **水平翻转**；  
    3）**颜色抖动**；  
    4）**等比例 resize 到 420×560**，并同步缩放相机内参；  
  - 然后在模型中用冻结的 DINOv2 ViT-L 对 batch 图像做前向，得到 `1024×30×40` 的 patch 特征（不再 resize）。
- 在模型前向阶段，对每个样本：
  1）从「已增强的训练坐标系」下的点云，通过 `apply_3d_transformation(reverse=True)` 反解出相机坐标；  
  2）用「更新后的 intrinsics + img_size_dino」将 3D 点投影到 420×560 图像（叠加 2D 水平翻转）；  
  3）将像素坐标映射到 DINO patch 网格 / 特征图上，得到点级 DINO 特征；  
  4）用 `build_sparse_fpn` 在 3D 侧构建多尺度 DINO FPN，并在 `Res16UNet34C` decoder 中多尺度注入。

> 核心约束：  
> **所有 3D/2D 增强都必须在 `img_meta` / `cam_info` 中可逆描述，2D–3D 对齐全程保持严格几何一致；DINOv2 完全冻结。**

---

## 1. Dataset 与基础加载（单样本级）

### 1.1 Dataset 信息

- 继续使用 `ScanNet200SegDataset_`（`oneformer3d/scannet_dataset.py`），确保 info 中包含：
  - 3D：`pts_path / pts_semantic_mask_path / pts_instance_mask_path / super_pts_path`
  - 2D：`img_path`（SV 单帧）、`pose`（cam2world 或 world2cam）
- `tools/create_data.py` 继续使用现有逻辑；DINO 方案不改变 info 结构。

### 1.2 初始加载 transforms（train_pipeline / test_pipeline）

按顺序：

1. `LoadPointsFromFile`
   - `coord_type='DEPTH', use_color=True, load_dim=6, use_dim=[0,1,2,3,4,5]`；
   - 输出 `results['points']`（XYZRGB）。

2. `LoadAnnotations3D_`
   - 输出 `pts_semantic_mask / pts_instance_mask / sp_pts_mask` 等。

3. `SwapChairAndFloor`、`PointSegClassMapping`
   - 保持 ESAM 原始类别映射逻辑（floor / chair 交换 + ScanNet200 remap）。

4. `LoadSingleImageFromFile(dataset_type='scannet200')`
   - 加载原始 RGB（480×640）到 `results['img']`；
   - 构造 `cam_info=[cam_meta]`，其中：
     - `intrinsics = [577.870605, 577.870605, 319.5, 239.5]`（ScanNet 标准内参，对应 480×640）；  
     - `pose` / `extrinsics` 保留原始相机位姿；
   - 不做任何几何增强（flip/resize），仅负责 I/O。

> 此时：点云与图像仍处于「原始坐标系」，为后续几何反解提供干净基准。

---

## 2. 3D 几何增强（可逆，并写入 img_meta）

目标：恢复 ESAM 的 3D 数据增强，同时保证所有 3D 变换可以被 `apply_3d_transformation(reverse=True)` 完整反解。

### 2.1 RandomFlip3D（同步 2D，仅水平）

pipeline 中：

```python
dict(
    type='RandomFlip3D',
    sync_2d=True,
    flip_ratio_bev_horizontal=0.5,
    flip_ratio_bev_vertical=0.0
)
```

- 对 3D：
  - 在 BEV 上做水平翻转（X/Y 维取负），同步更新 `points` / `elastic_coords` 等。
- 对 2D / `img_meta`：
  - 写入 `pcd_horizontal_flip=True/False`；
  - 写入 `img_flip=True/False`；
  - 在 `transformation_3d_flow` 中追加 `"HF"`（保证顺序与实际操作一致）。

> 之后，`apply_3d_transformation(..., reverse=True)` 会在反解时按 flow 的反序应用 `HF` 的逆操作。

### 2.2 GlobalRotScaleTrans

```python
dict(
    type='GlobalRotScaleTrans',
    rot_range=[-3.14, 3.14],
    scale_ratio_range=[0.8, 1.2],
    translation_std=[0.1, 0.1, 0.1],
    shift_height=False
)
```

- 对 3D：
  - 对点云进行全局旋转 / 缩放 / 平移。
- 对 `img_meta`：
  - 写入 `pcd_rotation`（3×3）、`pcd_scale_factor`（float）、`pcd_trans`（3）；
  - 在 `transformation_3d_flow` 中追加 `"R"`, `"S"`, `"T"`。

### 2.3 ElasticTransfrom

```python
dict(
    type='ElasticTransfrom',
    gran=[6, 20],
    mag=[40, 160],
    voxel_size=0.02,
    p=0.5
)
```

- 只生成 `elastic_coords` 作为 Minkowski 体素化时的抖动坐标；
- **DINO 投影建议始终使用未 elastic 的坐标 `points[:, :3]`**，以保持相机几何干净。

> 完成 2.x 后：  
> - 训练使用的 `points` 已在“增强坐标系”；  
> - `img_meta` 中完整记录 3D 变换链（含 `"HF"`），可在模型内反解回相机系。

---

## 3. 2D 分支数据增强（pipeline 中完成：颜色 + 等比例 resize）

2D 增强全部写在 dataset pipeline 中，模型只看到已经增强好的 `img` 与同步更新后的 `cam_info`。

### 3.1 颜色抖动（只改像素，不改几何）

- 在 `oneformer3d/loading.py` 新增 2D transform：`ColorJitterImg`：
  - 读入 `results['img']`（H×W×3 或 tensor）；
  - 使用 `torchvision.transforms.ColorJitter` 做亮度/对比度/饱和度/色相扰动；
  - 写回 `results['img']`；
  - 不修改 `cam_info` / pose / intrinsics。

pipeline 中：

```python
dict(
    type='ColorJitterImg',
    brightness=0.4,
    contrast=0.4,
    saturation=0.4,
    hue=0.1
),
```

### 3.2 等比例 resize 到 420×560 + 内参更新

- 在 `oneformer3d/loading.py` 新增 `ResizeForDINO`：

  - 输入：`results['img']`（约 480×640）、`results['cam_info']`。
  - 计算缩放因子：

    ```python
    H0, W0 = img.shape[:2]       # 预期 480, 640
    H1, W1 = 420, 560
    scale_h = H1 / H0
    scale_w = W1 / W0
    assert abs(scale_h - scale_w) < 1e-3
    ```

  - 对 `img` 做等比例 resize，得到 420×560；
  - 对 `cam_info` 中每个 `meta` 同步更新：

    ```python
    fx, fy, cx, cy = meta['intrinsics']
    meta['intrinsics'] = [fx * scale_w, fy * scale_h, cx * scale_w, cy * scale_h]
    meta['img_size_dino'] = (H1, W1)  # (420,560)
    ```

  - **若图像在 2.1 中做过水平翻转，flip 已经体现在像素中，内参只需按缩放更新，不需要额外修改；真正的 flip 在后续投影时通过 `img_flip` 标志处理（见 6.2）。**

pipeline 中紧跟 `ColorJitterImg`：

```python
dict(type='ResizeForDINO', target_size=(420, 560)),
```

> 至此：  
> - 传入 DINO 的 `img` 均为 420×560；  
> - `cam_info['intrinsics']` 和 `img_size_dino` 同步反映 resize 后的几何；  
> - 2D 侧的几何（flip + resize）完全由「像素 + 内参 + img_flip 标志」显式描述。

---

## 4. 打包与 DataPreprocessor：batch 结构与易错点

### 4.1 Pack3DDetInputs_：单样本 → packed_results

- 确认 `Pack3DDetInputs_`（`oneformer3d/formatting.py`）的 `INPUTS_KEYS` 至少包含：
  - `'points', 'img', 'cam_info', 'elastic_coords', 'sp_pts_mask', 'gt_sp_masks'` 等；
- 对单个样本，输出结构形如：

```python
packed = {
  'inputs': {
    'points': Tensor(N_i, C),
    'img': Tensor(3, 420, 560),
    'cam_info': [cam_meta_i],
    'elastic_coords': ...,
    ...
  },
  'data_samples': Det3DDataSample(含 img_meta, gt_instances, gt_pts_seg)
}
```

### 4.2 collate_data + Det3DDataPreprocessor_：组 batch

collate 后，`Det3DDataPreprocessor_.simple_process` 会得到：

- `inputs['points']` → `batch_inputs['points']`: `list[Tensor(N_i,C)]`，长度 = B；
- `inputs['img']` → 堆叠为 `batch_inputs['img']`: `Tensor(B, 3, 420, 560)`；
- `inputs['cam_info']` → `batch_inputs['cam_info']`: 一般为 `list[[cam_meta_i]]`（双层 list）；
- `data_samples` → `batch_data_samples`: `list[Det3DDataSample]`。

> 易错点：`cam_info` 经 collate 后通常是 `list[list[dict]]`，而不是 `list[dict]`，模型内部必须先做规范化。

### 4.3 规范化 cam_info（模型内部的小工具）

在 `ScanNet200MixFormer3D` 内部新增工具函数：

```python
def _normalize_cam_info(self, cam_raw, num_samples):
    cam_metas = []
    if isinstance(cam_raw, list):
        for item in cam_raw:
            if isinstance(item, list) and len(item) > 0:
                cam_metas.append(item[0])
            elif isinstance(item, dict):
                cam_metas.append(item)
            else:
                raise TypeError(...)
    elif isinstance(cam_raw, dict):
        cam_metas = [cam_raw for _ in range(num_samples)]
    else:
        raise TypeError(...)
    assert len(cam_metas) == num_samples
    return cam_metas
```

后续所有 per-sample 逻辑都统一使用 `cam_metas[b_idx]`，避免 batch 对齐错误。

---

## 5. 冻结 DINOv2 Backbone（在线特征，不参与训练）

### 5.1 DINOv2Backbone 模块

- 在 `oneformer3d/dino_backbone.py` 新增：

```python
@MODELS.register_module()
class DINOv2Backbone(BaseModule):
    def __init__(self, arch='dinov2_vitl14', checkpoint=None, device='cuda'):
        super().__init__()
        # 复用 tools/extract_dinov2_features.py 中的加载逻辑
        self.extractor = DINOv2FeatureExtractor(
            arch=arch,
            device=device,
            dtype=torch.float16,
            checkpoint=checkpoint
        )
        self.model = self.extractor.model
        self.model.eval()
        for p in self.model.parameters():
            p.requires_grad = False

    @torch.no_grad()
    def forward(self, imgs: torch.Tensor) -> torch.Tensor:
        # imgs: B×3×420×560，已做完所有 2D 增强
        # 输出: B×C×H_p×W_p (C=1024, H_p=30, W_p=40)
        ...
```

- **注意**：在这里 **不要再做 resize**，只做 DINO 所需的归一化与 pad 到 patch_size 的倍数；  
  输入 `imgs` 与后续投影使用的像素/内参保持一致，确保 DINO 特征与 pixel 一一对应；
  输出特征形状固定为 `1024×30×40`（ViT-L / patch_size=14 对 420×560 的结果）。

### 5.2 在 ScanNet200MixFormer3D 中挂载

- 在 `ScanNet200MixFormer3D.__init__` 中增加：

```python
self.dino = MODELS.build(dino_cfg) if dino_cfg is not None else None
```

- 配置文件中（如 `ESAM_sv_scannet200_CA_dino.py`）增加：

```python
model = dict(
    type='ScanNet200MixFormer3D',
    ...,
    backbone=dict(
        type='Res16UNet34C',
        in_channels=3,
        out_channels=96,
        config=...,
        dino_dim=1024,       # 与 DINO 输出通道一致
    ),
    dino_cfg=dict(
        type='DINOv2Backbone',
        arch='dinov2_vitl14',
        checkpoint='/path/to/dinov2_vitl14.pth'
    ),
    ...
)
```

- 在优化器参数组中显式排除 DINO 参数，确保其完全冻结。

---

## 6. 在线 DINO→点级特征→稀疏 FPN（保持 2D–3D 严格对齐）

### 6.1 在 extract_feat 中调用 DINO（不再 resize）

在 `ScanNet200MixFormer3D.extract_feat` 开头：

```python
dino_fpn = None
if self.dino is not None:
    imgs = batch_inputs_dict.get('img', None)           # B×3×420×560
    cam_raw = batch_inputs_dict.get('cam_info', None)
    points_list = batch_inputs_dict['points']           # list[N_i×C]
    if imgs is not None and cam_raw is not None:
        cam_metas = self._normalize_cam_info(cam_raw, len(points_list))
        feat_maps = self.dino(imgs)                     # B×1024×30×40
        dino_fpn = self._build_dino_fpn_online(points_list, feat_maps, cam_metas, batch_data_samples)
```

- **注意**：此处 `imgs` 已经是 420×560，不允许在 DINO 模块内再做 resize；  
  `feat_maps` 的空间分辨率与 patch 网格天然一一对应（30×40）。

### 6.2 `_build_dino_fpn_online`：逐样本 2D–3D mapping（batch 安全）

核心步骤（伪代码）：

```python
def _build_dino_fpn_online(self, points_list, feat_maps, cam_metas, batch_data_samples):
    coords_list, feats_list = [], []
    B = len(points_list)
    for b_idx in range(B):
        pts = points_list[b_idx]              # N_i×C
        xyz_train = pts[:, :3]                # 增强坐标系
        img_meta = batch_data_samples[b_idx].metainfo
        cam_meta = cam_metas[b_idx]           # dict
        feat_map = feat_maps[b_idx]           # 1024×30×40

        # 1) 反解 3D 增强：训练坐标 → 相机坐标
        xyz_cam = apply_3d_transformation(
            xyz_train.clone(),
            coord_type='DEPTH',
            img_meta=img_meta,
            reverse=True
        )

        # 2) 使用“更新后的内参 + img_size_dino + flip 标志”投影到 420×560 像素
        H1, W1 = cam_meta.get('img_size_dino', (420, 560))
        fx, fy, cx, cy = cam_meta['intrinsics']
        uv, valid = project_points_to_uv(
            xyz_cam,
            feat_hw=(H1, W1),
            max_depth=cam_meta.get('max_depth', 20.0),
            standard_intrinsics=(fx, fy, cx, cy)
        )

        # 若 img_flip=True，则在像素坐标上做水平翻转
        if img_meta.get('img_flip', False) or img_meta.get('flip', False):
            u = uv[:, 0]
            v = uv[:, 1]
            u = W1 - 1 - u
            uv = torch.stack([u, v], dim=-1)

        # 3) 从 DINO 特征图采样点级特征
        #    这里可以选择 patch 索引（u/14,v/14）或 unified_projection_and_sample
        feat2d = sample_img_feat(
            feat_map.unsqueeze(0),  # 1×C×30×40
            uv,
            valid,
            align_corners=False
        )                           # N_i×1024

        # 4) 构造 Minkowski 稀疏坐标并累计
        coords = (xyz_train / self.voxel_size).floor().to(torch.int32)
        batch_col = torch.full((coords.shape[0], 1), b_idx, dtype=torch.int32, device=coords.device)
        coords_batched = torch.cat([batch_col, coords], dim=1)

        coords_list.append(coords_batched)
        feats_list.append(feat2d)

    coords_batch = torch.cat(coords_list, dim=0)
    feats_batch = torch.cat(feats_list, dim=0)
    return build_sparse_fpn(coords_batch, feats_batch)  # [s1,s2,s4,s8,s16]
```

- 这一过程严格遵守：
  - 3D 变换由 `img_meta['transformation_3d_flow']` 和 `pcd_*` 全程可逆；
  - 2D flip 由 `img_flip` 标志在像素坐标上实现；
  - resize 后内参与 DINO 输入分辨率通过 `intrinsics + img_size_dino` 显式描述；
  - DINO 特征图 `feat_map` 与 420×560 像素 / patch 网格严格一一对应。

### 6.3 主干注入 DINO FPN

- `Res16UNet34C` 已经支持 `dino_dim`：
  - 将 `dino_fpn` 作为 `dino_feats` 传入：  
    `x = self.backbone(field.sparse(), dino_feats=dino_fpn)`；
  - 在 decoder 的 16→8 / 8→4 / 4→2 / 2→1 的上采样层中，利用 `dino_proj_*` + `features_at_coordinates` 对齐并加法注入；
  - 原有 skip concat 结构不变，保证 3D 主干结构兼容已有预训练权重。

---

## 7. 可执行任务拆分（实现路线图 + 接口细节 + 局部测试建议）

> 说明：下面按「从外到内」的顺序列出具体实现任务，每一条都包含：  
> - 目标与接口约定（文件位置 / 函数签名 / 关键字段）；  
> - 建议的**局部测试方式**，方便每一步改完就做小范围验证，而不是一次性大改。

---

### 7.1 数据与 pipeline 相关

1. **实现 `ColorJitterImg` transform（2D 颜色增强）**  
   - 位置：`oneformer3d/loading.py`  
   - 接口：
     - 装饰器：`@TRANSFORMS.register_module()`  
     - 类定义：  
       ```python
       class ColorJitterImg(BaseTransform):
           def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1):
               self.jitter = T.ColorJitter(brightness, contrast, saturation, hue)

           def transform(self, results: dict) -> dict:
               img = results['img']      # H×W×3 或 tensor
               # 转为 tensor / PIL，应用 jitter，再写回
               ...
               results['img'] = img_aug
               return results
       ```
     - 只读写 `results['img']`，不修改 `cam_info` 或几何相关字段。
   - 局部测试建议：
     - 写一个最简 pipeline（只含 `LoadSingleImageFromFile` + `ColorJitterImg`），调用 `TRANSFORMS.build` 构建后对单条样本运行；  
     - 打印/可视化前后图像均值或保存一张图，确认颜色确实发生扰动，但空间尺寸未改变。

2. **实现 `ResizeForDINO` transform（420×560 + 内参更新）**  
   - 位置：`oneformer3d/loading.py`  
   - 接口：
     - 装饰器：`@TRANSFORMS.register_module()`  
     - 类定义：
       ```python
       class ResizeForDINO(BaseTransform):
           def __init__(self, target_size=(420, 560)):
               self.target_h, self.target_w = target_size

           def transform(self, results: dict) -> dict:
               img = results['img']          # H0×W0×3 或 tensor
               H0, W0 = img.shape[-2], img.shape[-1]  # 注意 tensor/ndarray 分支
               H1, W1 = self.target_h, self.target_w
               scale_h = H1 / H0
               scale_w = W1 / W0
               assert abs(scale_h - scale_w) < 1e-3

               img_resized = resize_fn(img, (H1, W1))  # 可用 mmcv 或 torchvision
               results['img'] = img_resized

               if 'cam_info' in results:
                   cam_info = results['cam_info']
                   # cam_info 可能是 list[dict] 或 dict，按现有习惯优先支持 list[dict]
                   metas = cam_info if isinstance(cam_info, list) else [cam_info]
                   for meta in metas:
                       fx, fy, cx, cy = meta['intrinsics']
                       meta['intrinsics'] = [fx * scale_w, fy * scale_h, cx * scale_w, cy * scale_h]
                       meta['img_size_dino'] = (H1, W1)
               return results
       ```
   - 重要约定：
     - 不做 flip，只做等比例缩放；  
     - 如果前面已经做过 `RandomFlip3D(sync_2d=True)`，flip 体现在像素上，内参只乘缩放因子；  
     - 要兼容 `cam_info` 缺失（如 test 模式异常）时的安全返回。
   - 局部测试建议：
     - 单条样本跑 `LoadSingleImageFromFile` + `ResizeForDINO`，打印：  
       `img.shape`、`cam_info[0]['intrinsics']`、`cam_info[0].get('img_size_dino')`；  
     - 确认缩放前后 `fx, cx` 按宽度比例变化，`fy, cy` 按高度比例变化，且图像尺寸为 420×560。

3. **更新 DINO 训练 config（pipeline 串联）**  
   - 位置：`configs/ESAM_CA/ESAM_sv_scannet200_CA_dino.py`（以及对应的 ft 版本）  
   - 具体修改：
     - 删除 `LoadClipFeature`；  
     - 在 `train_pipeline` 中调整为：
       ```python
       train_pipeline = [
           dict(type='LoadPointsFromFile', ...),
           dict(type='LoadAnnotations3D_', ...),
           dict(type='SwapChairAndFloor'),
           dict(type='PointSegClassMapping'),
           dict(type='LoadSingleImageFromFile', dataset_type='scannet200'),
           dict(type='RandomFlip3D', sync_2d=True, flip_ratio_bev_horizontal=0.5, flip_ratio_bev_vertical=0.0),
           dict(type='GlobalRotScaleTrans', ...),
           dict(type='ElasticTransfrom', ...),
           dict(type='NormalizePointsColor_', ...),
           dict(type='ColorJitterImg', ...),
           dict(type='ResizeForDINO', target_size=(420, 560)),
           dict(type='AddSuperPointAnnotations', ...),
           dict(type='Pack3DDetInputs_', keys=[..., 'img', 'cam_info']),
       ]
       ```
     - `test_pipeline` 使用同样的 `LoadSingleImageFromFile`、`ResizeForDINO`，但去掉随机性。
   - 局部测试建议：
     - 使用 `tools/print_config.py`（如果有）或在训练脚本里打印当前 pipeline，确认所有 transform 顺序正确；  
     - 手动构造一个 DataLoader（`batch_size=1`），迭代一两个 batch，打印 `batch['inputs'].keys()` 和 `img` 尺寸。

4. **验证 `Pack3DDetInputs_` 与 `Det3DDataPreprocessor_` 的 batch 行为**  
   - 位置：`oneformer3d/formatting.py`, `oneformer3d/data_preprocessor.py`  
   - 需要确认：
     - `Pack3DDetInputs_` 的 `INPUTS_KEYS` 确实包含 `'img', 'cam_info'`；  
     - 单样本时 `packed['inputs']['cam_info']` 为 `[dict]`；  
     - batch 后 `batch_inputs['cam_info']` 为 `list[list[dict]]`；  
     - `batch_inputs['img']` 为 `Tensor(B,3,420,560)`。
   - 局部测试建议：
     - 在 `Det3DDataPreprocessor_.simple_process` 中暂时开启 debug：  
       ```python
       print("[Det3DDataPreprocessor_] inputs keys:", inputs.keys())
       print("[Det3DDataPreprocessor_] batch_inputs keys:", batch_inputs.keys())
       ```  
     - 运行 2–3 个 batch（`batch_size=2`），检查打印结构与预期一致，然后关闭 debug。

---

### 7.2 模型结构与 DINO 封装

5. **封装 `DINOv2Backbone`（冻结的 2D 提特征模块）**  
   - 位置：`oneformer3d/dino_backbone.py`（新文件或现有文件扩展）  
   - 主要接口：
     - 注册到 `MODELS`：`@MODELS.register_module()`；  
     - 构造函数：  
       ```python
       class DINOv2Backbone(BaseModule):
           def __init__(self, arch='dinov2_vitl14', checkpoint=None, device='cuda'):
               super().__init__()
               self.extractor = DINOv2FeatureExtractor(
                   arch=arch, device=device, dtype=torch.float16, checkpoint=checkpoint
               )
               self.model = self.extractor.model
               self.model.eval()
               for p in self.model.parameters():
                   p.requires_grad = False
       ```
     - 前向：
       ```python
       @torch.no_grad()
       def forward(self, imgs: torch.Tensor) -> torch.Tensor:
           # imgs: B×3×420×560
           # 只做 DINO 归一化 + patch token 提取，不做 resize
           ...
           return feat_maps  # B×1024×30×40
       ```
   - 关键点：
     - 禁止在此处对 `imgs` 更改空间尺寸；  
     - 只负责 DINO 专用的 normalize/pad/forward_features/reshape。
   - 局部测试建议：
     - 写一个小脚本，从数据管线拿一批 `imgs`（420×560），喂入 `DINOv2Backbone`；  
     - 打印输出 shape（B,1024,30,40）与平均值/方差，确保没有 NaN/Inf。

6. **在 `ScanNet200MixFormer3D` 挂载 DINO 模块**  
   - 位置：`oneformer3d/mixformer3d.py`  
   - 修改：
     - `__init__` 增加 `dino_cfg=None` 参数，并构造 `self.dino = MODELS.build(dino_cfg)`（如果不为 None）；  
     - `backbone` config 增加 `dino_dim=1024`；  
     - `get_param_groups` 或训练脚本中排除 DINO 参数（或依赖于 `requires_grad=False` 自动过滤）。
   - config 示例：
     ```python
     model = dict(
         type='ScanNet200MixFormer3D',
         ...,
         backbone=dict(
             type='Res16UNet34C',
             in_channels=3,
             out_channels=96,
             config=dict(...),
             dino_dim=1024,
         ),
         dino_cfg=dict(
             type='DINOv2Backbone',
             arch='dinov2_vitl14',
             checkpoint='/home/nebula/.../dinov2_vitl14.pth'
         ),
         ...
     )
     ```
   - 局部测试建议：
     - 构造模型实例，打印 `model.dino` 与 `model.backbone` 的类型；  
     - 确认 `sum(p.requires_grad for p in model.dino.parameters()) == 0`。

7. **实现 `_normalize_cam_info` 工具函数（统一 batch 结构）**  
   - 位置：`oneformer3d/mixformer3d.py` 内部，作为 `ScanNet200MixFormer3D` 的私有方法；  
   - 目标：将 `batch_inputs_dict['cam_info']` 统一规范为 `list[dict]`，长度 = batch_size；  
   - 签名：
     ```python
     def _normalize_cam_info(self, cam_raw: Any, num_samples: int) -> List[Dict]:
         ...
     ```
   - 需要处理的情况：
     - `cam_raw` 是 `list[list[dict]]`；  
     - `cam_raw` 是 `list[dict]`；  
     - `cam_raw` 是单个 `dict`；  
     - 其他情况直接抛异常，方便早期暴露问题。
   - 局部测试建议：
     - 在 `extract_feat` 开头对 `batch_inputs_dict['cam_info']` 进行 `_normalize_cam_info`，打印类型和长度；  
     - 用 `batch_size=1` 和 `batch_size=2` 两种情况验证输出稳定。

---

### 7.3 在线 DINO 映射与 FPN 构建

8. **在 `extract_feat` 中调用 DINO 并取得 `feat_maps`**  
   - 位置：`oneformer3d/mixformer3d.py`，`ScanNet200MixFormer3D.extract_feat`  
   - 修改：
     - 在现有逻辑中，替换/扩展 DINO 构建部分为：  
       ```python
       dino_fpn = None
       if self.dino is not None:
           imgs = batch_inputs_dict.get('img', None)      # B×3×420×560
           cam_raw = batch_inputs_dict.get('cam_info', None)
           points_list = batch_inputs_dict['points']      # list[Tensor(N_i, C)]
           if imgs is not None and cam_raw is not None:
               cam_metas = self._normalize_cam_info(cam_raw, len(points_list))
               feat_maps = self.dino(imgs)                # B×1024×30×40
               dino_fpn = self._build_dino_fpn_online(points_list, feat_maps, cam_metas, batch_data_samples)
       ```
     - 在后续调用 Minkowski U‑Net 时，传入 `dino_feats=dino_fpn`。
   - 局部测试建议：
     - 在 `batch_size=1` 条件下，跑 `extract_feat` 一次，打印：  
       - `feat_maps.shape`；  
       - `len(dino_fpn)` 与各尺度 `tensor_stride`；  
       - 若有异常（如 cam_info 结构不一致），优先修复。

9. **实现 `_build_dino_fpn_online`（2D–3D mapping 核心）**  
   - 位置：`oneformer3d/mixformer3d.py` 内部  
   - 关键接口：
     ```python
     def _build_dino_fpn_online(
         self,
         points_list: List[torch.Tensor],
         feat_maps: torch.Tensor,            # B×1024×30×40
         cam_metas: List[Dict],
         batch_data_samples: List[Any],
     ) -> List[ME.SparseTensor]:
         ...
     ```
   - 实现要点（每个样本 b_idx）：
     1）`xyz_train = points_list[b_idx][:, :3]`；  
     2）从 `batch_data_samples[b_idx].metainfo` 读取 `pcd_rotation / pcd_scale_factor / pcd_trans / transformation_3d_flow / img_flip`；  
     3）调用 `apply_3d_transformation(xyz_train.clone(), coord_type='DEPTH', img_meta=img_meta, reverse=True)` 得到 `xyz_cam`；  
     4）从 `cam_metas[b_idx]` 读取 `intrinsics` 和 `img_size_dino`，调用 `project_points_to_uv` 得到 `uv` 与 `valid`；  
     5）若 `img_meta` 中 `img_flip=True`，在像素坐标上做 `u = W1 - 1 - u`；  
     6）用 `sample_img_feat(feat_map.unsqueeze(0), uv, valid, align_corners=False)` 得到点级 DINO 特征 `feat2d`；  
     7）构造 Minkowski 坐标 `(batch_idx, voxel_x, voxel_y, voxel_z)` 和 `feat2d`，累计所有样本后调用 `build_sparse_fpn`。
   - 局部测试建议：
     - 开一个专门的 debug 路径，只对一个 scene / 一帧调用 `_build_dino_fpn_online`，打印：  
       - `valid` 的比例；  
       - `feat2d` 的均值/方差；  
       - FPN 五个尺度的坐标数；  
     - 若有需要，可用 `vis_demo/vis_dino_point_color.py` 把 `feat2d` 做 PCA 上色检查几何对齐。

10. **将 `dino_fpn` 传入 3D 主干并检查注入**  
    - 位置：`oneformer3d/mink_unet.py`、`oneformer3d/mixformer3d.py`  
    - 检查：
      - `Res16UNet34C.forward` 的签名是否是 `forward(self, x, dino_feats=None, memory=None)`；  
      - `dino_dim` 与 `dino_proj_*` 的通道匹配（1024 → PLANES[4..7]）；  
      - 在解码各尺度的注入处（16→8/8→4/4→2/2→1）使用 `_inject_dino`，只在 `dino_feats` 非空时生效。
    - 局部测试建议：
      - 分别在 `dino_fpn=None` 和 `dino_fpn` 为有效列表的两种情况前向一次，确保两种路径都不报错；  
      - 打印注入前后某层 feature 的均值/方差，确认注入没有引入 NaN / Inf。

---

### 7.4 验证与调试（逐步放大范围）

11. **单 batch / 单样本前向 debug（最小闭环）**  
    - 配置 `batch_size=1`，关闭随机性（固定 seed / 去掉部分 3D 增强），只在 train 模式跑若干 iteration：  
      - 检查：  
        - `_normalize_cam_info` 输出的 `cam_metas` 长度与类型；  
        - `apply_3d_transformation(reverse=True)` 对于「无增强」情形是否近似恒等（可以 assert 最大偏差阈值）；  
        - `uv` 的范围与 `valid` 比例（例如>0.3）；  
        - DINO 采样后的 `feat2d` 是否无 NaN / Inf。

12. **恢复 batch_size>1，检查 batch 对齐**  
    - 使用 `batch_size=2` 或更大，在 `_build_dino_fpn_online` 内部短暂打印：  
      - `b_idx`、`points_list[b_idx].shape`、`cam_metas[b_idx]['img_size_dino']`；  
      - 每个样本的 `coords_batched[:, 0].unique()` 是否只包含单一 batch index；  
    - 确认 `build_sparse_fpn` 输出的五个尺度在 `tensor_stride` 与坐标数上合理。

13. **启动短程训练 / 验证（端到端 sanity check）**  
    - 使用较小 lr（如当前 DINO 配置中的 `5e-5`），跑少量 epoch（如 5–10）：
      - 观察 seg_loss 曲线是否平稳下降；  
      - 检查显存占用是否与预期一致（DINO online 会略增）；  
      - 在验证集上跑一次评估，确认没有明显崩溃（指标不必立刻提升，但不能退化到完全无效）。

---

在完成以上任务后，ESAM 的单帧 ScanNet200‑SV 管线将具备：

- 恢复的 3D 数据增强（缓解过拟合）；  
- 冻结的在线 DINOv2 特征，严格与增强后 RGB 图和 3D 点云几何对齐；  
- 在 3D U‑Net decoder 中多尺度注入的 DINO FPN，具备更强的语义感知能力，同时几何上完全自洽，便于后续做 DITR 风格的进一步改进与消融。  
